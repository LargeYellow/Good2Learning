

# 强化学习复习框架

## (2) 马尔科夫决策过程（MDP）

### 强化学习需要引入MDP的原因

在强化学习中，存在一个环境的状态转移模型，它可以表示为一个概率模型，即在状态$s$下采取动作$a$，转移到下一个状态$s'$的概率，表示为$P_{ss'}^{a}$

为了降低状态转移模型的复杂性，我们假设状态转移的马尔科夫性，即假设转移到下一个状态$s'$的概率仅与上一个状态$s$有关,与之前的状态无关。用公式表示为：
$$
P_{s s^{\prime}}^{a}=P\left(S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right)
$$

### Return（回报）

在强化学习中，回报被定义为是未来Reward序列的一些特定函数，用来体现长期收益。

最简单的情况下，回报被定义为是Reward的总和：

$G_t=R_{t+1}+R_{t+2}+R_{t+3}+......==\sum_{k=0}^{\infty } R_{t+k+1}$

折扣回报(*Discount Reward*)的形式则为：

$G_t=R_{t+1}+\gamma R_{t+2}+ \gamma^2 R_{t+3}+......=\sum_{k=0}^{\infty } \gamma ^k R_{t+k+1}$

引入折扣因子$\gamma \in [0,1]$的原因有很多，其主要原因是：

- 首先，数学上的方便表示，在带环的马尔可夫过程中，可以避免陷入无限循环，达到收敛。
- 其次，随着时间的推移，远期利益的不确定性越来越大，符合人类对于眼前利益的追求。（一百万年之后中100万与今天中100万）
- 再次，在金融上，近期的奖励可能比远期的奖励获得更多的利息，越早获得奖励，收益越高。

###  Value Function（价值函数）

1. 我们把策略$\pi$下的价值函数记为$v_\pi (s)$,代表从状态$s$开始，智能体根据策略$\pi$进行决策所获得的回报$G_{t}$的期望值。对于MDP，可以正式定义$v_\pi(s)$为


$$
v_\pi(s)=E_{\pi}[G_{t}|S_{t}=s]=E_{\pi}[\sum_{k=0}^{\infty } \gamma ^k R_{t+k+1}|S_t=s]
$$

​	我们把函数$v_\pi(s)$称为策略$\pi$的状态价值函数（*state-value function*）

2. 我们把策略$\pi$下在状态$s$时采取动作$a$的价值记为$q_{\pi}(s,a)$,代表从状态$s$开始，执行动作$a$之后，智能体根据策略$\pi$进行决策所获得的回报的期望值，表示为：

$$
q_{\pi}(s,a)=E_{\pi}[G_{t}|S_{t}=s,A_{t}=a]=E_{\pi}[\sum_{k=0}^{\infty } \gamma ^k R_{t+k+1}|S_t=s,A_t=a]
$$

​	我们把函数$q_\pi(s,a)$称为策略$\pi$的动作价值函数（*action-value function*）

### 贝尔曼方程与价值函数的递推关系

根据价值函数的表达式，可以得到以下推导：
$$
\begin{aligned}
v_{\pi}(s) & =\mathbb{E}_{\pi}\left(R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s\right) \\
& =\mathbb{E}_{\pi}\left(R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right) \mid S_{t}=s\right) \\
& =\mathbb{E}_{\pi}\left(R_{t+1}+\gamma G_{t+1} \mid S_{t}=s\right) \\
& =\mathbb{E}_{\pi}\left(R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right)
\end{aligned}
$$
注意第三步到第四步之间能推导是因为回报$G_t$的期望等于回报$G_t$的期望（即$v_{\pi}\left(S_{t+1}\right)$)的期望。

这个递推关系式告诉我们，**一个状态的价值由该状态的奖励以及后续的状态价值按一定的衰减比例联合而成。**

同样的方法可以得到动作价值函数$q_{\pi}(s,a)$的递推关系式：
$$
q_{\pi}(s, a)=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) \mid S_{t}=s, A_{t}=a\right)
$$

<img src="https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302191341954.webp" style="zoom:25%;" />

这张图表示的是马尔科夫决策过程的一个状态转移关系。系统一开始处于状态$s$，随机性策略$\pi(a|s)$表示状态$s$下选择动作$a$的概率。状态$s$时采取动作$a$，环境给与一个Reward $r$,之后系统会依据状态转移概率$P_{ss'}^{a}$转移到一个新的状态$s'$。

由此可以得到状态价值函数$v_{\pi}(s)$与动作价值函数$q_{\pi}(s,a)$相互之间的推导关系
$$
v_{\pi}(s)=\sum_{a\in A}[\pi(a|s)q_{\pi}(s,a)]
$$

$$
q_{\pi}(s,a)=r+\gamma \sum_{s'\in S}[P_{s s^{\prime}}^{a}\cdot v_{\pi}(s')]
$$

其中$S$和$A$分别表示状态集和动作集。

所以有
$$
v_{\pi}(s)=\sum_{a\in A}[\pi(a|s)q_{\pi}(s,a)]=\sum_{a\in A} \left[\pi(a|s)\left(r+\gamma \sum_{s'\in S}P_{s s^{\prime}}^{a} v_{\pi}(s')\right)\right]
$$
该式被称为$v_{\pi}(s)的$贝尔曼方程（*Bellman Equation*），它用等式表示了状态价值$v_{\pi}(s)$与后继状态价值$v_{\pi}(s')$之间的递推关系。

### 总结

这一部分讨论了

1. 使用马尔科夫假设来简化强化学习模型的复杂度
2. 明确了强化学习中状态价值函数（*state-value function*）与动作价值函数（*action-value function*）的概念。
3. 通过推导得到了状态价值$v_{\pi}(s)$与后继状态价值$v_{\pi}(s')$之间的递推关系，称为贝尔曼方程（*Bellman Equation*）。

## (3)用动态规划(DP)求解

### 策略评估

求解给定策略的状态价值函数，这个问题的求解过程我们称其为策略评估（*Policy Evaluation*）。

策略评估的基本思路是从一个状态价值函数$v_{\pi}(s)$出发，使用贝尔曼方程，依据给定的策略$\pi$，状态转移概率$P_{ss'}^{a}$和奖励$r$来更新状态价值函数$v_{\pi}(s)$

假设我们在第$k$轮迭代已经计算出了所有的状态的状态价值，那么在第$k+1$轮我们可以利用第$k$轮计算出的状态价值计算出第$k+1$轮的状态价值。通过贝尔曼方程来完成,即：
$$
v_{k+1}(s)=\sum_{a \in A} \pi(a \mid s)\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{k}\left(s^{\prime}\right)\right)
$$
和上一节的式子唯一的区别是由于我们的策略$\pi$已经给定，我们不再写出，对应加上了迭代轮数的下标。我们每一轮可以对计算得到的新的状态价值函数再次进行迭代，直至状态价值的值改变很小(收敛)，那么我们就得出了问题的解，即给定策略的状态价值函数$v_{\pi}$。可以理解为是得到了采取该策略情况下，各个状态的价值。

### 策略改进

策略评估的目的是帮助搜索好的策略。假设我们已经得到了一个策略$\pi$的状态价值函数$v_{\pi}$，那我们如果不根据$a=\pi(s)$来选择动作，而能找到一个策略$\pi'$,

如果对于两个确定性策略$\pi$,$\pi'$,如果以下不等式成立：
$$
\forall s \in \mathcal{S}, q_{\pi}\left(s, \pi^{\prime}(s)\right)>=v_{\pi}(s)
$$
则以下不等式也必然成立：
$$
\forall s \in \mathcal{S}, v_{\pi^{\prime}}(s)>=v_{\pi}(s)
$$
也就是只要保证前一个不等式成立，就能保证策略$\pi'$一定不会比策略$\pi$差。

显然，如果我们根据策略$\pi$的动作价值函数来选择动作，即：
$$
\pi^{\prime}(s) = \underset{a}{\arg \max }[ q_{\pi}(s, a)]
$$
(注，这部分的详细推导需要参考Sutton教科书)

则这个新策略一定是满足之前的不等式的，因此必然不会比原始策略差。这种基于贪婪的方法改进策略的方法称之为策略改进（policy improvement）

### 策略迭代

一旦一个策略$\pi$根据$v_{\pi}$通过**策略改进**得到了新的策略$\pi'$，可以接着使用**策略评估**得到其价值函数$v_{\pi'}$，然后再通过**策略改进**得到新的策略$\pi'’$。。。。。。这样的迭代方法可以得到一个不断改进的策略和价值函数的序列。这种寻找最优策略的方法叫做策略迭代。

事实上，采用基于价值函数的贪心策略，策略改进定理确保了采用这种方法每次得到的新策略都不会比原始策略差，这也就起到了通过迭代改进策略的作用。

### 总结

采用动态规划的方法逐步迭代得到更优的策略。

这种策略迭代的本质我认为是不断更新状态价值函数，状态价值函数的变化会导致策略的变化，也就是所谓的得到了新策略。

其缺点是策略评估需要遍历所有状态，计算开销其实是非常昂贵的。

## (4)用蒙特卡洛法(MC)求解

动态规划可以用来帮助做强化学习中的策略迭代工作。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的状态转化模型$P$都无法知道，这时动态规划法根本没法使用。蒙特卡洛方法由此被提出。

### 不基于模型的强化学习问题

在动态规划法中，模型的状态转移概率矩阵$P$始终是已知的，即MDP已知，我们一般将其称为基于模型的强化学习问题。

但是有很多强化学习问题，我们没有办法事先得知模型状态转移概率矩阵$P$，这种问题，如蒙特卡洛法，就是不基于模型的强化学习问题。

### 蒙特卡洛求解特点

蒙特卡罗法通过采样若干经历完整的状态序列(*episode*)来估计状态的真实价值。所谓的经历完整，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，我们就可以来近似的估计状态价值，进而求解预测和控制问题了。

从蒙特卡罗法的特点来说，一是和动态规划比，它不需要依赖于模型状态转化概率。二是它从经历过的完整序列学习，完整的经历越多，学习效果越好。

### 策略评估

在蒙特卡洛法中，一个给定策略$\pi$的包含T个状态的完整序列如下：
$$
S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, \ldots S_{t}, A_{t}, R_{t+1}, \ldots R_{T}, S_{T}
$$
而在马尔科夫决策过程(MDP)中,对于价值函数$v_{\pi}(s)$的定义为：
$$
v_{\pi}(s)=\mathbb{E}_{\pi}\left(G_{t} \mid S_{t}=s\right)=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s\right)
$$
可以看出每个状态的价值函数等于所有该状态收获的期望，同时这个收获是通过后续的奖励与对应的衰减乘积求和得到。那么对于蒙特卡罗法来说，如果要求某一个状态的状态价值，只需要**求出所有的完整序列中该状态出现时候的收获再取平均值即可近似求解**，也就是：
$$
\begin{aligned}
G_{t}= & R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \gamma^{T-t-1} R_{T} \\
& v_{\pi}(s) \approx \operatorname{average}\left(G_{t}\right), \text { s.t. } S_{t}=s
\end{aligned}
$$

### 策略迭代

蒙特卡罗法求解控制问题的思路和动态规划价值迭代的的思路类似。在动态规划价值迭代的的思路中， 每轮迭代先做策略评估，计算出价值$v_{k}(s)$，然后基于据一定的方法（比如贪婪法）更新当前策略$\pi$。最后得到最优价值函数$v_{\star}$和最优策略$\pi_{\star}$。

蒙特卡洛方法与动态规划相比，主要有两点不同。一是蒙特卡洛法一般是优化最优动作价值函数$q_{\star}$而不是状态价值函数$v_{\star}$。二是动态规划一般基于贪婪法更新策略，而蒙特卡洛法一般采用$\epsilon $-贪婪法更新策略。

$\epsilon $-贪婪法通过设置一个较小的$\epsilon$值，使用$1-\epsilon$的概率贪婪地选择目前认为是最大行为价值的相位，使用$\epsilon$的概率随机地从所有m个可选行为中选取行为，用公式可以表示为：
$$
\pi(a|s)=\left\{\begin{array}{l l}{{\epsilon/m+1-\epsilon}}&{{i f\,a^{*}=\arg\operatorname*{max}_{a\in A}Q(s,a)}}\\ {{\epsilon/m}}&{{e l s e}}\end{array}\right.
$$
在实际求解控制问题时，为了使算法可以收敛，一般$\epsilon$会随着算法的迭代过程逐渐减小，并趋于0。这样在迭代前期，我们鼓励探索，而在后期，由于我们有了足够的探索量，开始趋于保守，以贪婪为主，使算法可以稳定收敛。这样我们可以得到一张和动态规划类似的图：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302191342734.webp)

### 蒙特卡洛算法流程

在这里总结下蒙特卡罗法求解强化学习控制问题的算法流程，这里的算法是在线(on-policy)版本的,相对的算法还有离线(off-policy)版本的。同时这里我们用的是every-visit,即个状态序列中每次出现的相同状态，都会计算对应的收获值。

在线蒙特卡罗法求解强化学习控制问题的算法流程如下:

输入：状态集$S$, 动作集$A$, 即时奖励$A$，衰减因子$\gamma$, 探索率$\epsilon$

输出：最优的动作价值函数$q_{\star}$和最优策略$\pi_{\star}$

1. 初始化所有的动作价值函数$Q(s,a)=0$,状态次数$N(s,a)=0$,采样次数$k=0$,随机初始化一个策略$\pi$。
2. $k=k+1$，基于策略$\pi$进行第$k$次蒙特卡洛采样，得到一个完整的状态序列：

$$
S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, \ldots S_{t}, A_{t}, R_{t+1}, \ldots R_{T}, S_{T}
$$

3. 对于该状态序列中出现的每一状态行为对$(S_t,A_t)$，计算其回报$G_t$，更新其计数$N(s,a)$和行为价值函数$Q(s,a)$:

$$
\begin{array}{c}{{G t=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\dots\gamma^{T-t-1}R_{T}}}\\ {{{}}}\\
{{N(S_{t},A_{t})=N(S_{t},A_{t})+1}}\\ {{{}}}\\ 
{{Q(S_{t},A_{t})=Q(S_{t},A_{t})+\frac{1}{N(S_{t},A_{t})}(G_{t}-Q(S_{t},A_{t}))}}\end{array}
$$

4. 基于新计算出的动作价值，更新当前的$\epsilon$-贪婪策略：
   $$
   \begin{array}{l}{{\epsilon=\frac{1}{k}}}\\ {{{}}}\\
   {{\pi(a|s)=\left\{\begin{array}{l l}{{\epsilon/m+1-\epsilon}}&{{i f\,a^{*}=\arg\operatorname*{max}_{a\in A}Q(s,a)}}\\ {{\epsilon/m}}&{{e l s e}}\end{array}\right.}}\end{array}
   $$
   
5. 如果所有的$Q(s,a)$收敛，则对应的所有$Q(s,a)$即为最优的动作价值函数$q_{\star}$。对应的策略$\pi(a|s)$即为最优策略$\pi_{\star}$。否则转到第二步。

### 总结

蒙特卡洛方法是第一个不基于模型的强化学习求解方法。它避开了动态规划计算过于复杂，同时也不需要事先知道环境的状态转移模型，可以用于处理海量的数据和模型。它的缺点在于每次采样都需要一个完整的状态序列，如果没有完整的状态序列，或者很难拿到较多的完整的状态序列，这时候蒙特卡罗法就不太好用了。

## (5)用时序差分法（TD）求解

时序差分法和蒙特卡洛法类似，都是不基于模型的强化学习问题求解方法。

### 与蒙特卡洛法的差异点

1. 在蒙特卡洛法中，回报$G_{t}$的计算需要完整的状态序列。当没有完整序列的情况下，时序差分法可以近似地求解某个状态的回报。

​	马尔科夫决策过程中的贝尔曼方程为
$$
v_{\pi}(s)=\mathbb{E}_{\pi}(R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s)
$$
​	这启发我们可以用$R_{t+1}+\gamma v_{\pi}(S_{t+1})$来近似地代替回报$G_{t}$，我们一般把$R_{t+1}+\gamma V(S_{t+1})$称为TD目标值，$R_{t+1}+\gamma V(S_{t+1})-V(S_{t})$称为TD误差。这样我们	只需要两个连续的状态和对应的奖励，就可以尝试求解强化学习问题了。

​	因此，在时序差分法中，回报的表达式为：
$$
G(t)=R_{t+1}+\gamma V(S_{t+1})
$$

2. 在时序差分法中，由于我们没有完整的序列，也就是没有对应的次数$N(S_{t})$，一般就用一个[0, 1]的系数$\alpha$代替。迭代的式子可以表示为：
   $$
   \begin{array}{c}{{V(S_{t})=V(S_{t})+\alpha(G_{t}-V(S_{t}))}}\\ {{{}}}\\ {{Q(S_{t},A_{t})=Q(S_{t},A_{t})+\alpha(G_{t}-Q(S_{t},A_{t}))}}\end{array}
   $$

3. 时序差分法在知道结果之前就可以学习，也可以在没有结果时学习，还可以在持续进行的环境中学习，而蒙特卡罗法则要等到最后结果才能学习，时序差分法可以更快速灵活的更新状态的价值估计，这在某些情况下有着非常重要的实际意义。
4. 时序差分法在更新状态价值时使用的是TD 目标值，即基于即时奖励和下一状态的预估价值来替代当前状态在状态序列结束时可能得到的收获，是当前状态价值的有偏估计，而蒙特卡罗法则使用实际的收获来更新状态价值，是某一策略下状态价值的无偏估计，这一点蒙特卡罗法占优。
5. 虽然时序差分法得到的价值是有偏估计，但是其方差却比蒙特卡罗法得到的方差要低，且对初始值敏感，通常比蒙特卡罗法更加高效。

### n步时序差分

在单步的时序差分算法中，使用$R_{t+1}+\gamma V(S_{t+1})$来近似替代回报$G_{t}$，即向前一步来近似回报$G_{t}$。由此可以想到，也可以向前两步，此时回报$G_{t}$的近似表达式为:
$$
G_{t}^{(2)}=R_{t+1}+\gamma R_{t+2}+\gamma^{2}V(S_{t+2})
$$
从两步，到三步，再到n步，我们可以归纳出n步时序差分回报$G_{t}^{(n)}$表达式为：
$$
G_{t}^{(n)}=R_{t+1}+\gamma R_{t+2+...}+\gamma^{n-1}R_{t+n}+\gamma^{n}V(S_{t+n})
$$
当n越来越大，趋于无穷，或者说趋于使用完整的状态序列时，n步时序差分就等价于蒙特卡罗法了。

### 总结

时序差分和蒙特卡罗法比它更加灵活，不需要一个完整的序列，因此是目前主流的强化学习求解问题的方法。

## (6)时序差分在线控制算法SARSA

​		SARSA算法是一种使用时序差分求解强化学习控制问题的方法。强化学习控制问题可以表述为：给定强化学习的5个要素：状态集$S$，动作集$A$，即时奖励$R$，衰减因子$\lambda$，探索率$\epsilon$,求解最优的动作价值函数$q_{\star}$和最优策略$\pi_{\star}$。

​		这一类强化学习的问题求解不需要环境的状态转化模型，是不基于模型的强化学习问题求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新当前的策略，再通过新的策略，来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。

​		时序差分法的控制问题可以分为两类，一类是在线控制，一直使用一个策略来更新价值函数和选择新的动作；另一类是离线控制，会使用两个策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。

​		SARSA算法属于在线控制的这一类，其一直使用$\epsilon$-贪婪法来更新价值函数和选择新的动作。

### SARSA算法概述

作为SARSA算法的名字本身来说，它实际上是由S,A,R,S,A几个字母组成的。而S,A,R分别代表状态（State），动作(Action),奖励(Reward)，这也是我们前面一直在使用的符号。这个流程体现在下图：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302191342861.webp)



在迭代的时候，我们首先基于$\epsilon$-贪婪法在当前状态$S$选择一个动作$A$，这样环境会转到一个新的状态$S’$,同时返回一个即时奖励$R$，在新的状态$S’$,我们会基于$\epsilon$-贪婪法在状态$S’$选择一个动作$A'$，但是我们并不会执行这个动作$A'$，只是用来更新价值函数。价值函数的更新公式为：
$$
Q(S, A)=Q(S, A)+\alpha\left(R+\gamma Q\left(S^{\prime}, A^{\prime}\right)-Q(S, A)\right)
$$
其中，$\gamma$是是衰减因子，$\alpha$是迭代步长,其值一般需要随着迭代的进行逐渐变小，这样才能保证动作价值函数$Q$可以收敛。



### SARSA算法流程

算法输入：迭代轮数$T$，状态集$S$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$,

输出：所有的状态和动作对应的价值Q

1. 随机初始化所有的状态和动作对应的价值$Q$. 对于终止状态其$Q$值初始化为0.

2. for i from 1 to T，进行迭代。

   a) 初始化$S$为当前状态序列的第一个状态。设置$A$为$\epsilon$-贪婪法在当前状态$S$选择的动作。

   b) 在状态$S$执行当前动作A,得到新状态$S'$和奖励$R$

   c) 用$\epsilon$-贪婪法在状态$S'$选择新的动作$A’$

   d) 更新价值函数$Q(S,A)$:
   $$
   Q(S, A)=Q(S, A)+\alpha\left(R+\gamma Q\left(S^{\prime}, A^{\prime}\right)-Q(S, A)\right)
   $$
   e) $S=S'$,$A=A'$

   f）如果$S'$是终止状态，则当前轮迭代完毕，否则转到步骤b)

需要注意的是，对于SARSA算法而言，价值函数更新时使用的$A'$将作为下一轮迭代开始时的执行动作$A$。

### 总结

SARSA算法和动态规划相比，不需要知道环境的状态转移模型；和蒙特卡洛法相比，不需要完整的状态序列，因此比较灵活。在传统的强化学习方法中使用比较广泛。

SARSA算法也有一个传统强化学习方法共有的问题，就是无法求解太复杂的问题。在SARSA算法中，$Q(S,A)$的值使用一张大表来存储，如果状态和动作都达到百万乃至千万级，需要在内存里保存的这张大表会超级大，甚至溢出，因此不是很适合解决规模很大的问题。当然，对于不是特别复杂的问题，使用SARSA还是很不错的一种强化学习问题求解方法。

## (7) 时序差分离线控制算法Q-Learning

时序差分的控制问题可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作，如SARSA算法；而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数，如Q-Learning算法。

对于Q-Learning，我们会使用$\epsilon$-贪婪法来选择新的动作，这部分和SARSA完全相同。但是对于价值函数的更新，Q-Learning使用的是贪婪法，而不是SARSA的$\epsilon$-贪婪法。这一点就是SARSA和Q-Learning本质的区别。

### Q-Learning 算法概述

Q-Learning算法的拓补图入下图所示：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302191342571.webp)

首先我们基于状态$S$，用$\epsilon$-贪婪法选择到动作$A$, 然后执行动作$A$，得到奖励$R$，并进入状态$S'$，此时，如果是SARSA，会继续基于状态$S'$，用$\epsilon$-贪婪法选择$A'$,然后来更新价值函数。但是Q-Learning则不同。对于Q-Learning，它基于状态$S'$，没有使用$\epsilon$贪婪法选择$A'$，而是使用贪婪法选择$A'$，也就是说，选择使$Q(S',a)$最大的$a$作为$A'$来更新价值函数。用数学公式表示就是：
$$
Q(S, A)=Q(S, A)+\alpha\left(R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right)
$$
对应到上图中就是在图下方的三个黑圆圈动作中选择一个使$Q(S',a)$最大的动作作为$A'$。

此时选择的动作只会参与价值函数的更新，不会真正的执行。价值函数更新后，新的执行动作需要基于状态$S'$，用$\epsilon$-贪婪法重新选择得到。这一点也和SARSA稍有不同。对于SARSA，价值函数更新使用的$A'$会作为下一阶段开始时候的执行动作。

### Q-Learning算法流程

算法输入：迭代轮数$T$，状态集$S$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$,

输出：所有的状态和动作对应的价值Q

1. 随机初始化所有的状态和动作对应的价值$Q$. 对于终止状态其$Q$值初始化为0.

2. for i from 1 to T，进行迭代。

   a) 初始化$S$为当前状态序列的第一个状态。

   b) 用$\epsilon$-贪婪法在当前状态$S$中选择出动作$A$

   c) 在状态$S$执行当前动作$A$,得到新状态$S’$和奖励$R$

   d) 更新价值函数$Q(S,A)$:
   $$
   Q(S, A)=Q(S, A)+\alpha\left(R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right)
   $$
   e) $S=S'$

   f) 如果$S'$是终止状态，则当前轮迭代完毕，否则转到步骤b)

### 与SARSA比较

1. Q-Learning直接学习的就是最优策略，而SARSA在学习最优策略的同时还在做探索。这导致学习最优策略的时候如果用SARSA，为了保证收敛，需要指定一个策略使得$\epsilon$-贪婪法的超参数$\epsilon$在迭代的过程中逐渐变小，而Q-Learning没有这个要求。
2. Q-Learning直接学习最优策略，但是最优策略会依赖于训练中产生的一系列数据，所以受样本数据的影响很大，甚至会影响Q函数的收敛。Deep Q-learning同样存在这个问题。
3. SARSA在收敛的过程中鼓励探索，这样学习过程会比较平滑，不至于过于激进。Q-learning则会遇到一些特殊的"最优"陷阱，例如经典的强化学习问题"Cliff Walk"

### 结论

1. SARSA与Q-Learning使用了同为时序差分算法，但是在更新价值函数时，一个和探索时采用相同策略，一个采用不同策略。
2. Q-Learning也面临着动作空间和状态空间变大时，所需维护的Q表非常巨大的问题，这同样限制了它的应用场景。



## (8) 状态函数的近似表示与Deep Q-Learning

### 为何需要价值函数的近似表示

之前的强化学习求解方法，无论是动态规划DP,蒙特卡洛方法，还是时序差分TD，使用的状态都是离散的有限个状态集合$S$。此时的问题规模比较小，比较容易求解。但是如果遇道复杂的状态集合，或者是连续的状态，就算进行了离散化的操作，集合也很大。此时如Q-Learning这样的传统方法，没有办法在内存中维护这么大的一张Q表。因此对状态函数的近似表示是我们必须要做的一个工作。

### 价值函数的近似表示

由于问题的状态集合规模大，一个可行的建模方法是价值函数的近似表示。方法是我们引入一个状态价值函数$\hat{v}$, 这个函数由参数$w$描述，并接受状态$s$作为输入，计算后得到状态$s$的价值，即我们期望：
$$
{\hat{v}}(s,w)\approx v_{\pi}(s)
$$
类似的，引入一个动作价值函数$\hat{q}$，这个函数由参数$w$描述，并接受状态$s$与动作$a$作为输入，计算后得到动作价值，即我们期望：
$$
\hat{q}(s,a,w)\approx q_{\pi}(s,a)
$$
价值函数近似的方法很多，比如最简单的线性表示法，用$\phi(s)$表示状态$s$的特征向量，则此时我们的状态价值函数可以近似表示为：
$$
\hat{v}(s,w)=\phi(s)^{T}w
$$
当然，除了线性表示法，还可以用决策树，最近邻，傅里叶变换，神经网络来表达我们的状态价值函数。而最常见，应用最广泛的表示方法是神经网络。对于神经网络，可以使用DNN，CNN或者RNN。没有特别的限制。如果把我们计算价值函数的神经网络看做一个黑盒子，那么整个近似过程可以看做下面这三种情况：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302201436889.webp)

1. 对于状态价值函数，神经网络的输入是状态$s$的特征向量，输出是状态价值${\hat{v}}(s,w)$

2. 对于动作价值函数，有两种方法

   a) 一种是输入状态$s$的特征向量和动作$a$，输出对应的动作价值${\hat{q}}(s,a,w)$

   b) 另一种是只输入状态$s$的特征向量，动作集合有多少个动作就有多少个输出${\hat{q}}(s,a_{i},w)$,这种方式隐含的要求是动作是有限个的离散动作。

### Deep Q-Learning算法思路

Deep Q-Learning算法的基本思路来源于Q-Learning。但是和Q-Learning不同的地方在于，它的Q值的计算不是直接通过状态值$s$和动作$a$来计算，而是通过上面讲到的Q网络来计算的。这个Q网络是一个神经网络，我们一般简称Deep Q-Learning为DQN。

DQN的输入是我们的状态s对应的状态向量$\phi(s)$， 输出是所有动作在该状态下的动作价值函数$Q$。Q网络可以是DNN，CNN或者RNN，没有具体的网络结构要求。

DQN主要使用的技巧是经验回放（*experience replay*）,即将每次和环境交互得到的奖励与状态更新情况都保存起来，用于后面目标Q值的更新。为什么需要经验回放呢？我们回忆一下Q-Learning，它是有一张Q表来保存所有的Q值的当前结果的，但是DQN是没有的，那么在做动作价值函数更新的时候，就需要其他的方法，这个方法就是经验回放。

通过经验回放得到的目标Q值和通过Q网络计算的Q值肯定是有误差的，那么我们可以通过梯度的反向传播来更新神经网络的参数$w$，当$w$收敛后，我们的就得到的近似的Q值计算方法，进而贪婪策略也就求出来了。

### Deep Q-Learning算法流程

算法输入：迭代轮数$T$，状态特征维度$n$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$, Q网络结构, 批量梯度下降的样本数$m$

输出：Q网络参数

1. 随机初始化Q网络的所有参数$w$，基于$w$初始化所有的状态和动作对应的价值$Q$。清空经验回放的集合$D$。

2.  for i from 1 to T，进行迭代。

   a) 初始化$S$为当前状态序列的第一个状态, 拿到其特征向量$\phi(S)$

   b) 在Q网络中使用$\phi(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用$\epsilon$-贪婪法在当前$Q$值输出中选择对应的动作$A$

   c) 在状态$S$执行当前动作$A$,得到新状态$S'$,得到新状态$S'$对应的特征向量$\phi(S')$和奖励$R$, 是否为终止状态is_end

   d) 将$\{\phi(S),A,R,\phi(S'),is\_end\}$这个五元组存入经验回放集合$D$

   e) $S=S'$

   f) 从经验回放集合$D$中采样$m$个样本$\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2......,m$，计算当前目标Q值$y_{j}$
   $$
   y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q(\phi(S'_j),A'_j,w) & {is\_end_j \;is\; false} \end{cases}
   $$
   g) 使用均方差损失函数$\frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数$w$

   h) 如果$S'$是终止状态，则当前迭代轮完毕，否则转到步骤b)

   注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$\epsilon$需要随着迭代的进行而变小。

### 总结

1. DQN由于对价值函数做了近似表示，因此有了解决大规模强化学习问题的能力。
2. DQN通过经验回放的方式来计算目标Q值，通过梯度反向传播来更新Q网络的所有参数$w$
3. DQN不一定能保证Q网络的收敛。

## (9) Nature DQN

### DQN(NIPS 2013)的问题

在DQN(NIPS 2013)中，使用的目标Q值的计算式子为：
$$
y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q(\phi(S'_j),A'_j,w) & {is\_end_j \;is\; false} \end{cases}
$$
这里目标Q值的计算使用了当前要训练的Q网络参数来计算$Q(\phi(S'_j),A'_j,w)$,但是$y_{i}$后续又要用于更新Q网络的参数。这样两者循环依赖，迭代起来两者的相关性太强了，不利于算法的收敛。

因此Nature DQN被提出来尝试用两个Q网络来减少目标Q值计算和要更新Q网络参数之间的依赖关系。

### Nature DQN的建模

Nature DQN使用了两个Q网络，一个当前Q网络$Q$用来选择动作，更新模型参数，另一个目标Q网络$Q'$用于计算目标Q值。目标Q网络的网络参数不需要迭代更新，而是每隔一段时间从当前Q网络$Q$复制过来，即延时更新，这样可以减少目标Q值和当前的Q值相关性。

要注意的是，两个Q网络的结构是一模一样的。这样才可以复制网络参数。

Nature DQN和上一篇的DQN相比，除了用一个新的相同结构的目标Q网络来计算目标Q值以外，其余部分基本是完全相同的。

### Nature DQN算法流程

算法输入：迭代轮数$T$，状态特征维度$n$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$, 当前Q网络$Q$，目标Q网络$Q'$, 批量梯度下降的样本数$m$,目标Q网络参数更新频率$C$。

输出：Q网络参数

1. 随机初始化所有的状态和动作对应的价值$Q$。 随机初始化当前Q网络的所有参数$w$,初始化目标Q网络$Q'$的参数$w'=w$。清空经验回放的集合$D$。

2. for i from i to T, 进行迭代

   a) 初始化S为当前状态序列的第一个状态，得到其特征向量$\phi(S)$

   b) 在Q网络中使用$\phi(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用$\epsilon$-贪婪法在当前Q值输出中选择对应的动作$A$
   
   c) 在状态$S$执行当前动作$A$,得到新状态$S'$,得到新状态$S'$对应的特征向量$\phi(S')$和奖励$R$, 是否为终止状态is_end
   
   d) 将$\{\phi(S),A,R,\phi(S'),is\_end\}$这个五元组存入经验回放集合$D$
   
   e) $S=S'$
   
   f) 从经验回放集合$D$中采样$m$个样本$\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2......,m$，计算当前目标Q值$y_{j}$
   $$
   y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q'(\phi(S'_j),A'_j,w') & {is\_end_j \;is\; false} \end{cases}
   $$
   g) 使用均方差损失函数$\frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数$w$
   
   h) 如果i%C=1,则更新目标Q网络参数$w'=w$
   
   i) 如果$S'$是终止状态，则当前迭代轮完毕，否则转到步骤b)
   
   注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$\epsilon$需要随着迭代的进行而变小。

### 总结

Nature DQN对DQN NIPS 2013做了相关性方面的改进，这个改进虽然不错，但是仍然没有解决DQN的 很多问题，比如：

1. 目标Q值的计算是否准确？全部通过max Q来计算有没有问题？
2. 随机采样的方法好吗？按道理不同样本的重要性是不一样的。
3. Q值代表状态，动作的价值，那么单独动作价值的评估会不会更准确？

第一个问题对应的改进是Double DQN, 第二个问题的改进是Prioritised Replay DQN，第三个问题的改进是Dueling DQN。之后会讨论。

## (10) Double DQN

在Nature DQN的算法流程，它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性。但Nature DQN还存在过估计的问题。

### DQN的目标Q值计算问题

在DDQN之前，基本上所有的目标Q值都是通过贪婪法直接得到的，无论是Q-Learning， DQN(NIPS 2013)还是 Nature DQN，都是如此。比如对于Nature DQN,虽然用了两个Q网络并使用目标Q网络计算Q值，其第j个样本的目标Q值的计算还是贪婪法得到的，计算入下式：
$$
y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q'(\phi(S'_j),A'_j,w') & {is\_end_j \;is\; false} \end{cases}
$$
使用max虽然可以快速让Q值向可能的优化目标靠拢，但是很容易过犹不及，导致过度估计(Over Estimation)，所谓过度估计就是最终我们得到的算法模型有很大的偏差(bias)。为了解决这个问题， DDQN通过解耦目标Q值动作的选择和目标Q值的计算这两步，来达到消除过度估计的问题。

### DDQN的算法建模

DDQN和Nature DQN一样，也有一样的两个Q网络结构。在Nature DQN的基础上，通过解耦目标Q值动作的选择和目标Q值的计算这两步，来消除过度估计的问题。

在上一节里，Nature DQN对于非终止状态，其目标Q值的计算式子是：
$$
y_j= R_j + \gamma\max_{a'}Q'(\phi(S'_j),A'_j,w')
$$
在DDQN这里，不再是直接在目标Q网络里面找各个动作中最大Q值，而是先在当前Q网络中先找出最大Q值对应的动作，即
$$
a^{max}(S'_j, w) = \arg\max_{a'}Q(\phi(S'_j),a,w)
$$
然后利用这个选择出来的动作$a^{max}(S'_j, w)$在目标网络里面去计算目标Q值。即：
$$
y_j = R_j + \gamma Q'(\phi(S'_j),a^{max}(S'_j, w),w')
$$
综合起来写就是：
$$
y_j = R_j + \gamma Q'(\phi(S'_j),\arg\max_{a'}Q(\phi(S'_j),a,w),w')
$$
除了目标Q值的计算方式以外，DDQN算法和Nature DQN的算法流程完全相同。

### DDQN算法流程

这里我们总结下DDQN的算法流程，和Nature DQN的区别仅仅在步骤2.f中目标Q值的计算。

算法输入：迭代轮数$T$，状态特征维度$n$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$, 当前Q网络$Q$，目标Q网络$Q'$, 批量梯度下降的样本数$m$,目标Q网络参数更新频率$C$。

输出：Q网络参数

1. 随机初始化所有的状态和动作对应的价值$Q$。 随机初始化当前Q网络的所有参数$w$,初始化目标Q网络$Q'$的参数$w'=w$。清空经验回放的集合$D$。

2. for i from i to T, 进行迭代

   a) 初始化S为当前状态序列的第一个状态，得到其特征向量$\phi(S)$

   b) 在Q网络中使用$\phi(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用$\epsilon$-贪婪法在当前Q值输出中选择对应的动作$A$

   c) 在状态$S$执行当前动作$A$,得到新状态$S'$,得到新状态$S'$对应的特征向量$\phi(S')$和奖励$R$, 是否为终止状态is_end

   d) 将$\{\phi(S),A,R,\phi(S'),is\_end\}$这个五元组存入经验回放集合$D$

   e) $S=S'$

   f) 从经验回放集合$D$中采样$m$个样本$\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2......,m$，计算当前目标Q值$y_{j}$
   $$
   y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma Q'(\phi(S'_j),\arg\max_{a'}Q(\phi(S'_j),a,w),w')& {is\_end_j\; is \;false} \end{cases}
   $$
   g) 使用均方差损失函数$\frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数$w$

   h) 如果i%C=1,则更新目标Q网络参数$w'=w$

   i) 如果$S'$是终止状态，则当前迭代轮完毕，否则转到步骤b)

   注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$\epsilon$需要随着迭代的进行而变小。
   
## (11) Prioritized Replay DQN

在之前讨论的Nature DQN， DDQN等方法中，它们都是通过经验回放来采样，进而做目标Q值的估算的。在采样的时候，经验回放池中所有的样本都有相等的概率被采样到。

在Q网络中，TD误差指的是目标Q网络计算的目标Q值与当前Q网络计算的Q值之间的差值，考虑到经验回放池中不同样本的TD误差是不同的，对于反向传播的作用也是不一样的。TD误差越大的样本，对反向传播的作用越大，反之则越小。

如果设置TD误差的绝对值$|\delta(t)|$较大的样本更容易被采样，那么我们的算法就会比较容易收敛，这也就是Prioritized Replay DQN的思路。

### Prioritized Replay DQN算法建模

Prioritized Replay DQN根据每个样本的TD误差绝对值$|\delta(t)|$，给定该样本的优先级正比于$|\delta(t)|$，将这个优先级的值存入经验回放池。

1. 由于引入了经验回放的优先级，那么Prioritized Replay DQN的经验回放池和之前的其他DQN算法的经验回放池就不一样了。因为这个优先级大小会影响它被采样的概率。在实际使用中，我们通常使用SumTree这样的二叉树结构来做我们的带优先级的经验回放池样本的存储。

​	具体的SumTree树结构如下图：

<img src="https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302231355262.webp" style="zoom: 33%;" />

​	所有的经验回放样本只保存在最下面的叶子节点上面，一个节点一个样本。内部节点不保存样本数据。

​	而叶子节点除了保存数据以外，还要保存该样本的优先级，就是图中的显示的数字。

​	对于内部节点每个节点只保存**自己的儿子节点的优先级值之和**，如图中内部节点上显示的数字。

​	这样的结构使用了类似二分查找的思想，方便了采样。以上面的树结构为例，根节点是42，如果要采样一个样本，那么我们可以在[0,42]之间做均匀采样。比如我们采样到了24，发现24比左边的子节点29小，就往左边找；比左边的子节点大，就往右边找，同时扣除左子节点的数字13......通过这样的方式就可以快速找到采样到的样本。

2. 除了经验回放池，现在我们的Q网络的算法损失函数也有优化，之前我们的损失函数是：
   $$
   \frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2
   $$
   现在我们新的考虑了样本优先级的损失函数是:
   $$
   \frac{1}{m}\sum\limits_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2
   $$
   其中$w_j$是第j个样本的优先级权重，由TD误差$|\delta(t)|$归一化得到。

3. 当我们对Q网络参数进行了梯度更新后，需要重新计算TD误差，并将TD误差更新到SunTree上面。

除了以上三个部分， Prioritized Replay DQN和DDQN的算法流程相同。

### Prioritized Replay DQN算法流程

下面我们总结下Prioritized Replay DQN的算法流程，基于上一节的DDQN，因此这个算法我们应该叫做Prioritized Replay DDQN。主流程参考论文、\<Prioritized Experience Replay\>(ICLR 2016)。

算法输入：迭代轮数$T$,状态特征维度$n$，动作集$A$,步长$\alpha$,采样系数权重$\beta$,衰减因子$\gamma$,探索率$\epsilon$,当前Q网络$Q$，目标Q网络$Q'$，批量梯度下降的样本数$m$，目标Q网络参数更新频率$C$，SumTree的叶子节点数$S$。

输出：Q网络参数。

1. 随机初始化所有的状态和动作对应的价值$Q$. 随机初始化当前Q网络的所有参数$w$,初始化目标Q网络$Q'$的参数$w'=w$。初始化经验回放SumTree的默认数据结构，所有SumTree的S个叶子节点的优先级$P_{j}$为1。

2. for i from 1 to T，进行迭代。

   a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量$\phi(S)$

   b) 在Q网络中使用$\phi(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用$\epsilon$贪婪法在当前Q值输出中选择对应的动作$A$

   c) 在状态$S$执行当前动作$A$,得到新状态$S'$,得到新状态$S'$对应的特征向量$\phi(S')$和奖励$R$, 是否为终止状态is_end

   d) 将$\{\phi(S),A,R,\phi(S'),is\_end\}$这个五元组存入SumTree

   e) $S=S'$

   f) 从经验回放集合$D$中采样$m$个样本$\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2......,m$，每个样本被采样的概率基于$P(j) = \frac{p_j}{\sum\limits_i(p_i)}$，损失函数权重$w_j = (N*P(j))^{-\beta}/\max_i(w_i)$，计算当前目标Q值$y_{j}$:
   $$
   y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma Q'(\phi(S'_j),\arg\max_{a'}Q(\phi(S'_j),a,w),w')& {is\_end_j\; is \;false} \end{cases}
   $$
   g) 使用均方差损失函数$\frac{1}{m}\sum\limits_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2$, 通过神经网络的梯度反向传播来更新Q网络的所有参数$w$

   h) 重新计算所有样本的TD误差$\delta_j = y_j- Q(\phi(S_j),A_j,w)$,更新SumTree中所有节点的优先级$p_j = |\delta_j|$

   i) 如果$i\%C=1$,则更新目标Q网络参数$w'=w$

   j) 如果$S'$是终止状态，当前轮迭代完毕，否则转到步骤b)

   注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$\epsilon$需要随着迭代的进行而变小。

### 总结

Prioritized Replay DQN和DDQN相比，收敛速度有了很大的提高，避免了一些没有价值的迭代，因此是一个不错的优化点。同时它也可以直接集成DDQN算法，所以是一个比较常用的DQN算法。

## (12) Dueling DQN

### Dueling DQN的优化点

在DDQN中，通过优化目标Q值的方法来优化算法；在Prioritized Replay DQN中，通过优化经验回放池，按照权重采样来优化算法；而在Dueling DQN中，通过优化神经网络的结构来优化算法。

具体如何优化网络结构呢？Dueling DQN考虑将Q网络分成两部分，第一部分仅仅与状态$S$有关，与具体要采用的动作$A$无关，这部分我们叫做价值函数部分，记作$V(S,w,\alpha)$,第二部分同时与状态$S$和动作$A$有关，这部分叫做优势函数部分(*Advantage Function*)，记为$A(S,A,w,\beta)$,最终的动作价值函数可以表示为：
$$
Q(S,A, w, \alpha, \beta) = V(S,w,\alpha) + A(S,A,w,\beta)
$$
其中，$w$是公共部分的网络参数，$\alpha$是价值函数独有部分的网络参数，$\beta$是优势函数独有部分的网络参数。

### Dueling DQN的网络结构

由于Q网络的价值函数被分为两部分，因此Dueling DQN的网络结构也和之前的DQN不同。

在前面DDQN等DQN算法中，使用了一个简单的三层神经网络：一个输入层，一个隐藏层和一个输出层。如下左图所示：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302251456277.webp)

而在Dueling DQN中，在隐藏层后面接了两个子网络结构，分别对应价值函数网络部分和优势函数网络部分。对应上面右图所示。最终Q网络的输出由价值函数网络的输出和优势函数网络的输出线性组合得到。

可以直接使用之前动作价值函数的组合公式来得到想要的动作价值，但是这个式子无法辨识最终输出里面$V(S,w,\alpha)$和$A(S,A,w,\beta)$各自的作用，为了体现这种可辨识性，实际使用的组合公式为
$$
Q(S,A, w, \alpha, \beta) = V(S,w,\alpha) + (A(S,A,w,\beta) - \frac{1}{\mathcal{A}}\sum\limits_{a' \in \mathcal{A}}A(S,a', w,\beta))
$$
上式其实是对优势函数部分做了中心化的处理。由于Dueling DQN仅仅涉及神经网络的中间结构的改进，所以算法主流程和其他算法没有差异。

### 总结

这5节主要记录了5个前后有关联的算法：DQN(NIPS2013), Nature DQN, DDQN, Prioritized Replay DQN和Dueling DQN。目前使用的比较主流的是后面三种算法思路，这三种算法思路也是可以混着一起使用的，相互并不排斥。

DQN算是深度强化学习的中的主流流派，代表了Value-Based这一大类深度强化学习算法。但是它也有自己的一些问题，**就是绝大多数DQN只能处理离散的动作集合，不能处理连续的动作集合。**而深度强化学习的另一个主流流派Policy-Based而可以较好的解决这个问题。

## (13) 策略梯度

之前的DQN算法的主要思想是对价值函数进行近似表示，基于价值来学习。这种Value Based强化学习方法目前得到了比较好的应用，但是Value Based强化学习方法也存在很多的局限性，在某些场景下其它方法更加合适，比如基于策略来学习的强化学习方法。

### Value Based强化学习方法的不足

第一点是对连续动作的处理能力不足。DQN之类的方法一般都是处理离散动作，无法处理连续动作，在一些需要输出连续动作的场景中，Policy Based的方法更加容易建模。

第二点是对受限状态下的问题处理能力不足。在使用特征来描述状态空间中的某一个状态时，有可能因为个体观测的限制或者建模的局限，导致真实环境下本来不同的两个状态却在我们建模后拥有相同的特征描述，进而很有可能导致我们的value Based方法无法得到最优解。此时使用Policy Based强化学习方法也很有效。

第三点是无法解决随机策略问题。Value Based强化学习方法对应的最优策略通常是确定性策略，因为其是从众多行为价值中选择一个最大价值的行为，而有些问题的最优策略却是随机策略，这种情况下同样是无法通过基于价值的学习来求解的。这时也可以考虑使用Policy Based强化学习方法。

由于上面这些原因，Value Based强化学习方法不能通吃所有的场景，我们需要新的解决上述问题的方法，比如Policy Based强化学习方法。

### Policy Based强化学习方法引入

在Value Based强化学习方法中，我们对价值函数进行了近似表示，引入了一个动作价值函数$\hat{q}$，这个函数由参数$w$描述，并接受状态$s$与动作$a$作为输入，计算后得到近似地动作价值，即：
$$
\hat{q}(s,a,w) \approx q_{\pi}(s,a)
$$
在Policy Based强化学习方法下，我们采取类似的思路，只不过这时我们对策略进行近似表示，此时策略$\pi$可以被描述为一个包含参数$\theta$的函数，即：
$$
\pi_{\theta}(s,a) = P(a|s,\theta)\approx  \pi(a|s)
$$
将策略表示成一个连续的函数后，我们就可以用连续函数的优化方法来寻找最优的策略了。而最常用的方法就是梯度上升法，因此需要明确优化目标。

### 策略梯度的优化目标

#### 策略梯度的期望回报

在强化学习中，用$\pi(a|s)$表示在状态$s$时选择动作$a$的概率分布函数，即$π_θ(a|s)=P\{A_t=a|S_t=s,θ_t=θ\}$,表示在时刻$t$，环境状态为$s$，参数为$\theta$，输出动作为$a$的概率为$P$

<img src="https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302251540963.webp" style="zoom: 50%;" />

在强化学习中，agent与环境进行一个episode的交互的动作轨迹$τ$即图中红线，可以表示为：
$$
τ=\{s_1,a_1,s_2,a_2,......,s_t,a_t,s_{t+1}\}
$$
易得，动作轨迹$τ$出现的概率为：
$$
\begin{aligned}
p(τ)
&=p\{s_1,a_1,s_2,a_2,......,s_t,a_t,s_{t+1}\}\\
&=p(s_1)*p(a_1|s_1)*p(s_2|s_1,a_1)*p(a_2|s_2)*p(s_3|s_2,a_2)*......*p(a_t|s_t)*p(s_{t+1}|s_t,a_t)\\
&=p(s_1)*\prod_{t=1}^{T}(p(a_t|s_t)*p(s_{t+1}|s_t,a_t))
\end{aligned}
$$
如果给定一个策略$π_θ(a|s)$，则$p(τ)$可以转化为：
$$
p_θ(τ)=\pi_θ(τ)=p(s_1)*\prod_{t=1}^{T}(p_θ(a_t|s_t)*p(s_{t+1}|s_t,a_t))
$$
其中$p_θ(a_t|s_t)$由策略$π_θ(a|s)$决定，表示策略采用$θ$情况下，$s_t$时选择$a_t$动作的概率

$p(s_{t+1}|s_t,a_t)$由环境决定，表示$s_t$做出$a_t$动作后，状态转移为$s_{t+1}$的概率

用$R_{t+1}$或者$R(a_t|s_t)$表示在$s_t$状态下，采取动作$a_t$可以得到的即时回报

可以将动作轨迹$\tau$的总回报表示为：
$$
G(\tau)=R_{t+1}+\gamma R_{t+2}+ \gamma^2 R_{t+3}+......=\sum_{k=0}^{\infty } \gamma ^k R_{t+k+1}
$$
期望回报可以表示为：
$$
\begin{aligned}
\overline{\mathrm{G}_{\theta}} 
&=\sum_{\tau} p_{\theta}(\tau) * \mathrm{G}(\tau) \\
&=\mathrm{E}_{\tau \sim \pi_{\theta}} \mathrm{G}(\tau)
\end{aligned}
$$
$\mathrm{E}_{\tau \sim \pi_{\theta}} \mathrm{G}(\tau)$即在$π_θ$策略下的期望回报

#### 最大化期望回报

强化学习的目标是要训练$π_θ$策略，来最大化期望回报。也就是要找到概率分布函数$π_θ(a|s)$的一组参数$\theta$取值，来最大化期望回报$\overline{\mathrm{G}_{\theta}}$
$$
\LARGE
\theta^{*}=\underset{\theta}{\operatorname{argmax}} \overline{\mathrm{G}_{\theta}}
$$
使用梯度上升法来调整$\theta$取值，求$\overline{\mathrm{R}_{\theta}}$对$\theta$的偏导数，即参数$\theta$的梯度：
$$
\begin{aligned}
\LARGE\nabla_{\theta} \overline{\mathrm{G}_{\theta}} 
&\LARGE=\nabla_{\theta} \sum_{\tau} \pi_{\theta}(\tau) * \mathrm{G}(\tau) \\
&\LARGE=\sum_{\tau} \nabla_{\theta} \pi_{\theta}(\tau) * \mathrm{G}(\tau)
\end{aligned}
$$
复合函数求导，存在：
$$
\begin{array}{l}
\LARGE\nabla_{\theta} \log \pi_{\theta}(\tau)=\frac{1}{\pi_{\theta}(\tau)} * \nabla_{\theta} \pi_{\theta}(\tau) \\
\LARGE\Rightarrow \nabla_{\theta} \pi_{\theta}(\tau)=\pi_{\theta}(\tau) * \nabla_{\theta} \log \pi_{\theta}(\tau)
\end{array}
$$
代入，得：
$$
\begin{aligned}
\LARGE\nabla_{\theta} \overline{\mathrm{G}_{\theta}} 
&\LARGE=\sum_{\tau} \nabla_{\theta} \pi_{\theta}(\tau) * \mathrm{G}(\tau) \\
&\LARGE=\sum_{\tau} \pi_{\theta}(\tau) * [ \nabla_{\theta} \log \pi_{\theta}(\tau) * \mathrm{G}(\tau) ] \\
&\LARGE=\mathrm{E}_{\tau \sim \pi_{\theta}(\tau)} [ \nabla_{\theta} \log \pi_{\theta}(\tau) * \mathrm{G}(\tau) ]
\end{aligned}
$$
因此可以知道，期望回报$\overline{\mathrm{G}_{\theta}}$的梯度等于$\nabla_{\theta} \log \pi_{\theta}(\tau) * \mathrm{G}(\tau)$的期望值

分析$\nabla_{\theta} \log \pi_{\theta}(\tau)$可得：
$$
\begin{aligned}
\LARGE\log \pi_{\theta}(\tau) 
&\LARGE=\log \left[\mathrm{p}\left(\mathrm{s}_{1}\right) * \prod_{\mathrm{t}=1}^{\mathrm{T}} \pi_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right) * \mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right)\right] \\
&\LARGE=\log \left[\mathrm{p}\left(\mathrm{s}_{1}\right)\right]+\sum_{\mathrm{t}=1}^{\mathrm{T}} \log \left[\pi_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)\right]+\sum_{\mathrm{t}=1}^{\mathrm{T}} \log \left[\mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right)\right]
\end{aligned}
$$
而对参数$\theta$求梯度，$\log \left[\mathrm{p}\left(\mathrm{s}_{1}\right)\right]$和$\sum_{\mathrm{t}=1}^{\mathrm{T}} \log \left[\mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right)\right]$不含$\theta$，导数显然为0，得到
$$
\begin{aligned}
\LARGE\nabla_{\theta}\log \pi_{\theta}(\tau) 
&\LARGE=\log \left[\mathrm{p}\left(\mathrm{s}_{1}\right) * \prod_{\mathrm{t}=1}^{\mathrm{T}} \pi_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right) * \mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right)\right] \\
&\LARGE=\xcancel{\log \left[\mathrm{p}\left(\mathrm{s}_{1}\right)\right]}+\sum_{\mathrm{t}=1}^{\mathrm{T}}\nabla_{\theta} \log \left[\pi_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)\right]+\xcancel{\sum_{\mathrm{t}=1}^{\mathrm{T}} \log \left[\mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right)\right]}
\end{aligned}
$$
最终梯度表示为：
$$
\begin{aligned}
\LARGE\nabla_{\theta} \mathrm{J}(\theta)
= \nabla_{\theta} \overline{\mathrm{G}_{\theta}}
= \mathrm{E}_{\tau \sim \pi_{\theta}(\tau)}{\{\sum_{\mathrm{t}=1}^{\mathrm{T}} \nabla_{\theta}\log \left[\pi_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)\right]}\} *
\{ \sum_{t=1}^{T } \gamma ^t R_{t+1} \}
\end{aligned}
$$
其中又有：
$$
\large
\{ \sum_{t=1}^{T } \gamma ^t R_{t+1} \} = Q_{\pi}(s,a)
$$
最终得到
$$
\nabla_{\theta} J(\theta) = \mathbb{E}_{\pi_{\theta}}[\nabla_{\theta}log \pi_{\theta}(s,a) Q_{\pi}(s,a)]
$$


优化目标对$\theta$求导的梯度最终就表示为了上式。

我们对于最大化目标值的研究就变成了对包含$\theta$的式子$\nabla_{\theta}log \pi_{\theta}(s,a)$的研究，这个式子我们一般称为分值函数（score function）

得到了梯度的表达式，最终还是需要研究策略函数$\pi_{\theta}(s,a)$的形式。

#### 策略函数的设计

策略函数$\pi_{\theta}(s,a)$在之前一直是用一个数学符号表示，现在我们要给予它一个具体的形式才能计算偏导数。

最常用的策略函数就是softmax策略函数了，它主要应用于离散空间中，softmax策略使用描述状态和行为的特征$\phi(s,a)$与参数$\theta$的线性组合来权衡一个行为发生的几率,即:
$$
\large
\pi_{\theta}(s,a) = \frac{e^{\phi(s,a)^T\theta}}{\sum\limits_{b \in A} e^{\phi(s,b)^T\theta}}
$$
则通过求导很容易求出对应的分值函数为：
$$
\nabla_{\theta}log \pi_{\theta}(s,a) = \phi(s,a) - \mathbb{E}_{\pi_{\theta}}[\phi(s,.)]
$$
另一种高斯策略则是应用于连续行为空间的一种常用策略。该策略对应的行为从高斯分布$\mathbb{N(\phi(s)^T\theta, \sigma^2)}$中产生。高斯策略对应的分值函数求导可以得到为:
$$
\nabla_{\theta}log \pi_{\theta}(s,a) =  = \frac{(a-\phi(s)^T\theta)\phi(s)}{\sigma^2}
$$
有策略梯度的公式和策略函数，我们可以得到第一版的策略梯度算法了。

### 蒙特卡洛策略梯度reinforce算法

这里我们讨论最简单的策略梯度算法，蒙特卡罗策略梯度reinforce算法, 使用价值函数$v(s)$来近似代替策略梯度公式里面的$Q_{\pi}(s,a)$。算法的流程很简单，如下所示：

输入 ： N个蒙特卡罗完整序列,训练步长$\alpha$

输出：策略函数的参数$\theta$

1. for 每个蒙特卡罗序列:

   a) 用蒙特卡罗法计算序列每个时间位置t的状态价值$v_{t}$

   b) 对序列每个时间位置t，使用梯度上升法，更新策略函数的参数$\theta$：
   $$
   \theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t)  v_t
   $$

2. 返回策略函数的参数$\theta$

这里的策略函数可以是softmax策略，高斯策略或者其他策略。

### 总结

策略梯度提供了和DQN之类的方法不同的新思路，但是蒙特卡罗策略梯度reinforce算法却并不完美。由于是蒙特卡罗法，需要完全的序列样本才能做算法迭代，同时蒙特卡罗法使用收获的期望来计算状态价值，会导致行为有较多的变异性，导致参数更新的方向很可能不是策略梯度的最优方向。

因此，Policy Based的强化学习方法还需要改进，注意到之前有Value Based强化学习方法，Policy Based+Value Based结合的策略梯度方法被称为Actor-Critic。

## 深度学习原理



## pytorch框架的使用

## 疑问

1. 强化学习中策略的含义
2. DQN 过估计的具体解释
