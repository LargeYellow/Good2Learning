

# 强化学习复习框架

## (2) 马尔科夫决策过程（MDP）

### 强化学习需要引入MDP的原因

在强化学习中，存在一个环境的状态转移模型，它可以表示为一个概率模型，即在状态$s$下采取动作$a$，转移到下一个状态$s'$的概率，表示为$P_{ss'}^{a}$

为了降低状态转移模型的复杂性，我们假设状态转移的马尔科夫性，即假设转移到下一个状态$s'$的概率仅与上一个状态$s$有关,与之前的状态无关。用公式表示为：
$$
P_{s s^{\prime}}^{a}=P\left(S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right)
$$

### Return（回报）

在强化学习中，回报被定义为是未来Reward序列的一些特定函数，用来体现长期收益。

最简单的情况下，回报被定义为是Reward的总和：

$G_t=R_{t+1}+R_{t+2}+R_{t+3}+......==\sum_{k=0}^{\infty } R_{t+k+1}$

折扣回报(*Discount Reward*)的形式则为：

$G_t=R_{t+1}+\gamma R_{t+2}+ \gamma^2 R_{t+3}+......=\sum_{k=0}^{\infty } \gamma ^k R_{t+k+1}$

###  Value Function（价值函数）

1. 我们把策略$\pi$下的价值函数记为$v_\pi (s)$,代表从状态$s$开始，智能体根据策略$\pi$进行决策所获得的回报的期望值。对于MDP，可以正式定义$v_\pi(s)$为


$$
v_\pi(s)=E_{\pi}[G_{t}|S_{t}=s]=E_{\pi}[\sum_{k=0}^{\infty } \gamma ^k R_{t+k+1}|S_t=s]
$$

​	我们把函数$v_\pi(s)$称为策略$\pi$的状态价值函数（state-value function）

2. 我们把策略$\pi$下在状态$s$时采取动作$a$的价值记为$q_{\pi}(s,a)$,代表从状态$s$开始，执行动作$a$之后，智能体根据策略$\pi$进行决策所获得的回报的期望值，表示为：

$$
q_{\pi}(s,a)=E_{\pi}[G_{t}|S_{t}=s,A_{t}=a]=E_{\pi}[\sum_{k=0}^{\infty } \gamma ^k R_{t+k+1}|S_t=s,A_t=a]
$$

​	我们把函数$q_\pi(s,a)$称为策略$\pi$的动作价值函数（action-value function）

### 贝尔曼方程与价值函数的递推关系

根据价值函数的表达式，可以得到以下推导：
$$
\begin{aligned}
v_{\pi}(s) & =\mathbb{E}_{\pi}\left(R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s\right) \\
& =\mathbb{E}_{\pi}\left(R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right) \mid S_{t}=s\right) \\
& =\mathbb{E}_{\pi}\left(R_{t+1}+\gamma G_{t+1} \mid S_{t}=s\right) \\
& =\mathbb{E}_{\pi}\left(R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right)
\end{aligned}
$$
注意第三步到第四步之间能推导是因为回报$G_t$的期望等于回报$G_t$的期望（即$v_{\pi}\left(S_{t+1}\right)$)的期望。

这个递推关系式告诉我们，**一个状态的价值由该状态的奖励以及后续的状态价值按一定的衰减比例联合而成。**

同样的方法可以得到动作价值函数$q_{\pi}(s,a)$的递推关系式：
$$
q_{\pi}(s, a)=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) \mid S_{t}=s, A_{t}=a\right)
$$

<img src="https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302191341954.webp" style="zoom:25%;" />

从图中很容易发现
$$
v_{\pi}(s)=\sum_{a\in A}[\pi(a|s)q_{\pi}(s,a)]
$$

$$
q_{\pi}(s,a)=r+\gamma \sum_{s'\in S}[P_{s s^{\prime}}^{a}\cdot v_{\pi}(s')]
$$

其中$S$和$A$分别表示状态集和动作集。

所以有
$$
v_{\pi}(s)=\sum_{a\in A}[\pi(a|s)q_{\pi}(s,a)]=\sum_{a\in A} \left[\pi(a|s)\left(r+\gamma \sum_{s'\in S}P_{s s^{\prime}}^{a} v_{\pi}(s')\right)\right]
$$
该式被称为$v_{\pi}(s)的$贝尔曼方程，它用等式表示了状态价值与后继状态价值之间的递推关系。

### 总结

这一部分讨论了

1. 使用马尔科夫假设来简化强化学习模型的复杂度
2. 明确了强化学习中状态价值函数（state-value function）与动作价值函数（action-value function）的概念。
3. 通过推导得到了状态价值与后继状态价值之间的递推关系，称为贝尔曼方程。

## (3)用动态规划(DP)求解

### 策略评估

求解给定策略的状态价值函数，这个问题的求解过程我们称其为策略评估（Policy Evaluation）。

策略评估的基本思路是从一个状态价值函数$v_{\pi}(s)$出发，使用贝尔曼方程，依据给定的策略$\pi$，状态转移概率$P_{ss'}^{a}$和奖励$r$来更新状态价值函数$v_{\pi}(s)$

假设我们在第$k$轮迭代已经计算出了所有的状态的状态价值，那么在第$k+1$轮我们可以利用第$k$轮计算出的状态价值计算出第$k+1$轮的状态价值。通过贝尔曼方程来完成,即：
$$
v_{k+1}(s)=\sum_{a \in A} \pi(a \mid s)\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{k}\left(s^{\prime}\right)\right)
$$
和上一节的式子唯一的区别是由于我们的策略$\pi$已经给定，我们不再写出，对应加上了迭代轮数的下标。我们每一轮可以对计算得到的新的状态价值函数再次进行迭代，直至状态价值的值改变很小(收敛)，那么我们就得出了问题的解，即给定策略的状态价值函数$v_{\pi}$。可以理解为是得到了采取该策略情况下，各个状态的价值。

### 策略改进

策略评估的目的是帮助搜索好的策略。假设我们已经得到了一个策略$\pi$的状态价值函数$v_{\pi}$，那我们如果不根据$a=\pi(s)$来选择动作，而能找到一个策略$\pi'$,

如果对于两个确定性策略$\pi$,$\pi'$,如果以下不等式成立：
$$
\forall s \in \mathcal{S}, q_{\pi}\left(s, \pi^{\prime}(s)\right)>=v_{\pi}(s)
$$
则以下不等式也必然成立：
$$
\forall s \in \mathcal{S}, v_{\pi^{\prime}}(s)>=v_{\pi}(s)
$$
也就是只要保证前一个不等式成立，就能保证策略$\pi'$一定不会比策略$\pi$差。

显然，如果我们根据策略$\pi$的动作价值函数来选择动作，即：
$$
\pi^{\prime}(s) = \underset{a}{\arg \max }[ q_{\pi}(s, a)]
$$
(注，这部分的详细推导需要参考Sutton教科书)

则这个新策略一定是满足之前的不等式的，因此必然不会比原始策略差。这种基于贪婪的方法改进策略的方法称之为策略改进（policy improvement）

### 策略迭代

一旦一个策略$\pi$根据$v_{\pi}$通过**策略改进**得到了新的策略$\pi'$，可以接着使用**策略评估**得到其价值函数$v_{\pi'}$，然后再通过**策略改进**得到新的策略$\pi'’$。。。。。。这样的迭代方法可以得到一个不断改进的策略和价值函数的序列。这种寻找最优策略的方法叫做策略迭代。

事实上，采用基于价值函数的贪心策略，策略改进定理确保了采用这种方法每次得到的新策略都不会比原始策略差，这也就起到了通过迭代改进策略的作用。

### 总结

采用动态规划的方法逐步迭代得到更优的策略。

这种策略迭代的本质我认为是不断更新状态价值函数，状态价值函数的变化会导致策略的变化，也就是所谓的得到了新策略。

其缺点是策略评估需要遍历所有状态，计算开销其实是非常昂贵的。

## (4)用蒙特卡洛法(MC)求解

动态规划可以用来帮助做强化学习中的策略迭代工作。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的状态转化模型$P$都无法知道，这时动态规划法根本没法使用。蒙特卡洛方法由此被提出。

### 不基于模型的强化学习问题

在动态规划法中，模型的状态转移概率矩阵$P$始终是已知的，即MDP已知，我们一般将其称为基于模型的强化学习问题。

但是有很多强化学习问题，我们没有办法事先得知模型状态转移概率矩阵$P$，这种问题，如蒙特卡洛法，就是不基于模型的强化学习问题。

### 蒙特卡洛求解特点

蒙特卡罗法通过采样若干经历完整的状态序列(*episode*)来估计状态的真实价值。所谓的经历完整，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，我们就可以来近似的估计状态价值，进而求解预测和控制问题了。

从蒙特卡罗法的特点来说，一是和动态规划比，它不需要依赖于模型状态转化概率。二是它从经历过的完整序列学习，完整的经历越多，学习效果越好。

### 策略评估

在蒙特卡洛法中，一个给定策略$\pi$的包含T个状态的完整序列如下：
$$
S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, \ldots S_{t}, A_{t}, R_{t+1}, \ldots R_{T}, S_{T}
$$
而在马尔科夫决策过程(MDP)中,对于价值函数$v_{\pi}(s)$的定义为：
$$
v_{\pi}(s)=\mathbb{E}_{\pi}\left(G_{t} \mid S_{t}=s\right)=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s\right)
$$
可以看出每个状态的价值函数等于所有该状态收获的期望，同时这个收获是通过后续的奖励与对应的衰减乘积求和得到。那么对于蒙特卡罗法来说，如果要求某一个状态的状态价值，只需要**求出所有的完整序列中该状态出现时候的收获再取平均值即可近似求解**，也就是：
$$
\begin{aligned}
G_{t}= & R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \gamma^{T-t-1} R_{T} \\
& v_{\pi}(s) \approx \operatorname{average}\left(G_{t}\right), \text { s.t. } S_{t}=s
\end{aligned}
$$

### 策略迭代

蒙特卡罗法求解控制问题的思路和动态规划价值迭代的的思路类似。在动态规划价值迭代的的思路中， 每轮迭代先做策略评估，计算出价值$v_{k}(s)$，然后基于据一定的方法（比如贪婪法）更新当前策略$\pi$。最后得到最优价值函数$v_{\star}$和最优策略$\pi_{\star}$。

蒙特卡洛方法与动态规划相比，主要有两点不同。一是蒙特卡洛法一般是优化最优动作价值函数$q_{\star}$而不是状态价值函数$v_{\star}$。二是动态规划一般基于贪婪法更新策略，而蒙特卡洛法一般采用$\epsilon $-贪婪法更新策略。

$\epsilon $-贪婪法通过设置一个较小的$\epsilon$值，使用$1-\epsilon$的概率贪婪地选择目前认为是最大行为价值的相位，使用$\epsilon$的概率随机地从所有m个可选行为中选取行为，用公式可以表示为：
$$
\pi(a|s)=\left\{\begin{array}{l l}{{\epsilon/m+1-\epsilon}}&{{i f\,a^{*}=\arg\operatorname*{max}_{a\in A}Q(s,a)}}\\ {{\epsilon/m}}&{{e l s e}}\end{array}\right.
$$
在实际求解控制问题时，为了使算法可以收敛，一般$\epsilon$会随着算法的迭代过程逐渐减小，并趋于0。这样在迭代前期，我们鼓励探索，而在后期，由于我们有了足够的探索量，开始趋于保守，以贪婪为主，使算法可以稳定收敛。这样我们可以得到一张和动态规划类似的图：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302191342734.webp)

### 蒙特卡洛算法流程

在这里总结下蒙特卡罗法求解强化学习控制问题的算法流程，这里的算法是在线(on-policy)版本的,相对的算法还有离线(off-policy)版本的。同时这里我们用的是every-visit,即个状态序列中每次出现的相同状态，都会计算对应的收获值。

在线蒙特卡罗法求解强化学习控制问题的算法流程如下:

输入：状态集$S$, 动作集$A$, 即时奖励$A$，衰减因子$\gamma$, 探索率$\epsilon$

输出：最优的动作价值函数$q_{\star}$和最优策略$\pi_{\star}$

1. 初始化所有的动作价值函数$Q(s,a)=0$,状态次数$N(s,a)=0$,采样次数$k=0$,随机初始化一个策略$\pi$。
2. $k=k+1$，基于策略$\pi$进行第$k$次蒙特卡洛采样，得到一个完整的状态序列：

$$
S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, \ldots S_{t}, A_{t}, R_{t+1}, \ldots R_{T}, S_{T}
$$

3. 对于该状态序列中出现的每一状态行为对$(S_t,A_t)$，计算其回报$G_t$，更新其计数$N(s,a)$和行为价值函数$Q(s,a)$:

$$
\begin{array}{c}{{G t=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\dots\gamma^{T-t-1}R_{T}}}\\ {{{}}}\\
{{N(S_{t},A_{t})=N(S_{t},A_{t})+1}}\\ {{{}}}\\ 
{{Q(S_{t},A_{t})=Q(S_{t},A_{t})+\frac{1}{N(S_{t},A_{t})}(G_{t}-Q(S_{t},A_{t}))}}\end{array}
$$

4. 基于新计算出的动作价值，更新当前的$\epsilon$-贪婪策略：
   $$
   \begin{array}{l}{{\epsilon=\frac{1}{k}}}\\ {{{}}}\\
   {{\pi(a|s)=\left\{\begin{array}{l l}{{\epsilon/m+1-\epsilon}}&{{i f\,a^{*}=\arg\operatorname*{max}_{a\in A}Q(s,a)}}\\ {{\epsilon/m}}&{{e l s e}}\end{array}\right.}}\end{array}
   $$
   
5. 如果所有的$Q(s,a)$收敛，则对应的所有$Q(s,a)$即为最优的动作价值函数$q_{\star}$。对应的策略$\pi(a|s)$即为最优策略$\pi_{\star}$。否则转到第二步。

### 总结

蒙特卡洛方法是第一个不基于模型的强化学习求解方法。它避开了动态规划计算过于复杂，同时也不需要事先知道环境的状态转移模型，可以用于处理海量的数据和模型。它的缺点在于每次采样都需要一个完整的状态序列，如果没有完整的状态序列，或者很难拿到较多的完整的状态序列，这时候蒙特卡罗法就不太好用了。

## (5)用时序差分法（TD）求解

时序差分法和蒙特卡洛法类似，都是不基于模型的强化学习问题求解方法。

### 与蒙特卡洛法的差异点

1. 在蒙特卡洛法中，回报$G_{t}$的计算需要完整的状态序列。当没有完整序列的情况下，时序差分法可以近似地求解某个状态的回报。

​	马尔科夫决策过程中的贝尔曼方程为
$$
v_{\pi}(s)=\mathbb{E}_{\pi}(R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s)
$$
​	这启发我们可以用$R_{t+1}+\gamma v_{\pi}(S_{t+1})$来近似地代替回报$G_{t}$，我们一般把$R_{t+1}+\gamma V(S_{t+1})$称为TD目标值，$R_{t+1}+\gamma V(S_{t+1})-V(S_{t})$称为TD误差。这样我们	只需要两个连续的状态和对应的奖励，就可以尝试求解强化学习问题了。

​	因此，在时序差分法中，回报的表达式为：
$$
G(t)=R_{t+1}+\gamma V(S_{t+1})
$$

2. 在时序差分法中，由于我们没有完整的序列，也就是没有对应的次数$N(S_{t})$，一般就用一个[0, 1]的系数$\alpha$代替。迭代的式子可以表示为：
   $$
   \begin{array}{c}{{V(S_{t})=V(S_{t})+\alpha(G_{t}-V(S_{t}))}}\\ {{{}}}\\ {{Q(S_{t},A_{t})=Q(S_{t},A_{t})+\alpha(G_{t}-Q(S_{t},A_{t}))}}\end{array}
   $$

3. 时序差分法在知道结果之前就可以学习，也可以在没有结果时学习，还可以在持续进行的环境中学习，而蒙特卡罗法则要等到最后结果才能学习，时序差分法可以更快速灵活的更新状态的价值估计，这在某些情况下有着非常重要的实际意义。
4. 时序差分法在更新状态价值时使用的是TD 目标值，即基于即时奖励和下一状态的预估价值来替代当前状态在状态序列结束时可能得到的收获，是当前状态价值的有偏估计，而蒙特卡罗法则使用实际的收获来更新状态价值，是某一策略下状态价值的无偏估计，这一点蒙特卡罗法占优。
5. 虽然时序差分法得到的价值是有偏估计，但是其方差却比蒙特卡罗法得到的方差要低，且对初始值敏感，通常比蒙特卡罗法更加高效。

### n步时序差分

在单步的时序差分算法中，使用$R_{t+1}+\gamma V(S_{t+1})$来近似替代回报$G_{t}$，即向前一步来近似回报$G_{t}$。由此可以想到，也可以向前两步，此时回报$G_{t}$的近似表达式为:
$$
G_{t}^{(2)}=R_{t+1}+\gamma R_{t+2}+\gamma^{2}V(S_{t+2})
$$
从两步，到三步，再到n步，我们可以归纳出n步时序差分回报$G_{t}^{(n)}$表达式为：
$$
G_{t}^{(n)}=R_{t+1}+\gamma R_{t+2+...}+\gamma^{n-1}R_{t+n}+\gamma^{n}V(S_{t+n})
$$
当n越来越大，趋于无穷，或者说趋于使用完整的状态序列时，n步时序差分就等价于蒙特卡罗法了。

### 总结

时序差分和蒙特卡罗法比它更加灵活，不需要一个完整的序列，因此是目前主流的强化学习求解问题的方法。

## (6)时序差分在线控制算法SARSA

​		SARSA算法是一种使用时序差分求解强化学习控制问题的方法。强化学习控制问题可以表述为：给定强化学习的5个要素：状态集$S$，动作集$A$，即时奖励$R$，衰减因子$\lambda$，探索率$\epsilon$,求解最优的动作价值函数$q_{\star}$和最优策略$\pi_{\star}$。

​		这一类强化学习的问题求解不需要环境的状态转化模型，是不基于模型的强化学习问题求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新当前的策略，再通过新的策略，来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。

​		时序差分法的控制问题可以分为两类，一类是在线控制，一直使用一个策略来更新价值函数和选择新的动作；另一类是离线控制，会使用两个策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。

​		SARSA算法属于在线控制的这一类，其一直使用$\epsilon$-贪婪法来更新价值函数和选择新的动作。

### SARSA算法概述

作为SARSA算法的名字本身来说，它实际上是由S,A,R,S,A几个字母组成的。而S,A,R分别代表状态（State），动作(Action),奖励(Reward)，这也是我们前面一直在使用的符号。这个流程体现在下图：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302191342861.webp)



在迭代的时候，我们首先基于$\epsilon$-贪婪法在当前状态$S$选择一个动作$A$，这样环境会转到一个新的状态$S’$,同时返回一个即时奖励$R$，在新的状态$S’$,我们会基于$\epsilon$-贪婪法在状态$S’$选择一个动作$A'$，但是我们并不会执行这个动作$A'$，只是用来更新价值函数。价值函数的更新公式为：
$$
Q(S, A)=Q(S, A)+\alpha\left(R+\gamma Q\left(S^{\prime}, A^{\prime}\right)-Q(S, A)\right)
$$
其中，$\gamma$是是衰减因子，$\alpha$是迭代步长,其值一般需要随着迭代的进行逐渐变小，这样才能保证动作价值函数$Q$可以收敛。



### SARSA算法流程

算法输入：迭代轮数$T$，状态集$S$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$,

输出：所有的状态和动作对应的价值Q

1. 随机初始化所有的状态和动作对应的价值$Q$. 对于终止状态其$Q$值初始化为0.

2. for i from 1 to T，进行迭代。

   a) 初始化$S$为当前状态序列的第一个状态。设置$A$为$\epsilon$-贪婪法在当前状态$S$选择的动作。

   b) 在状态$S$执行当前动作A,得到新状态$S'$和奖励$R$

   c) 用$\epsilon$-贪婪法在状态$S'$选择新的动作$A’$

   d) 更新价值函数$Q(S,A)$:
   $$
   Q(S, A)=Q(S, A)+\alpha\left(R+\gamma Q\left(S^{\prime}, A^{\prime}\right)-Q(S, A)\right)
   $$
   e) $S=S'$,$A=A'$

   f）如果$S'$是终止状态，则当前轮迭代完毕，否则转到步骤b)

需要注意的是，对于SARSA算法而言，价值函数更新时使用的$A'$将作为下一轮迭代开始时的执行动作$A$。

### 总结

SARSA算法和动态规划相比，不需要知道环境的状态转移模型；和蒙特卡洛法相比，不需要完整的状态序列，因此比较灵活。在传统的强化学习方法中使用比较广泛。

SARSA算法也有一个传统强化学习方法共有的问题，就是无法求解太复杂的问题。在SARSA算法中，$Q(S,A)$的值使用一张大表来存储，如果状态和动作都达到百万乃至千万级，需要在内存里保存的这张大表会超级大，甚至溢出，因此不是很适合解决规模很大的问题。当然，对于不是特别复杂的问题，使用SARSA还是很不错的一种强化学习问题求解方法。

## （7）时序差分离线控制算法Q-Learning

时序差分的控制问题可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作，如SARSA算法；而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数，如Q-Learning算法。

对于Q-Learning，我们会使用$\epsilon$-贪婪法来选择新的动作，这部分和SARSA完全相同。但是对于价值函数的更新，Q-Learning使用的是贪婪法，而不是SARSA的$\epsilon$-贪婪法。这一点就是SARSA和Q-Learning本质的区别。

### Q-Learning 算法概述

Q-Learning算法的拓补图入下图所示：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302191342571.webp)

首先我们基于状态$S$，用$\epsilon$-贪婪法选择到动作$A$, 然后执行动作$A$，得到奖励$R$，并进入状态$S'$，此时，如果是SARSA，会继续基于状态$S'$，用$\epsilon$-贪婪法选择$A'$,然后来更新价值函数。但是Q-Learning则不同。对于Q-Learning，它基于状态$S'$，没有使用$\epsilon$贪婪法选择$A'$，而是使用贪婪法选择$A'$，也就是说，选择使$Q(S',a)$最大的$a$作为$A'$来更新价值函数。用数学公式表示就是：
$$
Q(S, A)=Q(S, A)+\alpha\left(R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right)
$$
对应到上图中就是在图下方的三个黑圆圈动作中选择一个使$Q(S',a)$最大的动作作为$A'$。

此时选择的动作只会参与价值函数的更新，不会真正的执行。价值函数更新后，新的执行动作需要基于状态$S'$，用$\epsilon$-贪婪法重新选择得到。这一点也和SARSA稍有不同。对于SARSA，价值函数更新使用的$A'$会作为下一阶段开始时候的执行动作。

### Q-Learning算法流程

算法输入：迭代轮数$T$，状态集$S$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$,

输出：所有的状态和动作对应的价值Q

1. 随机初始化所有的状态和动作对应的价值$Q$. 对于终止状态其$Q$值初始化为0.

2. for i from 1 to T，进行迭代。

   a) 初始化$S$为当前状态序列的第一个状态。

   b) 用$\epsilon$-贪婪法在当前状态$S$中选择出动作$A$

   c) 在状态$S$执行当前动作$A$,得到新状态$S’$和奖励$R$

   d) 更新价值函数$Q(S,A)$:
   $$
   Q(S, A)=Q(S, A)+\alpha\left(R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right)
   $$
   e) $S=S'$

   f) 如果$S'$是终止状态，则当前轮迭代完毕，否则转到步骤b)

### 与SARSA比较

1. Q-Learning直接学习的就是最优策略，而SARSA在学习最优策略的同时还在做探索。这导致学习最优策略的时候如果用SARSA，为了保证收敛，需要指定一个策略使得$\epsilon$-贪婪法的超参数$\epsilon$在迭代的过程中逐渐变小，而Q-Learning没有这个要求。
2. Q-Learning直接学习最优策略，但是最优策略会依赖于训练中产生的一系列数据，所以受样本数据的影响很大，甚至会影响Q函数的收敛。Deep Q-learning同样存在这个问题。
3. SARSA在收敛的过程中鼓励探索，这样学习过程会比较平滑，不至于过于激进。Q-learning则会遇到一些特殊的"最优"陷阱，例如经典的强化学习问题"Cliff Walk"

### 结论

1. SARSA与Q-Learning使用了同为时序差分算法，但是在更新价值函数时，一个和探索时采用相同策略，一个采用不同策略。
2. Q-Learning也面临着动作空间和状态空间变大时，所需维护的Q表非常巨大的问题，这同样限制了它的应用场景。



## （8）状态函数的近似表示与Deep Q-Learning

### 为何需要价值函数的近似表示

之前的强化学习求解方法，无论是动态规划DP,蒙特卡洛方法，还是时序差分TD，使用的状态都是离散的有限个状态集合$S$。此时的问题规模比较小，比较容易求解。但是如果遇道复杂的状态集合，或者是连续的状态，就算进行了离散化的操作，集合也很大。此时如Q-Learning这样的传统方法，没有办法在内存中维护这么大的一张Q表。因此对状态函数的近似表示是我们必须要做的一个工作。

### 价值函数的近似表示

由于问题的状态集合规模大，一个可行的建模方法是价值函数的近似表示。方法是我们引入一个状态价值函数$\hat{v}$, 这个函数由参数$w$描述，并接受状态$s$作为输入，计算后得到状态$s$的价值，即我们期望：
$$
{\hat{v}}(s,w)\approx v_{\pi}(s)
$$
类似的，引入一个动作价值函数$\hat{q}$，这个函数由参数$w$描述，并接受状态$s$与动作$a$作为输入，计算后得到动作价值，即我们期望：
$$
\hat{q}(s,a,w)\approx q_{\pi}(s,a)
$$
价值函数近似的方法很多，比如最简单的线性表示法，用$\phi(s)$表示状态$s$的特征向量，则此时我们的状态价值函数可以近似表示为：
$$
\hat{v}(s,w)=\phi(s)^{T}w
$$
当然，除了线性表示法，还可以用决策树，最近邻，傅里叶变换，神经网络来表达我们的状态价值函数。而最常见，应用最广泛的表示方法是神经网络。对于神经网络，可以使用DNN，CNN或者RNN。没有特别的限制。如果把我们计算价值函数的神经网络看做一个黑盒子，那么整个近似过程可以看做下面这三种情况：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302201436889.webp)

1. 对于状态价值函数，神经网络的输入是状态$s$的特征向量，输出是状态价值${\hat{v}}(s,w)$

2. 对于动作价值函数，有两种方法

   a) 一种是输入状态$s$的特征向量和动作$a$，输出对应的动作价值${\hat{q}}(s,a,w)$

   b) 另一种是只输入状态$s$的特征向量，动作集合有多少个动作就有多少个输出${\hat{q}}(s,a_{i},w)$,这种方式隐含的要求是动作是有限个的离散动作。

### Deep Q-Learning算法思路

Deep Q-Learning算法的基本思路来源于Q-Learning。但是和Q-Learning不同的地方在于，它的Q值的计算不是直接通过状态值$s$和动作$a$来计算，而是通过上面讲到的Q网络来计算的。这个Q网络是一个神经网络，我们一般简称Deep Q-Learning为DQN。

DQN的输入是我们的状态s对应的状态向量$\phi(s)$， 输出是所有动作在该状态下的动作价值函数$Q$。Q网络可以是DNN，CNN或者RNN，没有具体的网络结构要求。

DQN主要使用的技巧是经验回放（*experience replay*）,即将每次和环境交互得到的奖励与状态更新情况都保存起来，用于后面目标Q值的更新。为什么需要经验回放呢？我们回忆一下Q-Learning，它是有一张Q表来保存所有的Q值的当前结果的，但是DQN是没有的，那么在做动作价值函数更新的时候，就需要其他的方法，这个方法就是经验回放。

通过经验回放得到的目标Q值和通过Q网络计算的Q值肯定是有误差的，那么我们可以通过梯度的反向传播来更新神经网络的参数$w$，当$w$收敛后，我们的就得到的近似的Q值计算方法，进而贪婪策略也就求出来了。

### Deep Q-Learning算法流程

算法输入：迭代轮数$T$，状态特征维度$n$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$, Q网络结构, 批量梯度下降的样本数$m$

输出：Q网络参数

1. 随机初始化Q网络的所有参数$w$，基于$w$初始化所有的状态和动作对应的价值$Q$。清空经验回放的集合$D$。

2.  for i from 1 to T，进行迭代。

   a) 初始化$S$为当前状态序列的第一个状态, 拿到其特征向量$\phi(S)$

   b) 在Q网络中使用$\phi(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用$\epsilon$-贪婪法在当前$Q$值输出中选择对应的动作$A$

   c) 在状态$S$执行当前动作$A$,得到新状态$S'$,得到新状态$S'$对应的特征向量$\phi(S')$和奖励$R$, 是否为终止状态is_end

   d) 将$\{\phi(S),A,R,\phi(S'),is\_end\}$这个五元组存入经验回放集合$D$

   e) $S=S'$

   f) 从经验回放集合$D$中采样$m$个样本$\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2......,m$，计算当前目标Q值$y_{j}$
   $$
   y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q(\phi(S'_j),A'_j,w) & {is\_end_j \;is\; false} \end{cases}
   $$
   g) 使用均方差损失函数$\frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数$w$

   h) 如果$S'$是终止状态，则当前迭代轮完毕，否则转到步骤b)

   注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$\epsilon$需要随着迭代的进行而变小。

### 总结

1. DQN由于对价值函数做了近似表示，因此有了解决大规模强化学习问题的能力。
2. DQN通过经验回放的方式来计算目标Q值，通过梯度反向传播来更新Q网络的所有参数$w$
3. DQN不一定能保证Q网络的收敛。

## （9）Nature DQN

### DQN(NIPS 2013)的问题

在DQN(NIPS 2013)中，使用的目标Q值的计算式子为：
$$
y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q(\phi(S'_j),A'_j,w) & {is\_end_j \;is\; false} \end{cases}
$$
这里目标Q值的计算使用了当前要训练的Q网络参数来计算$Q(\phi(S'_j),A'_j,w)$,但是$y_{i}$后续又要用于更新Q网络的参数。这样两者循环依赖，迭代起来两者的相关性太强了，不利于算法的收敛。

因此Nature DQN被提出来尝试用两个Q网络来减少目标Q值计算和要更新Q网络参数之间的依赖关系。

### Nature DQN的建模

Nature DQN使用了两个Q网络，一个当前Q网络$Q$用来选择动作，更新模型参数，另一个目标Q网络$Q'$用于计算目标Q值。目标Q网络的网络参数不需要迭代更新，而是每隔一段时间从当前Q网络$Q$复制过来，即延时更新，这样可以减少目标Q值和当前的Q值相关性。

要注意的是，两个Q网络的结构是一模一样的。这样才可以复制网络参数。

Nature DQN和上一篇的DQN相比，除了用一个新的相同结构的目标Q网络来计算目标Q值以外，其余部分基本是完全相同的。

### Nature DQN算法流程

算法输入：迭代轮数$T$，状态特征维度$n$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$, 当前Q网络$Q$，目标Q网络$Q'$, 批量梯度下降的样本数$m$,目标Q网络参数更新频率$C$。

输出：Q网络参数

1. 随机初始化所有的状态和动作对应的价值$Q$。 随机初始化当前Q网络的所有参数$w$,初始化目标Q网络$Q'$的参数$w'=w$。清空经验回放的集合$D$。

2. for i from i to T, 进行迭代

   a) 初始化S为当前状态序列的第一个状态，得到其特征向量$\phi(S)$

   b) 在Q网络中使用$\phi(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用$\epsilon$-贪婪法在当前Q值输出中选择对应的动作$A$
   
   c) 在状态$S$执行当前动作$A$,得到新状态$S'$,得到新状态$S'$对应的特征向量$\phi(S')$和奖励$R$, 是否为终止状态is_end
   
   d) 将$\{\phi(S),A,R,\phi(S'),is\_end\}$这个五元组存入经验回放集合$D$
   
   e) $S=S'$
   
   f) 从经验回放集合$D$中采样$m$个样本$\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2......,m$，计算当前目标Q值$y_{j}$
   $$
   y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q'(\phi(S'_j),A'_j,w') & {is\_end_j \;is\; false} \end{cases}
   $$
   g) 使用均方差损失函数$\frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数$w$
   
   h) 如果i%C=1,则更新目标Q网络参数$w'=w$
   
   i) 如果$S'$是终止状态，则当前迭代轮完毕，否则转到步骤b)
   
   注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$\epsilon$需要随着迭代的进行而变小。

### 总结

Nature DQN对DQN NIPS 2013做了相关性方面的改进，这个改进虽然不错，但是仍然没有解决DQN的 很多问题，比如：

1. 目标Q值的计算是否准确？全部通过max Q来计算有没有问题？
2. 随机采样的方法好吗？按道理不同样本的重要性是不一样的。
3. Q值代表状态，动作的价值，那么单独动作价值的评估会不会更准确？

第一个问题对应的改进是Double DQN, 第二个问题的改进是Prioritised Replay DQN，第三个问题的改进是Dueling DQN。之后会讨论。

## (10) Double DQN

在Nature DQN的算法流程，它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性。但Nature DQN还存在过估计的问题。

### DQN的目标Q值计算问题

在DDQN之前，基本上所有的目标Q值都是通过贪婪法直接得到的，无论是Q-Learning， DQN(NIPS 2013)还是 Nature DQN，都是如此。比如对于Nature DQN,虽然用了两个Q网络并使用目标Q网络计算Q值，其第j个样本的目标Q值的计算还是贪婪法得到的，计算入下式：
$$
y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q'(\phi(S'_j),A'_j,w') & {is\_end_j \;is\; false} \end{cases}
$$
使用max虽然可以快速让Q值向可能的优化目标靠拢，但是很容易过犹不及，导致过度估计(Over Estimation)，所谓过度估计就是最终我们得到的算法模型有很大的偏差(bias)。为了解决这个问题， DDQN通过解耦目标Q值动作的选择和目标Q值的计算这两步，来达到消除过度估计的问题。

### DDQN的算法建模

DDQN和Nature DQN一样，也有一样的两个Q网络结构。在Nature DQN的基础上，通过解耦目标Q值动作的选择和目标Q值的计算这两步，来消除过度估计的问题。

在上一节里，Nature DQN对于非终止状态，其目标Q值的计算式子是：
$$
y_j= R_j + \gamma\max_{a'}Q'(\phi(S'_j),A'_j,w')
$$
在DDQN这里，不再是直接在目标Q网络里面找各个动作中最大Q值，而是先在当前Q网络中先找出最大Q值对应的动作，即
$$
a^{max}(S'_j, w) = \arg\max_{a'}Q(\phi(S'_j),a,w)
$$
然后利用这个选择出来的动作$a^{max}(S'_j, w)$在目标网络里面去计算目标Q值。即：
$$
y_j = R_j + \gamma Q'(\phi(S'_j),a^{max}(S'_j, w),w')
$$
综合起来写就是：
$$
y_j = R_j + \gamma Q'(\phi(S'_j),\arg\max_{a'}Q(\phi(S'_j),a,w),w')
$$
除了目标Q值的计算方式以外，DDQN算法和Nature DQN的算法流程完全相同。

### DDQN算法流程

这里我们总结下DDQN的算法流程，和Nature DQN的区别仅仅在步骤2.f中目标Q值的计算。

算法输入：迭代轮数$T$，状态特征维度$n$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$, 当前Q网络$Q$，目标Q网络$Q'$, 批量梯度下降的样本数$m$,目标Q网络参数更新频率$C$。

输出：Q网络参数

1. 随机初始化所有的状态和动作对应的价值$Q$。 随机初始化当前Q网络的所有参数$w$,初始化目标Q网络$Q'$的参数$w'=w$。清空经验回放的集合$D$。

2. for i from i to T, 进行迭代

   a) 初始化S为当前状态序列的第一个状态，得到其特征向量$\phi(S)$

   b) 在Q网络中使用$\phi(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用$\epsilon$-贪婪法在当前Q值输出中选择对应的动作$A$

   c) 在状态$S$执行当前动作$A$,得到新状态$S'$,得到新状态$S'$对应的特征向量$\phi(S')$和奖励$R$, 是否为终止状态is_end

   d) 将$\{\phi(S),A,R,\phi(S'),is\_end\}$这个五元组存入经验回放集合$D$

   e) $S=S'$

   f) 从经验回放集合$D$中采样$m$个样本$\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2......,m$，计算当前目标Q值$y_{j}$
   $$
   y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma Q'(\phi(S'_j),\arg\max_{a'}Q(\phi(S'_j),a,w),w')& {is\_end_j\; is \;false} \end{cases}
   $$
   g) 使用均方差损失函数$\frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数$w$

   h) 如果i%C=1,则更新目标Q网络参数$w'=w$

   i) 如果$S'$是终止状态，则当前迭代轮完毕，否则转到步骤b)

   注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$\epsilon$需要随着迭代的进行而变小。

## 深度学习原理

## pytorch框架的使用
