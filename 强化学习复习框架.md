

# 强化学习复习框架

# 第一部分 准备知识

## 微积分相关

### 导数

一元函数$f(x)$在某处沿$x$轴正方向的变化率

### 偏导数

多元函数$y=f(x_{1},x_{2},...,x_{n})$在某点处沿某一坐标轴$(x_{1}、x_{2}、...x_{n})$正方向的变化率$\frac{\partial f}{\partial x_{i}} $

### 方向导数

多元函数$y=f(x_{1},x_{2},...,x_{n})$在某点处沿某个方向的导数值

### 梯度

梯度是多元函数$y=f(x_{1},x_{2},...,x_{n})$在某点处的最大方向导数，该函数在该点处沿梯度方向有最大变化率。

以二元函数$f(x,y)$为例，在某一点$P(x_0,y_0)$处，梯度是一个向量，表示函数$f(x,y)$在点$P$处增加最快的方向和速率。梯度的记法为$\nabla f(x_0,y_0)$，它的表达式为：
$$
\nabla f(x_0,y_0) = \frac{\partial f}{\partial x}(x_0,y_0)\vec{i}+\frac{\partial f}{\partial y}(x_0,y_0)\vec{j}
$$
其中$\frac{\partial f}{\partial x}(x_0,y_0)$和$\frac{\partial f}{\partial y}(x_0,y_0)$分别表示函数$f(x,y)$在点$P(x_0,y_0)$处的偏导数，$\vec{i}$和$\vec{j}$分别是$x$轴和$y$轴的单位向量。

梯度的模长表示函数在该点的变化率最大值，即梯度的模长$|\nabla f(x_0,y_0)|$等于该点处的方向导数的最大值$D_{\vec{v}}f(x_0,y_0)$，且该最大值出现在梯度的方向上。

总结起来就是梯度的方向是各个变量轴方向向量的线性组合，梯度的模长即表示函数在该点变化率的最大值。

### 链式法则

链式法则是微积分中的一个重要概念，用于计算复合函数的导数。

如果$y$是由$x$的函数$y=f(u)$和$u$的函数$u=g(x)$所组成的复合函数$y=f(g(x))$，则根据链式法则，该复合函数的导数可以表示为：

$$\frac{dy}{dx}=\frac{dy}{du}\cdot\frac{du}{dx}$$

其中$\frac{dy}{du}$表示函数$y=f(u)$对$u$的导数，$\frac{du}{dx}$表示函数$u=g(x)$对$x$的导数。链式法则的意义在于，复合函数的导数等于内层函数对自变量的导数与外层函数对内层函数的导数的乘积。

链式法则可以扩展到多元函数和多层复合函数，例如对于三个函数$f(u,v)$，$u(x,y)$和$v(x,y)$组成的复合函数$z=f(u,v)$，其中$u$和$v$均为$x$和$y$的函数，链式法则可以表示为：

$$\frac{\partial z}{\partial x}=\frac{\partial z}{\partial u}\cdot\frac{\partial u}{\partial x}+\frac{\partial z}{\partial v}\cdot\frac{\partial v}{\partial x}$$

$$\frac{\partial z}{\partial y}=\frac{\partial z}{\partial u}\cdot\frac{\partial u}{\partial y}+\frac{\partial z}{\partial v}\cdot\frac{\partial v}{\partial y}$$

## 线性代数相关

### 向量的几何意义

在平面和三维空间中，向量可以表示为有向线段，具有大小和方向的特点。向量的加法表示平移和合成，向量的数乘表示缩放和反向。

### 矩阵的几何意义

1. 矩阵的列向量表示坐标轴方向

矩阵可以看做是一个向量空间到另一个向量空间的映射，其中的列向量就代表了原向量空间的基向量在新向量空间中的坐标，而矩阵的列数就是新向量空间的维数。例如，一个 2x2 的矩阵可以看做是将二维空间中的向量映射到另一个二维空间中的变换，矩阵的两个列向量就代表了新坐标系下的两个坐标轴方向。

2. 矩阵的行向量表示坐标系系数

矩阵的行向量可以表示原向量空间的基向量在新向量空间中的系数，也就是坐标系的缩放和旋转参数。例如，一个旋转矩阵可以将一个向量绕原点旋转一定的角度，旋转矩阵的行向量就表示了旋转后的坐标系在原坐标系中的系数。

3. 矩阵的列空间表示向量空间的范围

矩阵的列空间是由矩阵的列向量所张成的向量空间，它代表了矩阵的映射结果的所有可能取值。例如，一个二维变换矩阵的列空间代表了所有在二维空间中可能出现的向量。

4. 矩阵的行空间表示映射后的向量的方向

矩阵的行空间是由矩阵的行向量所张成的向量空间，它代表了映射后的向量所处的方向。例如，一个投影矩阵的行空间代表了所有被投影到的方向。

综上所述，矩阵的几何意义主要体现在矩阵的列向量、行向量、列空间和行空间中。通过矩阵的几何意义，可以更加深入地理解矩阵和线性变换之间的关系，并更好地应用矩阵来解决实际问题。

### 矩阵的秩

矩阵的秩是一个重要的概念，它可以告诉我们矩阵中所包含的信息和矩阵的性质。在线性代数中，矩阵的秩是指矩阵中非零行的最大数目，也就是矩阵中线性无关的行向量的个数。

具体来说，对于一个 $m \times n$ 的矩阵 $A$，如果矩阵 $A$ 的某些行向量可以被其他行向量线性表示出来，那么这些行向量就是线性相关的。如果矩阵 $A$ 的所有行向量都是线性无关的，那么矩阵 $A$ 的秩就是 $m$。如果矩阵 $A$ 的一些行向量是线性相关的，那么这些行向量可以被其他行向量线性表示出来，因此可以将它们从矩阵 $A$ 中删去，得到一个新的矩阵，新矩阵的秩比原矩阵的秩小。通过这样的方法，我们可以一步步删去线性相关的行向量，直到所有的行向量都是线性无关的，此时得到的矩阵的秩就是原矩阵的秩。

矩阵的秩有很多重要的应用，例如：

1. 判断矩阵的列向量是否线性无关，从而判断矩阵是否可逆。
2. 判断线性方程组的解的个数，如果矩阵的秩小于等于变量个数，那么线性方程组可能有无数解或无解；如果矩阵的秩等于变量个数，那么线性方程组有唯一解。
3. 判断矩阵的奇异性，如果矩阵的秩小于矩阵的行数或列数，那么矩阵是奇异的；如果矩阵的秩等于矩阵的行数或列数，那么矩阵是非奇异的。

总之，矩阵的秩是线性代数中一个非常重要的概念，它可以告诉我们矩阵的线性无关性、可逆性、解的个数等信息，对于许多问题的求解都有着重要的作用。

### 范数

第一范数和第二范数是向量的两种范数计算方式。

对于一个 $n$ 维向量 $x=(x_1,x_2,\cdots,x_n)$，其 $p$ 阶范数（$p$-norm）定义为：
$$
||x||_{p}=\left(\sum_{i=1}^{n}|x_{i}|^{p}\right)^{\frac{1}{p}}
$$
其中 $p$ 取不同的值会得到不同的范数，其中 $p=1$ 和 $p=2$ 的范数分别称为第一范数和第二范数。

第一范数：$||x||_{1}=|x_{1}|+|x_{2}|+\cdot\cdot\cdot+|x_{n}|$

第二范数：$\|x\|_{2}=\sqrt{x_{1}^{2}+x_{2}^{2}+\cdots+x_{n}^{2}}$

相比于第一范数，第二范数更注重向量的大小，因为它对向量的每一个元素都取平方，同时对较大的元素赋予了更大的权重，因此第二范数也被称为欧几里得范数。

在机器学习和优化领域，第一范数和第二范数被广泛应用于正则化，特别是在 Lasso 和 Ridge 回归中。在 Lasso 回归中，使用第一范数作为正则化项可以促使模型系数变得稀疏，从而实现特征选择的目的；在 Ridge 回归中，使用第二范数作为正则化项可以防止模型过拟合，提高模型的泛化性能。

### 逆矩阵的求解方法

要求一个矩阵的逆矩阵，可以使用高斯-约旦消元法（Gaussian-Jordan elimination）或LU分解等方法，其中高斯-约旦消元法是比较常用的一种方法。具体来说，对于一个 $n \times n$ 的矩阵 $A$，我们可以将其与一个 $n \times n$ 的单位矩阵 $I$ 组合成一个 $n \times 2n$ 的矩阵：

$$[A|I]$$

然后对矩阵 $[A|I]$ 进行一系列的行变换，使得矩阵 $A$ 变为单位矩阵，这样矩阵 $I$ 就变为了矩阵 $A$ 的逆矩阵。如果矩阵 $A$ 没有逆矩阵，则在进行高斯-约旦消元时会出现某个步骤无法进行或产生矛盾，此时我们就可以判断矩阵 $A$ 没有逆矩阵。

具体来说，高斯-约旦消元的过程如下：

1. 对于矩阵 $[A|I]$，从第一行开始，找到第一个非零元素所在的列，设其为第 $j$ 列。
2. 将第一行的第 $j$ 列元素除以其值，使其变为 1。然后将第一行加上第 $k$ 行的 $(-a_{kj})$ 倍，使得第 $k$ 行的第 $j$ 列元素变为 0。
3. 重复以上步骤，直到将矩阵 $A$ 变为单位矩阵为止。此时矩阵 $I$ 就是矩阵 $A$ 的逆矩阵。

需要注意的是，如果矩阵 $A$ 的行列式为 0，则该矩阵没有逆矩阵。这是因为，如果矩阵 $A$ 没有逆矩阵，那么对于任意的向量 $\mathbf{b}$，方程组 $A\mathbf{x} = \mathbf{b}$ 都无法求解，也就是说，矩阵 $A$ 的行列式为 0 意味着矩阵 $A$ 的行向量或列向量中存在线性相关的向量，从而矩阵 $A$ 的秩不满足要求。

因此，判断一个矩阵是否有逆矩阵，可以先计算其行列式，如果行列式为 0，则该矩阵没有逆矩阵；如果行列式不为 0，则可以使用上述方法求解其逆矩阵。

### 特征值和特征向量

### 矩阵分解的应用

## 概率论与数理统计相关

### 最大似然估计

最大似然估计（Maximum Likelihood Estimation，简称MLE）是一种常用的参数估计方法。该方法的核心思想是：在已知一组观测数据的情况下，寻找一个参数值，使得该参数值下观测到这组数据的概率最大。

具体来说，假设有一个随机变量$X$，其概率分布为$p(x \mid \theta)$，其中$\theta$表示待估计的参数。现在观测到了一组独立同分布的样本${x_{1},x_{2},...x_{n}}$，希望利用这些样本来估计参数$\theta$的值。根据最大似然估计的原理，可以通过最大化样本的联合概率密度函数来求得参数θ的值，即
$$
θ_{MLE} = argmax(p(x_{1}, x_{2}, ..., x_{n} \mid θ))
$$
将样本的联合概率密度函数展开为各个样本的概率密度函数的乘积，即
$$
p(x_{1},x_{2},...,x_{n} \mid \theta) = \prod_{i=1}^{n} p(x_{i} \mid \theta )
$$
则最大似然估计的问题转化为了一个求解极大值的问题，即
$$
θ_{MLE} = argmax(\prod_{i=1}^{n} p(x_{i} \mid \theta ))
$$
极大似然估计就是要求解使得似然函数$L(\theta)$最大的参数$\theta$。一般来说，求解极大似然估计需要以下步骤：

1. 写出似然函数$L(\theta)=\prod_{i=1}^{n} p(x_{i} \mid \theta )$
2. 对似然函数取对数，即$ln(L(\theta))$，这样可以将乘积变为求和，且对数具有单调性，不影响最优解。
3. 对$\theta$求偏导数，并令其等于0，得到似然方程。
4. 求解似然方程，得到参数$\theta$的极大似然估计值。

在实际应用中，有时候似然函数不容易求解或求解过于复杂，这时候可以采用优化算法，如牛顿法、拟牛顿法、梯度下降等，来求解似然函数的最大值。

需要注意的是，极大似然估计只是一种估计方法，其估计结果不一定是真实参数的确切值。同时，最大化似然函数有可能导致过拟合问题，因此在实际应用中需要结合数据的特点和领域知识进行判断和调整。

最大似然估计是一种经典的参数估计方法，具有很好的理论性质和广泛的应用。它可以用来估计各种不同的参数，如均值、方差、概率分布的参数等。同时，最大似然估计也是其他统计学方法的基础，如假设检验、置信区间等。

### 大数定律

大数定律（Law of Large Numbers）是概率论中的一个重要定理，指的是在独立重复试验中，随着试验次数的增加，样本均值逐渐收敛于其期望值的概率趋近于1的现象。换句话说，当样本量足够大时，样本均值会越来越接近于总体均值。

大数定律分为弱大数定律和强大数定律两种形式：

1. 弱大数定律：样本均值与总体均值之间的差异随着样本数量的增加而逐渐减小，但不一定收敛于0。
2. 强大数定律：样本均值与总体均值之间的差异随着样本数量的增加而收敛于0，即样本均值几乎必定收敛于总体均值。

大数定律是概率论中的一个基本定理，它在很多实际问题中都有着广泛的应用，例如在统计学、金融学、物理学、工程学等领域。大数定律的应用可以帮助我们理解随机现象的规律性，对于科学研究和工程实践具有重要的意义。

### 中心极限定理

中心极限定理（Central Limit Theorem）是概率论中的一个重要定理，指的是当样本量足够大时，样本均值的分布会逐渐趋近于正态分布。中心极限定理的基本思想是，将大量相互独立的随机变量进行加和后，其分布趋近于正态分布。

具体来说，中心极限定理有以下两种形式：

1. 切比雪夫形式的中心极限定理：对于任意的分布，只要其方差存在，则随机变量的标准化和服从于标准正态分布。
2. 林德伯格-列维形式的中心极限定理：对于相互独立、同分布的随机变量序列，它们的和的分布会趋近于正态分布。

中心极限定理是概率论中的一个重要定理，它表明在很多情况下，大量相互独立的随机变量的和的分布趋近于正态分布。中心极限定理在统计学、金融学、物理学、工程学等领域都有着广泛的应用，例如在抽样调查、假设检验、时间序列分析等方面都有着重要的应用。

### 全概率公式

在讲全概率公式之前，首先要理解什么是“样本空间的划分”【又称“完备事件群”。】 

我们将满足

1. $A_{1},A_{2},A_{3},...,A_{n}$是一组两两互斥的事件；
2. $A_{1} \cup A_{2} \cup A_{3}...\cup A_{n}=\Omega$

这样的一组事件称为一个“完备事件群”。简而言之，就是事件之间**两两互斥**，所有事件的**并集**是**整个样本空间**（必然事件）。

定理：一般地，设$A_{1},A_{2},...,A_{n}$是一组两两互斥的事件，$A_{1} \cup A_{2} \cup A_{3}...\cup A_{n}=\Omega$,

且$P(A_{i})>0,i=1,2,...,n$

则对任意的事件$B\subseteq \Omega$,有

$P(B)=P(A_{1})P(B \mid A_{1})+P(A_{2})P(B \mid A_{2})+...+P(A_{n})P(B \mid A_{n})=\sum_{i=1}^{n}P(A_{i})P(B \mid A_{i})$

<img src="https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202303261056912.webp" style="zoom:67%;" />

总结：

1. 全概率公式的主要用处在于它可以将一个复杂事件的概率计算问题，分解为若干个简单事件的概率计算问题，最后应用概率的可加性求出最终结果。
2. 全概率公式问题的关键是找到划分（完备事件组）
3. 在实际应用下，这个划分$A_{1},A_{2},A_{3},...,A_{n}$是导致$B$发生的各种可能原因，$P(B)$是事件$B$分别在这些原因下发生的条件概率的加权平均。

### 贝叶斯概率

设实验$E$的样本空间为$\Omega$,$B$为事件，$A_{1},A_{2},...,A_{n}$为$\Omega$的一个划分，且$P(B)>0,P(A_{i})>0(i=1,2,...n)$

则：
$$
P(A_{i} \mid B)=\frac{P(A_{i}B)}{P(B)}=\frac{P(B \mid A_{i})P(A_{i})}{P(B)}
\\
\\
P(A_{i} \mid B)=\frac{P(B \mid A_{i})P(A_{i})}{\sum_{j=1}^{n}P(B \mid A_{j})P(A_{j})},i=1,2,...n
$$
$P(A)$是$A$的先验概率。

$P(B)$是$B$的先验概率。

先验概率指的是在考虑任何新证据之前，我们根据以前的知识、经验或假设来对事件发生的概率进行估计。它是在没有任何新信息或证据的情况下，我们对事件概率的初始估计。例如，如果我们要预测某个人的身高，我们可以使用人口普查数据中身高的先验概率分布来估计这个人身高的概率分布。

$P(B \mid A)$被称为似然度，似然度指的是在给定某个参数值的前提下，数据发生的概度。在这个公式内，指的是当事件$A$发生的前提下，事件$B$发生的概率。在贝叶斯统计中，似然度是一个关键的组成部分，因为它是计算后验概率的基础。

$P(A \mid B)$被称为后验概率。后验概率则是在考虑了新的证据或信息后，我们对事件概率进行修正或更新的概率。它是在获得新信息或证据之后，我们对事件概率的更新估计。例如，在通过医学检查后，根据先验概率和测试结果可以计算出患病的后验概率。

贝叶斯概率的意义有：

1. 综合了主观先验和实证数据：贝叶斯概率允许我们将主观先验知识和实证数据进行有机地结合，从而得到更加准确的概率估计。这是一个重要的优势，因为在许多实际问题中，我们往往会面临着缺乏数据、数据不完整或数据不可靠的情况。
2. 能够对不确定性进行量化：贝叶斯概率能够对不确定性进行量化，即对我们对事件发生的不确定性进行描述。这对于许多实际问题来说非常重要，因为在现实生活中，我们经常面临着不确定性和风险，并需要对它们进行有效地管理和控制。
3. 能够提供决策支持：贝叶斯概率能够为我们提供决策支持，帮助我们在不确定性的情况下做出更好的决策。它可以帮助我们确定最优的决策，或者在风险管理和控制方面提供重要的信息和建议。
4. 能够提高模型的预测精度：贝叶斯概率可以通过利用先验知识和实证数据对模型进行修正和调整，从而提高模型的预测精度。这对于许多应用领域来说都是至关重要的，例如金融、医疗、工程等。

## 优化理论相关

### 随机梯度下降

### 最小二乘法

### 牛顿法

### 优化器

## 信息学相关

### 信息量

在信息论中，信息量是一个消息中所包含的信息的度量。它表示了一个消息所提供的新信息的大小或不确定性的减少程度。一般来说，如果一条消息提供了更多的信息，那么它的信息量就会更大；反之，如果一条消息提供的信息已经被人所知，那么它的信息量就会很小。

一个随机变量的每个可能取值都有一个对应的信息量，其大小取决于该取值出现的概率。对于一个取值为x的离散随机变量，其信息量可以用以下公式计算：
$$
I(x) = -log_{2}(P(x))
$$
其中，$P(x)$是该取值的概率，可以看出，当一个事件的概率越小，它所包含的信息量就越大，这是因为在这种情况下该事件的出现是不太可能的，因此它提供的信息量就越大。

信息量在通信和数据处理中非常重要，它可以被用来评估信道的容量、设计数据压缩算法、进行错误纠正等。

### 信息熵

信息熵是信息论中的一个概念，它是用来描述一段信息中包含的信息量的大小。在信息论中，信息是指通过某个信道传输的数据，它可以被表示为一系列离散的符号，比如二进制编码中的0和1。信息熵是衡量这些符号中包含的信息量的度量，也被称为平均不确定度或平均信息量。

具体来说，信息熵是一个随机变量的不确定性的度量。它可以被定义为该随机变量所有可能取值的信息量的加权平均，其中权重是每个取值出现的概率。也就是说，如果一个随机变量可以取到多个可能的值，并且每个值出现的概率不同，那么它的信息熵就是所有可能值的信息量的加权平均。

综上所述，信息熵$H(X)$，它代表一个随机变量$X$所有可能取值的自信息$I(x)$的加权求和：
$$
H(X)=\int_{X} I(x) d x=\int_{X}-\log (P(x)) P(x) d x
$$


### 相对熵

相对熵（也称为KL散度）是信息论中一种用于衡量两个概率分布之间差异的度量。它衡量的是当我们使用一个分布来近似另一个分布时所需要额外的信息量。

设$P(x),Q(x)$是随机变量$X$上的两个概率分布，则在离散和连续随机变量的情形下，相对熵的定义分别为
$$
KL(P||Q)=\sum P(x)log \frac{P(x)}{Q(x)}
\\
KL(P||Q) = \int P(x) log \frac{P(x)}{Q(x)} dx
$$


 上的两个概率分布，则在离散和连续随机变量的情形下，相对熵的定义分别为

相对熵的值越小，表示两个概率分布越相似；反之，相对熵的值越大，表示两个概率分布越不相似。

需要注意的是，相对熵是不对称的,也就是说$KL(P||Q)$和$KL(Q||P)$通常是不相等的。

### 交叉熵

交叉熵是信息论中一种常用的度量方法，通常用于比较两个概率分布P与Q之间的差异，其中P表示真实分布，Q表示非真实分布。在机器学习中，交叉熵常被用作损失函数，用于衡量预测结果与真实结果之间的差异，从而优化模型参数。

在离散和连续随机变量的情形下，交叉熵的计算方式分别如下：
$$
H(P,Q)=\sum_{x}P(x)log(\frac{1}{Q(x)})
\\
H(P,Q)=-\int_{x}P(x)logQ(x)dx=E_{x \sim P}[-logQ]
$$


## 机器学习相关

### 损失函数

#### 均方误差

均方误差（Mean Squared Error，MSE）是一种衡量模型预测值与实际值之间差异的损失函数。在机器学习和统计学中，MSE常常被用作回归问题的损失函数。

MSE的计算公式为：
$$
\mathrm{MSE}=\frac{1}{n} \sum_{i=1}^{n}\left(Y_{i}-\hat{Y}_{i}\right)^{2}
$$
其中，n为样本数量，yᵢ为第i个样本的真实值，ŷᵢ为模型对第i个样本的预测值。

直观来说，MSE计算了预测值与真实值之间的平均误差的平方。MSE值越小，说明模型的预测结果越接近真实值。在训练模型时，通常使用梯度下降等优化方法最小化MSE损失函数，以调整模型的参数使预测结果更准确。

需要注意的是，MSE只适用于连续型的预测值，对于分类问题或者离散型的预测值，需要使用其他的损失函数，比如交叉熵损失函数。

#### 绝对平均误差

#### 交叉熵

#### 对数损失

#### Hinge损失函数

### 反向传播算法

### 正则化方法

在机器学习中，正则化是一种用于防止过拟合的常用方法。过拟合指的是模型在训练集上表现很好，但在测试集上表现很差的现象。正则化通过对模型的复杂度进行限制，可以有效避免过拟合问题。

机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作$L_{1}$-norm和$L_2$-norm，中文称作$L_1$正则化 和$L_2$正则化，或者 $L_1$范数 和 L_2范数。

#### L1正则化

L1正则化也叫做Lasso正则化，它是在损失函数中加入L1范数的惩罚项。L1范数是指向量中各个元素的绝对值之和。L1正则化的主要作用是可以将模型中一些不重要的特征的系数逐渐变为0，从而实现特征的选择和模型的简化。L1正则化的效果比较明显，可以得到稀疏解，但是对于特征数比样本数多的问题，容易得到不稳定解。

假设有如下带L1正则化的损失函数：
$$
J=J_{0}+\alpha \sum_{w}|w|
$$




#### L2正则化

#### Dropout正则化

### 分类问题

### 回归问题

### Batch Normlization

### 模型评估方法

#### 留出法

留出法即Holdout检验是最简单也是最直接的验证方法， 它将原始的样本集合随机划分成训练集和验证集两部分。 比方说， 我们把样本按照70%～30% 的比例分成两部分， 70% 的样本用于模型训练；30% 的样本用于模型验证，包括绘制ROC曲线、 计算精确率和召回率等指标来评估模型性能。

Holdout检验的缺点很明显， 即在验证集上计算出来的最后评估指标与原始分组有很大关系。 为了消除随机性， 研究者们引入了“交叉检验”的思想。

####  k折交叉验证

K折交叉验证（K-fold cross-validation）是一种常用的机器学习模型评估方法。它将原始数据集分成K个子集，其中一个子集作为验证集，剩下的K-1个子集作为训练集，然后用训练集来训练模型，在验证集上评估模型的性能指标。这个过程重复K次，每次选择不同的验证集，然后将K个评估结果的平均值作为模型的最终性能指标。

K折交叉验证的具体步骤如下：

1. 将原始数据集随机分成K个子集。
2. 对于每个子集i，将其作为验证集，将其他K-1个子集作为训练集，用训练集来训练模型，在验证集上评估模型的性能指标。
3. 重复步骤2 K次，每次选择不同的验证集。
4. 将K个评估结果的平均值作为模型的最终性能指标。

K折交叉验证可以有效地评估机器学习模型的性能，因为它充分利用了数据集中的信息，并且能够有效地避免过拟合的问题。此外，K折交叉验证还可以用来选择最优的超参数，比如正则化系数、学习率等，以进一步提高模型的性能。

#### 自助法

自助法（bootstrapping）是一种用于模型评估和参数估计的统计方法。它通过从原始数据集中随机抽样生成多个新的数据集，对这些新数据集进行模型训练和评估，从而得到一个对模型性能或参数的稳健估计。

<img src="https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202303251433373.webp" style="zoom:50%;" />

它的主要思想是通过有放回地从样本中随机抽取若干个样本来构建新的训练集，并使用这些训练集来训练模型并进行评估。

自助法的主要步骤如下：

1. 从原始数据集中随机抽取一个样本，将其放入一个新的数据集中，再把该样本放回原始数据集中。
2. 重复以上步骤，直到新的数据集的大小等于原始数据集的大小。
3. 使用新的数据集来训练模型，并在原始数据集上进行评估。

由于每次抽取样本都是有放回的，因此在新的数据集中，有些样本可能会出现多次，而有些样本则可能完全没有出现。这种方法可以有效地利用原始数据集中的信息，从而提高模型的性能评估准确度。

自助法的优点是能够有效地利用有限的数据集，并且能够评估模型在不同数据集上的性能稳定性。但是它的缺点是需要生成多个不同的训练集，因此计算量比较大，并且生成的训练集中可能会包含与原始数据集相同的样本，从而导致模型的泛化能力下降。

#### 测试集和验证集的区别

在机器学习中，通常将数据集划分为三个部分：训练集、验证集和测试集。训练集用来训练模型，验证集用来调节模型的超参数，测试集用来评估模型的性能。

验证集和测试集的区别在于它们的使用方式和目的不同。

验证集是用来调节模型的超参数的。当我们训练一个机器学习模型时，通常需要调节一些超参数，比如正则化系数、学习率等，以达到最优的性能。我们可以通过在验证集上训练和评估不同的模型，选择最优的超参数组合来调节模型。验证集通常是从训练集中划分出来的一部分数据，它的规模应该足够大，以确保能够准确地评估模型的性能，但是不应该太大，否则会浪费大量的数据。

测试集是用来评估模型的性能的。当我们调节好模型的超参数后，需要使用测试集来评估模型的性能。测试集通常是从原始数据集中划分出来的一部分数据，它的规模应该足够大，以确保能够准确地评估模型的性能，但是不应该与训练集和验证集重叠，否则会导致模型对测试集过拟合。

总的来说，训练集用来训练模型，验证集用来调节模型的超参数，测试集用来评估模型的性能。验证集和测试集的划分应该遵循相同的分布，以确保对模型的性能进行公正的评估。

### 模型评估指标

#### 准确率

在机器学习和统计学中，准确率（Accuracy）是指分类器正确识别出的样本数占总样本数的比例。可以用下面的公式表示：

Accuracy = (True Positive + True Negative) / (True Positive + False Positive + True Negative + False Negative)

其中，True Positive表示分类器将正样本正确地识别为正样本的数量，False Positive表示分类器将负样本错误地识别为正样本的数量，True Negative表示分类器将负样本正确地识别为负样本的数量，False Negative表示分类器将正样本错误地识别为负样本的数量。

准确率是评估分类器性能的重要指标之一，它描述了分类器的整体正确率。准确率越高，表示分类器的性能越好，但是在样本不平衡的情况下，准确率可能会出现偏差，因此需要结合其他指标进行评估。

#### 召回率

在机器学习和信息检索中，召回率（Recall）是指在所有实际正样本中，分类器正确识别出的正样本数占比。可以用下面的公式表示：

Recall = True Positive / (True Positive + False Negative)

其中，True Positive表示分类器将正样本正确地识别为正样本的数量，False Negative表示分类器将正样本错误地识别为负样本的数量。

召回率描述了分类器对正样本的识别能力，即分类器能够识别多少个正样本。召回率越高，表示分类器对正样本的覆盖率越好，但可能会导致假阳性率增加。召回率越低，表示分类器漏识别的正样本数量越多，可能会导致假阴性率增加。因此，在实际应用中，需要根据具体情况综合考虑召回率和精确率等指标来评估分类器的性能。

#### 为什么要结合准确率和召回率

准确率和召回率是机器学习中常用的两个指标，它们分别描述了分类器的正确率和识别能力。在实际应用中，往往需要综合考虑准确率和召回率等多个指标来评估分类器的性能，因为仅仅使用一个指标来评估可能会产生误导。

具体地说，仅考虑准确率会忽略分类器在处理不平衡样本数据时的表现，因为当负样本比正样本多很多时，分类器只需要把所有样本都预测为负样本，就可以得到很高的准确率，但是这样做显然是没有意义的。此时，召回率就可以帮助我们评估分类器对正样本的覆盖率，从而更全面地评估分类器的性能。

相反地，仅考虑召回率也会带来问题。例如，一个分类器可以将所有样本都预测为正样本，这样可以得到完美的召回率，但显然是没有意义的。此时，准确率就可以帮助我们评估分类器对所有样本的正确识别能力。

因此，综合考虑准确率和召回率等多个指标可以更全面地评估分类器的性能，并帮助我们选择最合适的分类器。在实际应用中，还可以根据不同的需求和场景选择适合的评估指标。

#### F1分数

F1分数是一个综合考虑准确率和召回率的评估指标，它是准确率和召回率的调和平均值，可以用以下公式表示：

F1 = 2 * (precision * recall) / (precision + recall)

其中，precision表示精确率，recall表示召回率。F1分数的取值范围是0到1之间，当F1分数越接近1时，表示分类器的性能越好。

F1分数综合了准确率和召回率两个指标，对于不同的问题，有不同的侧重点。对于一些问题，精确率更重要，例如股票交易中的“买入”决策，需要保证决策的正确性；而对于一些问题，召回率更重要，例如疾病诊断，需要保证尽可能多的病人被正确诊断。

F1分数可以帮助我们综合考虑精确率和召回率，从而更全面地评估分类器的性能。在实际应用中，可以根据不同的问题和场景选择适合的评估指标，或者综合使用多个评估指标来评估分类器的性能。

### 聚类算法

### 决策树

### 支持向量机

### 贝叶斯

## 图相关

### 邻接矩阵相乘

## 马尔科夫性质

马尔科夫性质是指一个过程的未来状态只依赖于当前状态，而不依赖于过去的状态。具体地说，如果一个随机过程满足在给定当前状态下，未来状态的概率分布仅仅依赖于当前状态，那么这个过程就满足马尔科夫性质。

在马尔科夫过程中，状态空间和状态转移矩阵是两个核心概念。状态空间是指所有可能的状态的集合，状态转移矩阵描述了从一个状态到另一个状态的概率分布。通过状态空间和状态转移矩阵，我们可以计算出任何时刻系统处于某个状态的概率分布，以及系统在未来任意时刻的状态分布。这些概率分布的计算可以通过马尔科夫链的转移矩阵的幂次来实现。

## 马尔科夫决策过程

# 第二部分 经典强化学习

## 马尔科夫决策过程（MDP）

### 强化学习需要引入MDP的原因

在强化学习中，存在一个环境的状态转移模型，它可以表示为一个概率模型，即在状态$s$下采取动作$a$，转移到下一个状态$s'$的概率，表示为$P_{ss'}^{a}$

为了降低状态转移模型的复杂性，我们假设状态转移的马尔科夫性，即假设转移到下一个状态$s'$的概率仅与上一个状态$s$有关,与之前的状态无关。用公式表示为：
$$
P_{s s^{\prime}}^{a}=P\left(S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right)
$$

### Return（回报）

在强化学习中，回报被定义为是未来Reward序列的一些特定函数，用来体现长期收益。

最简单的情况下，回报被定义为是Reward的总和：

$G_t=R_{t+1}+R_{t+2}+R_{t+3}+......==\sum_{k=0}^{\infty } R_{t+k+1}$

折扣回报(*Discount Reward*)的形式则为：

$G_t=R_{t+1}+\gamma R_{t+2}+ \gamma^2 R_{t+3}+......=\sum_{k=0}^{\infty } \gamma ^k R_{t+k+1}$

引入折扣因子$\gamma \in [0,1]$的原因有很多，其主要原因是：

- 首先，数学上的方便表示，在带环的马尔可夫过程中，可以避免陷入无限循环，达到收敛。
- 其次，随着时间的推移，远期利益的不确定性越来越大，符合人类对于眼前利益的追求。（一百万年之后中100万与今天中100万）
- 再次，在金融上，近期的奖励可能比远期的奖励获得更多的利息，越早获得奖励，收益越高。

###  Value Function（价值函数）

1. 我们把策略$\pi$下的价值函数记为$v_\pi (s)$,代表从状态$s$开始，智能体根据策略$\pi$进行决策所获得的回报$G_{t}$的期望值。对于MDP，可以正式定义$v_\pi(s)$为


$$
v_\pi(s)=E_{\pi}[G_{t}|S_{t}=s]=E_{\pi}[\sum_{k=0}^{\infty } \gamma ^k R_{t+k+1}|S_t=s]
$$

​	我们把函数$v_\pi(s)$称为策略$\pi$的状态价值函数（*state-value function*）

2. 我们把策略$\pi$下在状态$s$时采取动作$a$的价值记为$q_{\pi}(s,a)$,代表从状态$s$开始，执行动作$a$之后，智能体根据策略$\pi$进行决策所获得的回报的期望值，表示为：

$$
q_{\pi}(s,a)=E_{\pi}[G_{t}|S_{t}=s,A_{t}=a]=E_{\pi}[\sum_{k=0}^{\infty } \gamma ^k R_{t+k+1}|S_t=s,A_t=a]
$$

​	我们把函数$q_\pi(s,a)$称为策略$\pi$的动作价值函数（*action-value function*）

### 贝尔曼方程与价值函数的递推关系

根据价值函数的表达式，可以得到以下推导：
$$
\begin{aligned}
v_{\pi}(s) & =\mathbb{E}_{\pi}\left(R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s\right) \\
& =\mathbb{E}_{\pi}\left(R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right) \mid S_{t}=s\right) \\
& =\mathbb{E}_{\pi}\left(R_{t+1}+\gamma G_{t+1} \mid S_{t}=s\right) \\
& =\mathbb{E}_{\pi}\left(R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s\right)
\end{aligned}
$$
注意第三步到第四步之间能推导是因为回报$G_t$的期望等于回报$G_t$的期望（即$v_{\pi}\left(S_{t+1}\right)$)的期望。

这个递推关系式告诉我们，**一个状态的价值由该状态的奖励以及后续的状态价值按一定的衰减比例联合而成。**

同样的方法可以得到动作价值函数$q_{\pi}(s,a)$的递推关系式：
$$
q_{\pi}(s, a)=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) \mid S_{t}=s, A_{t}=a\right)
$$

<img src="https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302191341954.webp" style="zoom:25%;" />

这张图表示的是马尔科夫决策过程的一个状态转移关系。系统一开始处于状态$s$，随机性策略$\pi(a|s)$表示状态$s$下选择动作$a$的概率。状态$s$时采取动作$a$，环境给与一个Reward $r$,之后系统会依据状态转移概率$P_{ss'}^{a}$转移到一个新的状态$s'$。

由此可以得到状态价值函数$v_{\pi}(s)$与动作价值函数$q_{\pi}(s,a)$相互之间的推导关系
$$
v_{\pi}(s)=\sum_{a\in A}[\pi(a|s)q_{\pi}(s,a)]
$$

$$
q_{\pi}(s,a)=r+\gamma \sum_{s'\in S}[P_{s s^{\prime}}^{a}\cdot v_{\pi}(s')]
$$

其中$S$和$A$分别表示状态集和动作集。

所以有
$$
v_{\pi}(s)=\sum_{a\in A}[\pi(a|s)q_{\pi}(s,a)]=\sum_{a\in A} \left[\pi(a|s)\left(r+\gamma \sum_{s'\in S}P_{s s^{\prime}}^{a} v_{\pi}(s')\right)\right]
$$
该式被称为$v_{\pi}(s)的$贝尔曼方程（*Bellman Equation*），它用等式表示了状态价值$v_{\pi}(s)$与后继状态价值$v_{\pi}(s')$之间的递推关系。

### 总结

这一部分讨论了

1. 使用马尔科夫假设来简化强化学习模型的复杂度
2. 明确了强化学习中状态价值函数（*state-value function*）与动作价值函数（*action-value function*）的概念。
3. 通过推导得到了状态价值$v_{\pi}(s)$与后继状态价值$v_{\pi}(s')$之间的递推关系，称为贝尔曼方程（*Bellman Equation*）。

## 用动态规划(DP)求解

### 策略评估

求解给定策略的状态价值函数，这个问题的求解过程我们称其为策略评估（*Policy Evaluation*）。

策略评估的基本思路是从一个状态价值函数$v_{\pi}(s)$出发，使用贝尔曼方程，依据给定的策略$\pi$，状态转移概率$P_{ss'}^{a}$和奖励$r$来更新状态价值函数$v_{\pi}(s)$

假设我们在第$k$轮迭代已经计算出了所有的状态的状态价值，那么在第$k+1$轮我们可以利用第$k$轮计算出的状态价值计算出第$k+1$轮的状态价值。通过贝尔曼方程来完成,即：
$$
v_{k+1}(s)=\sum_{a \in A} \pi(a \mid s)\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{k}\left(s^{\prime}\right)\right)
$$
和上一节的式子唯一的区别是由于我们的策略$\pi$已经给定，我们不再写出，对应加上了迭代轮数的下标。我们每一轮可以对计算得到的新的状态价值函数再次进行迭代，直至状态价值的值改变很小(收敛)，那么我们就得出了问题的解，即给定策略的状态价值函数$v_{\pi}$。可以理解为是得到了采取该策略情况下，各个状态的价值。

### 策略改进

策略评估的目的是帮助搜索好的策略。假设我们已经得到了一个策略$\pi$的状态价值函数$v_{\pi}$，那我们如果不根据$a=\pi(s)$来选择动作，而能找到一个策略$\pi'$,

如果对于两个确定性策略$\pi$,$\pi'$,如果以下不等式成立：
$$
\forall s \in \mathcal{S}, q_{\pi}\left(s, \pi^{\prime}(s)\right)>=v_{\pi}(s)
$$
则以下不等式也必然成立：
$$
\forall s \in \mathcal{S}, v_{\pi^{\prime}}(s)>=v_{\pi}(s)
$$
也就是只要保证前一个不等式成立，就能保证策略$\pi'$一定不会比策略$\pi$差。

显然，如果我们根据策略$\pi$的动作价值函数来选择动作，即：
$$
\pi^{\prime}(s) = \underset{a}{\arg \max }[ q_{\pi}(s, a)]
$$
(注，这部分的详细推导需要参考Sutton教科书)

则这个新策略一定是满足之前的不等式的，因此必然不会比原始策略差。这种基于贪婪的方法改进策略的方法称之为策略改进（policy improvement）

### 策略迭代

一旦一个策略$\pi$根据$v_{\pi}$通过**策略改进**得到了新的策略$\pi'$，可以接着使用**策略评估**得到其价值函数$v_{\pi'}$，然后再通过**策略改进**得到新的策略$\pi'’$。。。。。。这样的迭代方法可以得到一个不断改进的策略和价值函数的序列。这种寻找最优策略的方法叫做策略迭代。

事实上，采用基于价值函数的贪心策略，策略改进定理确保了采用这种方法每次得到的新策略都不会比原始策略差，这也就起到了通过迭代改进策略的作用。

### 总结

采用动态规划的方法逐步迭代得到更优的策略。

这种策略迭代的本质我认为是不断更新状态价值函数，状态价值函数的变化会导致策略的变化，也就是所谓的得到了新策略。

其缺点是策略评估需要遍历所有状态，计算开销其实是非常昂贵的。

## 用蒙特卡洛法(MC)求解

动态规划可以用来帮助做强化学习中的策略迭代工作。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的状态转化模型$P$都无法知道，这时动态规划法根本没法使用。蒙特卡洛方法由此被提出。

### 不基于模型的强化学习问题

在动态规划法中，模型的状态转移概率矩阵$P$始终是已知的，即MDP已知，我们一般将其称为基于模型的强化学习问题。

但是有很多强化学习问题，我们没有办法事先得知模型状态转移概率矩阵$P$，这种问题，如蒙特卡洛法，就是不基于模型的强化学习问题。

### 蒙特卡洛求解特点

蒙特卡罗法通过采样若干经历完整的状态序列(*episode*)来估计状态的真实价值。所谓的经历完整，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，我们就可以来近似的估计状态价值，进而求解预测和控制问题了。

从蒙特卡罗法的特点来说，一是和动态规划比，它不需要依赖于模型状态转化概率。二是它从经历过的完整序列学习，完整的经历越多，学习效果越好。

### 策略评估

在蒙特卡洛法中，一个给定策略$\pi$的包含T个状态的完整序列如下：
$$
S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, \ldots S_{t}, A_{t}, R_{t+1}, \ldots R_{T}, S_{T}
$$
而在马尔科夫决策过程(MDP)中,对于价值函数$v_{\pi}(s)$的定义为：
$$
v_{\pi}(s)=\mathbb{E}_{\pi}\left(G_{t} \mid S_{t}=s\right)=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s\right)
$$
可以看出每个状态的价值函数等于所有该状态收获的期望，同时这个收获是通过后续的奖励与对应的衰减乘积求和得到。那么对于蒙特卡罗法来说，如果要求某一个状态的状态价值，只需要**求出所有的完整序列中该状态出现时候的收获再取平均值即可近似求解**，也就是：
$$
\begin{aligned}
G_{t}= & R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \gamma^{T-t-1} R_{T} \\
& v_{\pi}(s) \approx \operatorname{average}\left(G_{t}\right), \text { s.t. } S_{t}=s
\end{aligned}
$$

### 策略迭代

蒙特卡罗法求解控制问题的思路和动态规划价值迭代的的思路类似。在动态规划价值迭代的的思路中， 每轮迭代先做策略评估，计算出价值$v_{k}(s)$，然后基于据一定的方法（比如贪婪法）更新当前策略$\pi$。最后得到最优价值函数$v_{\star}$和最优策略$\pi_{\star}$。

蒙特卡洛方法与动态规划相比，主要有两点不同。一是蒙特卡洛法一般是优化最优动作价值函数$q_{\star}$而不是状态价值函数$v_{\star}$。二是动态规划一般基于贪婪法更新策略，而蒙特卡洛法一般采用$\epsilon $-贪婪法更新策略。

$\epsilon $-贪婪法通过设置一个较小的$\epsilon$值，使用$1-\epsilon$的概率贪婪地选择目前认为是最大行为价值的相位，使用$\epsilon$的概率随机地从所有m个可选行为中选取行为，用公式可以表示为：
$$
\pi(a|s)=\left\{\begin{array}{l l}{{\epsilon/m+1-\epsilon}}&{{i f\,a^{*}=\arg\operatorname*{max}_{a\in A}Q(s,a)}}\\ {{\epsilon/m}}&{{e l s e}}\end{array}\right.
$$
在实际求解控制问题时，为了使算法可以收敛，一般$\epsilon$会随着算法的迭代过程逐渐减小，并趋于0。这样在迭代前期，我们鼓励探索，而在后期，由于我们有了足够的探索量，开始趋于保守，以贪婪为主，使算法可以稳定收敛。这样我们可以得到一张和动态规划类似的图：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302191342734.webp)

### 蒙特卡洛算法流程

在这里总结下蒙特卡罗法求解强化学习控制问题的算法流程，这里的算法是在线(on-policy)版本的,相对的算法还有离线(off-policy)版本的。同时这里我们用的是every-visit,即个状态序列中每次出现的相同状态，都会计算对应的收获值。

在线蒙特卡罗法求解强化学习控制问题的算法流程如下:

输入：状态集$S$, 动作集$A$, 即时奖励$A$，衰减因子$\gamma$, 探索率$\epsilon$

输出：最优的动作价值函数$q_{\star}$和最优策略$\pi_{\star}$

1. 初始化所有的动作价值函数$Q(s,a)=0$,状态次数$N(s,a)=0$,采样次数$k=0$,随机初始化一个策略$\pi$。
2. $k=k+1$，基于策略$\pi$进行第$k$次蒙特卡洛采样，得到一个完整的状态序列：

$$
S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, \ldots S_{t}, A_{t}, R_{t+1}, \ldots R_{T}, S_{T}
$$

3. 对于该状态序列中出现的每一状态行为对$(S_t,A_t)$，计算其回报$G_t$，更新其计数$N(s,a)$和行为价值函数$Q(s,a)$:

$$
\begin{array}{c}{{G t=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\dots\gamma^{T-t-1}R_{T}}}\\ {{{}}}\\
{{N(S_{t},A_{t})=N(S_{t},A_{t})+1}}\\ {{{}}}\\ 
{{Q(S_{t},A_{t})=Q(S_{t},A_{t})+\frac{1}{N(S_{t},A_{t})}(G_{t}-Q(S_{t},A_{t}))}}\end{array}
$$

4. 基于新计算出的动作价值，更新当前的$\epsilon$-贪婪策略：
   $$
   \begin{array}{l}{{\epsilon=\frac{1}{k}}}\\ {{{}}}\\
   {{\pi(a|s)=\left\{\begin{array}{l l}{{\epsilon/m+1-\epsilon}}&{{i f\,a^{*}=\arg\operatorname*{max}_{a\in A}Q(s,a)}}\\ {{\epsilon/m}}&{{e l s e}}\end{array}\right.}}\end{array}
   $$
   
5. 如果所有的$Q(s,a)$收敛，则对应的所有$Q(s,a)$即为最优的动作价值函数$q_{\star}$。对应的策略$\pi(a|s)$即为最优策略$\pi_{\star}$。否则转到第二步。

### 总结

蒙特卡洛方法是第一个不基于模型的强化学习求解方法。它避开了动态规划计算过于复杂，同时也不需要事先知道环境的状态转移模型，可以用于处理海量的数据和模型。它的缺点在于每次采样都需要一个完整的状态序列，如果没有完整的状态序列，或者很难拿到较多的完整的状态序列，这时候蒙特卡罗法就不太好用了。

## 用时序差分法（TD）求解

时序差分法和蒙特卡洛法类似，都是不基于模型的强化学习问题求解方法。

### 与蒙特卡洛法的差异点

1. 在蒙特卡洛法中，回报$G_{t}$的计算需要完整的状态序列。当没有完整序列的情况下，时序差分法可以近似地求解某个状态的回报。

​	马尔科夫决策过程中的贝尔曼方程为
$$
v_{\pi}(s)=\mathbb{E}_{\pi}(R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s)
$$
​	这启发我们可以用$R_{t+1}+\gamma v_{\pi}(S_{t+1})$来近似地代替回报$G_{t}$，我们一般把$R_{t+1}+\gamma V(S_{t+1})$称为TD目标值，$R_{t+1}+\gamma V(S_{t+1})-V(S_{t})$称为TD误差。这样我们	只需要两个连续的状态和对应的奖励，就可以尝试求解强化学习问题了。

​	因此，在时序差分法中，回报的表达式为：
$$
G(t)=R_{t+1}+\gamma V(S_{t+1})
$$

2. 在时序差分法中，由于我们没有完整的序列，也就是没有对应的次数$N(S_{t})$，一般就用一个[0, 1]的系数$\alpha$代替。迭代的式子可以表示为：
   $$
   \begin{array}{c}{{V(S_{t})=V(S_{t})+\alpha(G_{t}-V(S_{t}))}}\\ {{{}}}\\ {{Q(S_{t},A_{t})=Q(S_{t},A_{t})+\alpha(G_{t}-Q(S_{t},A_{t}))}}\end{array}
   $$

3. 时序差分法在知道结果之前就可以学习，也可以在没有结果时学习，还可以在持续进行的环境中学习，而蒙特卡罗法则要等到最后结果才能学习，时序差分法可以更快速灵活的更新状态的价值估计，这在某些情况下有着非常重要的实际意义。
4. 时序差分法在更新状态价值时使用的是TD 目标值，即基于即时奖励和下一状态的预估价值来替代当前状态在状态序列结束时可能得到的收获，是当前状态价值的有偏估计，而蒙特卡罗法则使用实际的收获来更新状态价值，是某一策略下状态价值的无偏估计，这一点蒙特卡罗法占优。
5. 虽然时序差分法得到的价值是有偏估计，但是其方差却比蒙特卡罗法得到的方差要低，且对初始值敏感，通常比蒙特卡罗法更加高效。

### n步时序差分

在单步的时序差分算法中，使用$R_{t+1}+\gamma V(S_{t+1})$来近似替代回报$G_{t}$，即向前一步来近似回报$G_{t}$。由此可以想到，也可以向前两步，此时回报$G_{t}$的近似表达式为:
$$
G_{t}^{(2)}=R_{t+1}+\gamma R_{t+2}+\gamma^{2}V(S_{t+2})
$$
从两步，到三步，再到n步，我们可以归纳出n步时序差分回报$G_{t}^{(n)}$表达式为：
$$
G_{t}^{(n)}=R_{t+1}+\gamma R_{t+2+...}+\gamma^{n-1}R_{t+n}+\gamma^{n}V(S_{t+n})
$$
当n越来越大，趋于无穷，或者说趋于使用完整的状态序列时，n步时序差分就等价于蒙特卡罗法了。

### 总结

时序差分和蒙特卡罗法比它更加灵活，不需要一个完整的序列，因此是目前主流的强化学习求解问题的方法。

## 时序差分在线控制算法SARSA

​		SARSA算法是一种使用时序差分求解强化学习控制问题的方法。强化学习控制问题可以表述为：给定强化学习的5个要素：状态集$S$，动作集$A$，即时奖励$R$，衰减因子$\lambda$，探索率$\epsilon$,求解最优的动作价值函数$q_{\star}$和最优策略$\pi_{\star}$。

​		这一类强化学习的问题求解不需要环境的状态转化模型，是不基于模型的强化学习问题求解方法。对于它的控制问题求解，和蒙特卡罗法类似，都是价值迭代，即通过价值函数的更新，来更新当前的策略，再通过新的策略，来产生新的状态和即时奖励，进而更新价值函数。一直进行下去，直到价值函数和策略都收敛。

​		时序差分法的控制问题可以分为两类，一类是在线控制，一直使用一个策略来更新价值函数和选择新的动作；另一类是离线控制，会使用两个策略，一个策略用于选择新的动作，另一个策略用于更新价值函数。

​		SARSA算法属于在线控制的这一类，其一直使用$\epsilon$-贪婪法来更新价值函数和选择新的动作。

### SARSA算法概述

作为SARSA算法的名字本身来说，它实际上是由S,A,R,S,A几个字母组成的。而S,A,R分别代表状态（State），动作(Action),奖励(Reward)，这也是我们前面一直在使用的符号。这个流程体现在下图：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302191342861.webp)



在迭代的时候，我们首先基于$\epsilon$-贪婪法在当前状态$S$选择一个动作$A$，这样环境会转到一个新的状态$S’$,同时返回一个即时奖励$R$，在新的状态$S’$,我们会基于$\epsilon$-贪婪法在状态$S’$选择一个动作$A'$，但是我们并不会执行这个动作$A'$，只是用来更新价值函数。价值函数的更新公式为：
$$
Q(S, A)=Q(S, A)+\alpha\left(R+\gamma Q\left(S^{\prime}, A^{\prime}\right)-Q(S, A)\right)
$$
其中，$\gamma$是是衰减因子，$\alpha$是迭代步长,其值一般需要随着迭代的进行逐渐变小，这样才能保证动作价值函数$Q$可以收敛。



### SARSA算法流程

算法输入：迭代轮数$T$，状态集$S$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$,

输出：所有的状态和动作对应的价值Q

1. 随机初始化所有的状态和动作对应的价值$Q$. 对于终止状态其$Q$值初始化为0.

2. for i from 1 to T，进行迭代。

   a) 初始化$S$为当前状态序列的第一个状态。设置$A$为$\epsilon$-贪婪法在当前状态$S$选择的动作。

   b) 在状态$S$执行当前动作A,得到新状态$S'$和奖励$R$

   c) 用$\epsilon$-贪婪法在状态$S'$选择新的动作$A’$

   d) 更新价值函数$Q(S,A)$:
   $$
   Q(S, A)=Q(S, A)+\alpha\left(R+\gamma Q\left(S^{\prime}, A^{\prime}\right)-Q(S, A)\right)
   $$
   e) $S=S'$,$A=A'$

   f）如果$S'$是终止状态，则当前轮迭代完毕，否则转到步骤b)

需要注意的是，对于SARSA算法而言，价值函数更新时使用的$A'$将作为下一轮迭代开始时的执行动作$A$。

### 总结

SARSA算法和动态规划相比，不需要知道环境的状态转移模型；和蒙特卡洛法相比，不需要完整的状态序列，因此比较灵活。在传统的强化学习方法中使用比较广泛。

SARSA算法也有一个传统强化学习方法共有的问题，就是无法求解太复杂的问题。在SARSA算法中，$Q(S,A)$的值使用一张大表来存储，如果状态和动作都达到百万乃至千万级，需要在内存里保存的这张大表会超级大，甚至溢出，因此不是很适合解决规模很大的问题。当然，对于不是特别复杂的问题，使用SARSA还是很不错的一种强化学习问题求解方法。

## 时序差分离线控制算法Q-Learning

时序差分的控制问题可以分为两类，一类是在线控制，即一直使用一个策略来更新价值函数和选择新的动作，如SARSA算法；而另一类是离线控制，会使用两个控制策略，一个策略用于选择新的动作，另一个策略用于更新价值函数，如Q-Learning算法。

对于Q-Learning，我们会使用$\epsilon$-贪婪法来选择新的动作，这部分和SARSA完全相同。但是对于价值函数的更新，Q-Learning使用的是贪婪法，而不是SARSA的$\epsilon$-贪婪法。这一点就是SARSA和Q-Learning本质的区别。

### Q-Learning 算法概述

Q-Learning算法的拓补图入下图所示：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302191342571.webp)

首先我们基于状态$S$，用$\epsilon$-贪婪法选择到动作$A$, 然后执行动作$A$，得到奖励$R$，并进入状态$S'$，此时，如果是SARSA，会继续基于状态$S'$，用$\epsilon$-贪婪法选择$A'$,然后来更新价值函数。但是Q-Learning则不同。对于Q-Learning，它基于状态$S'$，没有使用$\epsilon$贪婪法选择$A'$，而是使用贪婪法选择$A'$，也就是说，选择使$Q(S',a)$最大的$a$作为$A'$来更新价值函数。用数学公式表示就是：
$$
Q(S, A)=Q(S, A)+\alpha\left(R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right)
$$
对应到上图中就是在图下方的三个黑圆圈动作中选择一个使$Q(S',a)$最大的动作作为$A'$。

此时选择的动作只会参与价值函数的更新，不会真正的执行。价值函数更新后，新的执行动作需要基于状态$S'$，用$\epsilon$-贪婪法重新选择得到。这一点也和SARSA稍有不同。对于SARSA，价值函数更新使用的$A'$会作为下一阶段开始时候的执行动作。

### Q-Learning算法流程

算法输入：迭代轮数$T$，状态集$S$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$,

输出：所有的状态和动作对应的价值Q

1. 随机初始化所有的状态和动作对应的价值$Q$. 对于终止状态其$Q$值初始化为0.

2. for i from 1 to T，进行迭代。

   a) 初始化$S$为当前状态序列的第一个状态。

   b) 用$\epsilon$-贪婪法在当前状态$S$中选择出动作$A$

   c) 在状态$S$执行当前动作$A$,得到新状态$S’$和奖励$R$

   d) 更新价值函数$Q(S,A)$:
   $$
   Q(S, A)=Q(S, A)+\alpha\left(R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right)
   $$
   e) $S=S'$

   f) 如果$S'$是终止状态，则当前轮迭代完毕，否则转到步骤b)

### 与SARSA比较

1. Q-Learning直接学习的就是最优策略，而SARSA在学习最优策略的同时还在做探索。这导致学习最优策略的时候如果用SARSA，为了保证收敛，需要指定一个策略使得$\epsilon$-贪婪法的超参数$\epsilon$在迭代的过程中逐渐变小，而Q-Learning没有这个要求。
2. Q-Learning直接学习最优策略，但是最优策略会依赖于训练中产生的一系列数据，所以受样本数据的影响很大，甚至会影响Q函数的收敛。Deep Q-learning同样存在这个问题。
3. SARSA在收敛的过程中鼓励探索，这样学习过程会比较平滑，不至于过于激进。Q-learning则会遇到一些特殊的"最优"陷阱，例如经典的强化学习问题"Cliff Walk"

### 结论

1. SARSA与Q-Learning使用了同为时序差分算法，但是在更新价值函数时，一个和探索时采用相同策略，一个采用不同策略。
2. Q-Learning也面临着动作空间和状态空间变大时，所需维护的Q表非常巨大的问题，这同样限制了它的应用场景。

# 第三部分 深度强化学习

## 第一节 状态函数的近似表示与Deep Q-Learning

### 为何需要价值函数的近似表示

之前的强化学习求解方法，无论是动态规划DP,蒙特卡洛方法，还是时序差分TD，使用的状态都是离散的有限个状态集合$S$。此时的问题规模比较小，比较容易求解。但是如果遇道复杂的状态集合，或者是连续的状态，就算进行了离散化的操作，集合也很大。此时如Q-Learning这样的传统方法，没有办法在内存中维护这么大的一张Q表。因此对状态函数的近似表示是我们必须要做的一个工作。

### 价值函数的近似表示

由于问题的状态集合规模大，一个可行的建模方法是价值函数的近似表示。方法是我们引入一个状态价值函数$\hat{v}$, 这个函数由参数$w$描述，并接受状态$s$作为输入，计算后得到状态$s$的价值，即我们期望：
$$
{\hat{v}}(s,w)\approx v_{\pi}(s)
$$
类似的，引入一个动作价值函数$\hat{q}$，这个函数由参数$w$描述，并接受状态$s$与动作$a$作为输入，计算后得到动作价值，即我们期望：
$$
\hat{q}(s,a,w)\approx q_{\pi}(s,a)
$$
价值函数近似的方法很多，比如最简单的线性表示法，用$\phi(s)$表示状态$s$的特征向量，则此时我们的状态价值函数可以近似表示为：
$$
\hat{v}(s,w)=\phi(s)^{T}w
$$
当然，除了线性表示法，还可以用决策树，最近邻，傅里叶变换，神经网络来表达我们的状态价值函数。而最常见，应用最广泛的表示方法是神经网络。对于神经网络，可以使用DNN，CNN或者RNN。没有特别的限制。如果把我们计算价值函数的神经网络看做一个黑盒子，那么整个近似过程可以看做下面这三种情况：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302201436889.webp)

1. 对于状态价值函数，神经网络的输入是状态$s$的特征向量，输出是状态价值${\hat{v}}(s,w)$

2. 对于动作价值函数，有两种方法

   a) 一种是输入状态$s$的特征向量和动作$a$，输出对应的动作价值${\hat{q}}(s,a,w)$

   b) 另一种是只输入状态$s$的特征向量，动作集合有多少个动作就有多少个输出${\hat{q}}(s,a_{i},w)$,这种方式隐含的要求是动作是有限个的离散动作。

### Deep Q-Learning算法思路

Deep Q-Learning算法的基本思路来源于Q-Learning。但是和Q-Learning不同的地方在于，它的Q值的计算不是直接通过状态值$s$和动作$a$来计算，而是通过上面讲到的Q网络来计算的。这个Q网络是一个神经网络，我们一般简称Deep Q-Learning为DQN。

DQN的输入是我们的状态s对应的状态向量$\phi(s)$， 输出是所有动作在该状态下的动作价值函数$Q$。Q网络可以是DNN，CNN或者RNN，没有具体的网络结构要求。

DQN主要使用的技巧是经验回放（*experience replay*）,即将每次和环境交互得到的奖励与状态更新情况都保存起来，用于后面目标Q值的更新。为什么需要经验回放呢？我们回忆一下Q-Learning，它是有一张Q表来保存所有的Q值的当前结果的，但是DQN是没有的，那么在做动作价值函数更新的时候，就需要其他的方法，这个方法就是经验回放。

通过经验回放得到的目标Q值和通过Q网络计算的Q值肯定是有误差的，那么我们可以通过梯度的反向传播来更新神经网络的参数$w$，当$w$收敛后，我们的就得到的近似的Q值计算方法，进而贪婪策略也就求出来了。

### Deep Q-Learning算法流程

算法输入：迭代轮数$T$，状态特征维度$n$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$, Q网络结构, 批量梯度下降的样本数$m$

输出：Q网络参数

1. 随机初始化Q网络的所有参数$w$，基于$w$初始化所有的状态和动作对应的价值$Q$。清空经验回放的集合$D$。

2.  for i from 1 to T，进行迭代。

   a) 初始化$S$为当前状态序列的第一个状态, 拿到其特征向量$\phi(S)$

   b) 在Q网络中使用$\phi(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用$\epsilon$-贪婪法在当前$Q$值输出中选择对应的动作$A$

   c) 在状态$S$执行当前动作$A$,得到新状态$S'$,得到新状态$S'$对应的特征向量$\phi(S')$和奖励$R$, 是否为终止状态is_end

   d) 将$\{\phi(S),A,R,\phi(S'),is\_end\}$这个五元组存入经验回放集合$D$

   e) $S=S'$

   f) 从经验回放集合$D$中采样$m$个样本$\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2......,m$，计算当前目标Q值$y_{j}$
   $$
   y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q(\phi(S'_j),A'_j,w) & {is\_end_j \;is\; false} \end{cases}
   $$
   g) 使用均方差损失函数$\frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数$w$

   h) 如果$S'$是终止状态，则当前迭代轮完毕，否则转到步骤b)

   注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$\epsilon$需要随着迭代的进行而变小。

### 总结

1. DQN由于对价值函数做了近似表示，因此有了解决大规模强化学习问题的能力。
2. DQN通过经验回放的方式来计算目标Q值，通过梯度反向传播来更新Q网络的所有参数$w$
3. DQN不一定能保证Q网络的收敛。

## 第二节 Nature DQN

### DQN(NIPS 2013)的问题

在DQN(NIPS 2013)中，使用的目标Q值的计算式子为：
$$
y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q(\phi(S'_j),A'_j,w) & {is\_end_j \;is\; false} \end{cases}
$$
这里目标Q值的计算使用了当前要训练的Q网络参数来计算$Q(\phi(S'_j),A'_j,w)$,但是$y_{i}$后续又要用于更新Q网络的参数。这样两者循环依赖，迭代起来两者的相关性太强了，不利于算法的收敛。

因此Nature DQN被提出来尝试用两个Q网络来减少目标Q值计算和要更新Q网络参数之间的依赖关系。

### Nature DQN的建模

Nature DQN使用了两个Q网络，一个当前Q网络$Q$用来选择动作，更新模型参数，另一个目标Q网络$Q'$用于计算目标Q值。目标Q网络的网络参数不需要迭代更新，而是每隔一段时间从当前Q网络$Q$复制过来，即延时更新，这样可以减少目标Q值和当前的Q值相关性。

要注意的是，两个Q网络的结构是一模一样的。这样才可以复制网络参数。

Nature DQN和上一篇的DQN相比，除了用一个新的相同结构的目标Q网络来计算目标Q值以外，其余部分基本是完全相同的。

### Nature DQN算法流程

算法输入：迭代轮数$T$，状态特征维度$n$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$, 当前Q网络$Q$，目标Q网络$Q'$, 批量梯度下降的样本数$m$,目标Q网络参数更新频率$C$。

输出：Q网络参数

1. 随机初始化所有的状态和动作对应的价值$Q$。 随机初始化当前Q网络的所有参数$w$,初始化目标Q网络$Q'$的参数$w'=w$。清空经验回放的集合$D$。

2. for i from i to T, 进行迭代

   a) 初始化S为当前状态序列的第一个状态，得到其特征向量$\phi(S)$

   b) 在Q网络中使用$\phi(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用$\epsilon$-贪婪法在当前Q值输出中选择对应的动作$A$
   
   c) 在状态$S$执行当前动作$A$,得到新状态$S'$,得到新状态$S'$对应的特征向量$\phi(S')$和奖励$R$, 是否为终止状态is_end
   
   d) 将$\{\phi(S),A,R,\phi(S'),is\_end\}$这个五元组存入经验回放集合$D$
   
   e) $S=S'$
   
   f) 从经验回放集合$D$中采样$m$个样本$\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2......,m$，计算当前目标Q值$y_{j}$
   $$
   y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q'(\phi(S'_j),A'_j,w') & {is\_end_j \;is\; false} \end{cases}
   $$
   g) 使用均方差损失函数$\frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数$w$
   
   h) 如果i%C=1,则更新目标Q网络参数$w'=w$
   
   i) 如果$S'$是终止状态，则当前迭代轮完毕，否则转到步骤b)
   
   注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$\epsilon$需要随着迭代的进行而变小。

### 总结

Nature DQN对DQN NIPS 2013做了相关性方面的改进，这个改进虽然不错，但是仍然没有解决DQN的 很多问题，比如：

1. 目标Q值的计算是否准确？全部通过max Q来计算有没有问题？
2. 随机采样的方法好吗？按道理不同样本的重要性是不一样的。
3. Q值代表状态，动作的价值，那么单独动作价值的评估会不会更准确？

第一个问题对应的改进是Double DQN, 第二个问题的改进是Prioritised Replay DQN，第三个问题的改进是Dueling DQN。之后会讨论。

## 第三节 Double DQN

在Nature DQN的算法流程，它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性。但Nature DQN还存在过估计的问题。

### DQN的目标Q值计算问题

在DDQN之前，基本上所有的目标Q值都是通过贪婪法直接得到的，无论是Q-Learning， DQN(NIPS 2013)还是 Nature DQN，都是如此。比如对于Nature DQN,虽然用了两个Q网络并使用目标Q网络计算Q值，其第j个样本的目标Q值的计算还是贪婪法得到的，计算入下式：
$$
y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma\max_{a'}Q'(\phi(S'_j),A'_j,w') & {is\_end_j \;is\; false} \end{cases}
$$
使用max虽然可以快速让Q值向可能的优化目标靠拢，但是很容易过犹不及，导致过度估计(Over Estimation)，所谓过度估计就是最终我们得到的算法模型有很大的偏差(bias)。为了解决这个问题， DDQN通过解耦目标Q值动作的选择和目标Q值的计算这两步，来达到消除过度估计的问题。

### DDQN的算法建模

DDQN和Nature DQN一样，也有一样的两个Q网络结构。在Nature DQN的基础上，通过解耦目标Q值动作的选择和目标Q值的计算这两步，来消除过度估计的问题。

在上一节里，Nature DQN对于非终止状态，其目标Q值的计算式子是：
$$
y_j= R_j + \gamma\max_{a'}Q'(\phi(S'_j),A'_j,w')
$$
在DDQN这里，不再是直接在目标Q网络里面找各个动作中最大Q值，而是先在当前Q网络中先找出最大Q值对应的动作，即
$$
a^{max}(S'_j, w) = \arg\max_{a'}Q(\phi(S'_j),a,w)
$$
然后利用这个选择出来的动作$a^{max}(S'_j, w)$在目标网络里面去计算目标Q值。即：
$$
y_j = R_j + \gamma Q'(\phi(S'_j),a^{max}(S'_j, w),w')
$$
综合起来写就是：
$$
y_j = R_j + \gamma Q'(\phi(S'_j),\arg\max_{a'}Q(\phi(S'_j),a,w),w')
$$
除了目标Q值的计算方式以外，DDQN算法和Nature DQN的算法流程完全相同。

### DDQN算法流程

这里我们总结下DDQN的算法流程，和Nature DQN的区别仅仅在步骤2.f中目标Q值的计算。

算法输入：迭代轮数$T$，状态特征维度$n$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$, 当前Q网络$Q$，目标Q网络$Q'$, 批量梯度下降的样本数$m$,目标Q网络参数更新频率$C$。

输出：Q网络参数

1. 随机初始化所有的状态和动作对应的价值$Q$。 随机初始化当前Q网络的所有参数$w$,初始化目标Q网络$Q'$的参数$w'=w$。清空经验回放的集合$D$。

2. for i from i to T, 进行迭代

   a) 初始化S为当前状态序列的第一个状态，得到其特征向量$\phi(S)$

   b) 在Q网络中使用$\phi(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用$\epsilon$-贪婪法在当前Q值输出中选择对应的动作$A$

   c) 在状态$S$执行当前动作$A$,得到新状态$S'$,得到新状态$S'$对应的特征向量$\phi(S')$和奖励$R$, 是否为终止状态is_end

   d) 将$\{\phi(S),A,R,\phi(S'),is\_end\}$这个五元组存入经验回放集合$D$

   e) $S=S'$

   f) 从经验回放集合$D$中采样$m$个样本$\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2......,m$，计算当前目标Q值$y_{j}$
   $$
   y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma Q'(\phi(S'_j),\arg\max_{a'}Q(\phi(S'_j),a,w),w')& {is\_end_j\; is \;false} \end{cases}
   $$
   g) 使用均方差损失函数$\frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$，通过神经网络的梯度反向传播来更新Q网络的所有参数$w$

   h) 如果i%C=1,则更新目标Q网络参数$w'=w$

   i) 如果$S'$是终止状态，则当前迭代轮完毕，否则转到步骤b)

   注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$\epsilon$需要随着迭代的进行而变小。
   
## 第四节 Prioritized Replay DQN

在之前讨论的Nature DQN， DDQN等方法中，它们都是通过经验回放来采样，进而做目标Q值的估算的。在采样的时候，经验回放池中所有的样本都有相等的概率被采样到。

在Q网络中，TD误差指的是目标Q网络计算的目标Q值与当前Q网络计算的Q值之间的差值，考虑到经验回放池中不同样本的TD误差是不同的，对于反向传播的作用也是不一样的。TD误差越大的样本，对反向传播的作用越大，反之则越小。

如果设置TD误差的绝对值$|\delta(t)|$较大的样本更容易被采样，那么我们的算法就会比较容易收敛，这也就是Prioritized Replay DQN的思路。

### Prioritized Replay DQN算法建模

Prioritized Replay DQN根据每个样本的TD误差绝对值$|\delta(t)|$，给定该样本的优先级正比于$|\delta(t)|$，将这个优先级的值存入经验回放池。

1. 由于引入了经验回放的优先级，那么Prioritized Replay DQN的经验回放池和之前的其他DQN算法的经验回放池就不一样了。因为这个优先级大小会影响它被采样的概率。在实际使用中，我们通常使用SumTree这样的二叉树结构来做我们的带优先级的经验回放池样本的存储。

​	具体的SumTree树结构如下图：

<img src="https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302231355262.webp" style="zoom: 33%;" />

​	所有的经验回放样本只保存在最下面的叶子节点上面，一个节点一个样本。内部节点不保存样本数据。

​	而叶子节点除了保存数据以外，还要保存该样本的优先级，就是图中的显示的数字。

​	对于内部节点每个节点只保存**自己的儿子节点的优先级值之和**，如图中内部节点上显示的数字。

​	这样的结构使用了类似二分查找的思想，方便了采样。以上面的树结构为例，根节点是42，如果要采样一个样本，那么我们可以在[0,42]之间做均匀采样。比如我们采样到了24，发现24比左边的子节点29小，就往左边找；比左边的子节点大，就往右边找，同时扣除左子节点的数字13......通过这样的方式就可以快速找到采样到的样本。

2. 除了经验回放池，现在我们的Q网络的算法损失函数也有优化，之前我们的损失函数是：
   $$
   \frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2
   $$
   现在我们新的考虑了样本优先级的损失函数是:
   $$
   \frac{1}{m}\sum\limits_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2
   $$
   其中$w_j$是第j个样本的优先级权重，由TD误差$|\delta(t)|$归一化得到。

3. 当我们对Q网络参数进行了梯度更新后，需要重新计算TD误差，并将TD误差更新到SunTree上面。

除了以上三个部分， Prioritized Replay DQN和DDQN的算法流程相同。

### Prioritized Replay DQN算法流程

下面我们总结下Prioritized Replay DQN的算法流程，基于上一节的DDQN，因此这个算法我们应该叫做Prioritized Replay DDQN。主流程参考论文、\<Prioritized Experience Replay\>(ICLR 2016)。

算法输入：迭代轮数$T$,状态特征维度$n$，动作集$A$,步长$\alpha$,采样系数权重$\beta$,衰减因子$\gamma$,探索率$\epsilon$,当前Q网络$Q$，目标Q网络$Q'$，批量梯度下降的样本数$m$，目标Q网络参数更新频率$C$，SumTree的叶子节点数$S$。

输出：Q网络参数。

1. 随机初始化所有的状态和动作对应的价值$Q$. 随机初始化当前Q网络的所有参数$w$,初始化目标Q网络$Q'$的参数$w'=w$。初始化经验回放SumTree的默认数据结构，所有SumTree的S个叶子节点的优先级$P_{j}$为1。

2. for i from 1 to T，进行迭代。

   a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量$\phi(S)$

   b) 在Q网络中使用$\phi(S)$作为输入，得到Q网络的所有动作对应的Q值输出。用$\epsilon$贪婪法在当前Q值输出中选择对应的动作$A$

   c) 在状态$S$执行当前动作$A$,得到新状态$S'$,得到新状态$S'$对应的特征向量$\phi(S')$和奖励$R$, 是否为终止状态is_end

   d) 将$\{\phi(S),A,R,\phi(S'),is\_end\}$这个五元组存入SumTree

   e) $S=S'$

   f) 从经验回放集合$D$中采样$m$个样本$\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2......,m$，每个样本被采样的概率基于$P(j) = \frac{p_j}{\sum\limits_i(p_i)}$，损失函数权重$w_j = (N*P(j))^{-\beta}/\max_i(w_i)$，计算当前目标Q值$y_{j}$:
   $$
   y_j= \begin{cases} R_j& {is\_end_j\; is \;true}\\ R_j + \gamma Q'(\phi(S'_j),\arg\max_{a'}Q(\phi(S'_j),a,w),w')& {is\_end_j\; is \;false} \end{cases}
   $$
   g) 使用均方差损失函数$\frac{1}{m}\sum\limits_{j=1}^mw_j(y_j-Q(\phi(S_j),A_j,w))^2$, 通过神经网络的梯度反向传播来更新Q网络的所有参数$w$

   h) 重新计算所有样本的TD误差$\delta_j = y_j- Q(\phi(S_j),A_j,w)$,更新SumTree中所有节点的优先级$p_j = |\delta_j|$

   i) 如果$i\%C=1$,则更新目标Q网络参数$w'=w$

   j) 如果$S'$是终止状态，当前轮迭代完毕，否则转到步骤b)

   注意，上述第二步的f步和g步的Q值计算也都需要通过Q网络计算得到。另外，实际应用中，为了算法较好的收敛，探索率$\epsilon$需要随着迭代的进行而变小。

### 总结

Prioritized Replay DQN和DDQN相比，收敛速度有了很大的提高，避免了一些没有价值的迭代，因此是一个不错的优化点。同时它也可以直接集成DDQN算法，所以是一个比较常用的DQN算法。

## 第五节 Dueling DQN

### Dueling DQN的优化点

在DDQN中，通过优化目标Q值的方法来优化算法；在Prioritized Replay DQN中，通过优化经验回放池，按照权重采样来优化算法；而在Dueling DQN中，通过优化神经网络的结构来优化算法。

具体如何优化网络结构呢？Dueling DQN考虑将Q网络分成两部分，第一部分仅仅与状态$S$有关，与具体要采用的动作$A$无关，这部分我们叫做价值函数部分，记作$V(S,w,\alpha)$,第二部分同时与状态$S$和动作$A$有关，这部分叫做优势函数部分(*Advantage Function*)，记为$A(S,A,w,\beta)$,最终的动作价值函数可以表示为：
$$
Q(S,A, w, \alpha, \beta) = V(S,w,\alpha) + A(S,A,w,\beta)
$$
其中，$w$是公共部分的网络参数，$\alpha$是价值函数独有部分的网络参数，$\beta$是优势函数独有部分的网络参数。

### Dueling DQN的网络结构

由于Q网络的价值函数被分为两部分，因此Dueling DQN的网络结构也和之前的DQN不同。

在前面DDQN等DQN算法中，使用了一个简单的三层神经网络：一个输入层，一个隐藏层和一个输出层。如下左图所示：

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302251456277.webp)

而在Dueling DQN中，在隐藏层后面接了两个子网络结构，分别对应价值函数网络部分和优势函数网络部分。对应上面右图所示。最终Q网络的输出由价值函数网络的输出和优势函数网络的输出线性组合得到。

可以直接使用之前动作价值函数的组合公式来得到想要的动作价值，但是这个式子无法辨识最终输出里面$V(S,w,\alpha)$和$A(S,A,w,\beta)$各自的作用，为了体现这种可辨识性，实际使用的组合公式为
$$
Q(S,A, w, \alpha, \beta) = V(S,w,\alpha) + (A(S,A,w,\beta) - \frac{1}{\mathcal{A}}\sum\limits_{a' \in \mathcal{A}}A(S,a', w,\beta))
$$
上式其实是对优势函数部分做了中心化的处理。由于Dueling DQN仅仅涉及神经网络的中间结构的改进，所以算法主流程和其他算法没有差异。

### 总结

这5节主要记录了5个前后有关联的算法：DQN(NIPS2013), Nature DQN, DDQN, Prioritized Replay DQN和Dueling DQN。目前使用的比较主流的是后面三种算法思路，这三种算法思路也是可以混着一起使用的，相互并不排斥。

DQN算是深度强化学习的中的主流流派，代表了Value-Based这一大类深度强化学习算法。但是它也有自己的一些问题，**就是绝大多数DQN只能处理离散的动作集合，不能处理连续的动作集合。**而深度强化学习的另一个主流流派Policy-Based而可以较好的解决这个问题。

## 第六节 策略梯度

之前的DQN算法的主要思想是对价值函数进行近似表示，基于价值来学习。这种Value Based强化学习方法目前得到了比较好的应用，但是Value Based强化学习方法也存在很多的局限性，在某些场景下其它方法更加合适，比如基于策略来学习的强化学习方法。

### Value Based强化学习方法的不足

第一点是对连续动作的处理能力不足。DQN之类的方法一般都是处理离散动作，无法处理连续动作，在一些需要输出连续动作的场景中，Policy Based的方法更加容易建模。

第二点是对受限状态下的问题处理能力不足。在使用特征来描述状态空间中的某一个状态时，有可能因为个体观测的限制或者建模的局限，导致真实环境下本来不同的两个状态却在我们建模后拥有相同的特征描述，进而很有可能导致我们的value Based方法无法得到最优解。此时使用Policy Based强化学习方法也很有效。

第三点是无法解决随机策略问题。Value Based强化学习方法对应的最优策略通常是确定性策略，因为其是从众多行为价值中选择一个最大价值的行为，而有些问题的最优策略却是随机策略，这种情况下同样是无法通过基于价值的学习来求解的。这时也可以考虑使用Policy Based强化学习方法。

由于上面这些原因，Value Based强化学习方法不能通吃所有的场景，我们需要新的解决上述问题的方法，比如Policy Based强化学习方法。

### Policy Based强化学习方法引入

在Value Based强化学习方法中，我们对价值函数进行了近似表示，引入了一个动作价值函数$\hat{q}$，这个函数由参数$w$描述，并接受状态$s$与动作$a$作为输入，计算后得到近似地动作价值，即：
$$
\hat{q}(s,a,w) \approx q_{\pi}(s,a)
$$
在Policy Based强化学习方法下，我们采取类似的思路，只不过这时我们对策略进行近似表示，此时策略$\pi$可以被描述为一个包含参数$\theta$的函数，即：
$$
\pi_{\theta}(s,a) = P(a|s,\theta)\approx  \pi(a|s)
$$
将策略表示成一个连续的函数后，我们就可以用连续函数的优化方法来寻找最优的策略了。而最常用的方法就是梯度上升法，因此需要明确优化目标。

### 策略梯度的优化目标

#### 策略梯度的期望回报

在强化学习中，用$\pi(a|s)$表示在状态$s$时选择动作$a$的概率分布函数，即$π_θ(a|s)=P\{A_t=a|S_t=s,θ_t=θ\}$,表示在时刻$t$，环境状态为$s$，参数为$\theta$，输出动作为$a$的概率为$P$

<img src="https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202302251540963.webp" style="zoom: 50%;" />

在强化学习中，agent与环境进行一个episode的交互的动作轨迹$τ$即图中红线，可以表示为：
$$
τ=\{s_1,a_1,s_2,a_2,......,s_t,a_t,s_{t+1}\}
$$
易得，动作轨迹$τ$出现的概率为：
$$
\begin{aligned}
p(τ)
&=p\{s_1,a_1,s_2,a_2,......,s_t,a_t,s_{t+1}\}\\
&=p(s_1)*p(a_1|s_1)*p(s_2|s_1,a_1)*p(a_2|s_2)*p(s_3|s_2,a_2)*......*p(a_t|s_t)*p(s_{t+1}|s_t,a_t)\\
&=p(s_1)*\prod_{t=1}^{T}(p(a_t|s_t)*p(s_{t+1}|s_t,a_t))
\end{aligned}
$$
如果给定一个策略$π_θ(a|s)$，则$p(τ)$可以转化为：
$$
p_θ(τ)=\pi_θ(τ)=p(s_1)*\prod_{t=1}^{T}(p_θ(a_t|s_t)*p(s_{t+1}|s_t,a_t))
$$
其中$p_θ(a_t|s_t)$由策略$π_θ(a|s)$决定，表示策略采用$θ$情况下，$s_t$时选择$a_t$动作的概率

$p(s_{t+1}|s_t,a_t)$由环境决定，表示$s_t$做出$a_t$动作后，状态转移为$s_{t+1}$的概率

用$R_{t+1}$或者$R(a_t|s_t)$表示在$s_t$状态下，采取动作$a_t$可以得到的即时回报

可以将动作轨迹$\tau$的总回报表示为：
$$
G(\tau)=R_{t+1}+\gamma R_{t+2}+ \gamma^2 R_{t+3}+......=\sum_{k=0}^{\infty } \gamma ^k R_{t+k+1}
$$
期望回报可以表示为：
$$
\begin{aligned}
\overline{\mathrm{G}_{\theta}} 
&=\sum_{\tau} p_{\theta}(\tau) * \mathrm{G}(\tau) \\
&=\mathrm{E}_{\tau \sim \pi_{\theta}} \mathrm{G}(\tau)
\end{aligned}
$$
$\mathrm{E}_{\tau \sim \pi_{\theta}} \mathrm{G}(\tau)$即在$π_θ$策略下的期望回报

#### 最大化期望回报

强化学习的目标是要训练$π_θ$策略，来最大化期望回报。也就是要找到概率分布函数$π_θ(a|s)$的一组参数$\theta$取值，来最大化期望回报$\overline{\mathrm{G}_{\theta}}$
$$
\LARGE
\theta^{*}=\underset{\theta}{\operatorname{argmax}} \overline{\mathrm{G}_{\theta}}
$$
使用梯度上升法来调整$\theta$取值，求$\overline{\mathrm{R}_{\theta}}$对$\theta$的偏导数，即参数$\theta$的梯度：
$$
\begin{aligned}
\LARGE\nabla_{\theta} \overline{\mathrm{G}_{\theta}} 
&\LARGE=\nabla_{\theta} \sum_{\tau} \pi_{\theta}(\tau) * \mathrm{G}(\tau) \\
&\LARGE=\sum_{\tau} \nabla_{\theta} \pi_{\theta}(\tau) * \mathrm{G}(\tau)
\end{aligned}
$$
复合函数求导，存在：
$$
\begin{array}{l}
\LARGE\nabla_{\theta} \log \pi_{\theta}(\tau)=\frac{1}{\pi_{\theta}(\tau)} * \nabla_{\theta} \pi_{\theta}(\tau) \\
\LARGE\Rightarrow \nabla_{\theta} \pi_{\theta}(\tau)=\pi_{\theta}(\tau) * \nabla_{\theta} \log \pi_{\theta}(\tau)
\end{array}
$$
代入，得：
$$
\begin{aligned}
\LARGE\nabla_{\theta} \overline{\mathrm{G}_{\theta}} 
&\LARGE=\sum_{\tau} \nabla_{\theta} \pi_{\theta}(\tau) * \mathrm{G}(\tau) \\
&\LARGE=\sum_{\tau} \pi_{\theta}(\tau) * [ \nabla_{\theta} \log \pi_{\theta}(\tau) * \mathrm{G}(\tau) ] \\
&\LARGE=\mathrm{E}_{\tau \sim \pi_{\theta}(\tau)} [ \nabla_{\theta} \log \pi_{\theta}(\tau) * \mathrm{G}(\tau) ]
\end{aligned}
$$
因此可以知道，期望回报$\overline{\mathrm{G}_{\theta}}$的梯度等于$\nabla_{\theta} \log \pi_{\theta}(\tau) * \mathrm{G}(\tau)$的期望值

分析$\nabla_{\theta} \log \pi_{\theta}(\tau)$可得：
$$
\begin{aligned}
\LARGE\log \pi_{\theta}(\tau) 
&\LARGE=\log \left[\mathrm{p}\left(\mathrm{s}_{1}\right) * \prod_{\mathrm{t}=1}^{\mathrm{T}} \pi_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right) * \mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right)\right] \\
&\LARGE=\log \left[\mathrm{p}\left(\mathrm{s}_{1}\right)\right]+\sum_{\mathrm{t}=1}^{\mathrm{T}} \log \left[\pi_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)\right]+\sum_{\mathrm{t}=1}^{\mathrm{T}} \log \left[\mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right)\right]
\end{aligned}
$$
而对参数$\theta$求梯度，$\log \left[\mathrm{p}\left(\mathrm{s}_{1}\right)\right]$和$\sum_{\mathrm{t}=1}^{\mathrm{T}} \log \left[\mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right)\right]$不含$\theta$，导数显然为0，得到
$$
\begin{aligned}
\LARGE\nabla_{\theta}\log \pi_{\theta}(\tau) 
&\LARGE=\log \left[\mathrm{p}\left(\mathrm{s}_{1}\right) * \prod_{\mathrm{t}=1}^{\mathrm{T}} \pi_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right) * \mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right)\right] \\
&\LARGE=\xcancel{\log \left[\mathrm{p}\left(\mathrm{s}_{1}\right)\right]}+\sum_{\mathrm{t}=1}^{\mathrm{T}}\nabla_{\theta} \log \left[\pi_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)\right]+\xcancel{\sum_{\mathrm{t}=1}^{\mathrm{T}} \log \left[\mathrm{p}\left(\mathrm{s}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}, \mathrm{a}_{\mathrm{t}}\right)\right]}
\end{aligned}
$$
最终梯度表示为：
$$
\begin{aligned}
\LARGE\nabla_{\theta} \mathrm{J}(\theta)
= \nabla_{\theta} \overline{\mathrm{G}_{\theta}}
= \mathrm{E}_{\tau \sim \pi_{\theta}(\tau)}{\{\sum_{\mathrm{t}=1}^{\mathrm{T}} \nabla_{\theta}\log \left[\pi_{\theta}\left(\mathrm{a}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}\right)\right]} *
\mathrm{G}(\tau) \}
\end{aligned}
$$
这就是梯度策略的核心表达式，我们想要求解的梯度策略其实是一个期望，具体工程实现可以采用蒙特卡罗的思想来求取期望，也就是采样求均值来近似表示期望。通过收集一系列的$\mathcal{D}=\left\{\tau_{i}\right\}_{i=1, \ldots, N}$，其中每一条轨迹都是由agent采用策略$\pi_{\theta}$与环境交互采样得到的，则策略梯度可以表示为：
$$
\hat{g}=\frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) G(\tau)
$$
其中，$|\mathcal{D}|$表示采样的轨迹的数量。

### 总结

策略梯度提供了和DQN之类的方法不同的新思路，但是蒙特卡罗策略梯度reinforce算法却并不完美。由于是蒙特卡罗法，需要完全的序列样本才能做算法迭代，同时蒙特卡罗法使用收获的期望来计算状态价值，会导致行为有较多的变异性，导致参数更新的方向很可能不是策略梯度的最优方向。

因此，Policy Based的强化学习方法还需要改进，注意到之前有Value Based强化学习方法，Policy Based+Value Based结合的策略梯度方法被称为Actor-Critic。

## 第七节 Actor-Critic

使用策略梯度（Policy Gradient）的强化学习方法，需要使用蒙特卡洛策略梯度reinforce算法来求解。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。

因此人们提出了策略(Policy Based)和价值(Value Based)相结合的方法：Actor-Critic算法。

### Actor-Critic算法简介

Actor-Critic从名字上看包括两部分，演员(Actor)和评价者(Critic)。其中Actor使用上一节的策略函数$\pi_{\theta}(s,a)$，负责生成动作(Action)并和环境交互。而Critic使用之前的价值函数$\hat{v}(s, w)$和$\hat{q}(s,a,w)$，负责评估Actor的表现，并指导Actor下一阶段的动作。

上一篇的策略梯度中，策略函数就是我们的Actor，但是没有Critic，而是使用了蒙特卡罗法来计算每一步的价值部分替代了Critic的功能，导致了这种方法适用的场景比较受限。因此现在我们使用类似DQN中用的价值函数来替代蒙特卡罗法，作为一个比较通用的Critic。

也就是说在Actor-Critic算法中，我们需要做两组近似，第一组是策略函数的近似：
$$
\pi_{\theta}(s,a) = P(a|s,\theta)\approx  \pi(a|s)
$$
第二组是价值函数的近似，对于状态价值和动作价值函数分别是：
$$
\hat{v}(s, w) \approx v_{\pi}(s)
\\
\hat{q}(s,a,w) \approx q_{\pi}(s,a)
$$
对于我们上一节讲到的蒙特卡罗策略梯度reinforce算法，我们需要进行改造才能变成Actor-Critic算法。

首先，在蒙特卡罗策略梯度reinforce算法中，我们的策略的参数更新公式是：
$$
\theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t)  v_t
$$
梯度更新部分中，$\nabla_{\theta}log \pi_{\theta}(s_t,a_t)$代表了分值函数，不需要改动。而$v_{t}$这部分需要改动，这部分不能再由蒙特卡洛法得到，而应该用Critic得到。

对于Critic来说，这部分可以参考之前DQN的做法，使用一个Q网络来表示Critic，这个Q网络的输入可以是状态，而输出是每个动作的价值或者最优动作的价值。

总结起来就是，Critic通过Q网络计算状态的最优价值$v_{t}$,而Actor利用$v_{t}$这个最优价值迭代更新策略函数的参数$\theta$,调整动作的选择，进而得到反馈和新的状态，Critic使用反馈和新的状态更新Q网络参数$w$，在后面Critic会使用新的网络参数$w$来帮Actor计算状态的最优价值$v_{t}$。

### Actor-Critic算法可选形式

Actor-Critic算法有许多种可以选择的评估形式

1. 基于状态价值：
   $$
   \theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t)  V(s,w)
   $$

2. 基于动作价值：参照DQN的思路，使用动作价值函数Q来做价值评估
   $$
   \theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t)  Q(s,a,w)
   $$

3. 基于TD误差：TD误差的表达式是$\delta(t) = R_{t+1} + \gamma V(S_{t+1}) -V(S_t)$或者$\delta(t) = R_{t+1} + \gamma Q(S_{t+1}，A_{t+1} ) -Q(S_t,A_t)$:
   $$
   \theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t)\delta(t)
   $$

4. 基于优势函数：优势函数的定义是$A(S,A,w,\beta) = Q(S,A, w, \alpha, \beta) - V(S,w,\alpha)$,即动作价值函数与状态价值函数的差值：
   $$
   \theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(s_t,a_t)A(S,A,w,\beta)
   $$

对于Critic本身的模型参数$w$而言，一般都是使用均方误差损失函数来做迭代更新，类似于之前DQN方法中的做法。

### Actor-Critic算法流程

给定一种评估点基于TD误差，Critic使用神经网络来计算TD误差并更新网络参数，Actor也使用神经网络来更新网络参数。

算法输入：迭代轮数$T$，状态特征维度$n$, 动作集$A$, 步长$\alpha$,$\beta$，衰减因子$\gamma$, 探索率$\epsilon$, Critic网络结构和Actor网络结构。

输出：Actor网络参数$\theta$,Critic网络参数$w$

1. 随机初始化所有的状态和动作对应的价值$Q$

2. for i from 1 to T,进行迭代

   a) 初始化S为当前状态序列的第一个状态, 拿到其特征向量$\phi(S)$

   b) 在Actor网络中使用$\phi(S)$作为输入，输出动作$A$，基于动作$A$得到新的状态$S'$,奖励$R$。

   c) 在Critic网络中分别使用$\phi(S)$, $\phi(S')$作为输入，得到Q值输出$V(S)$,$V(S’)$

   d) 计算TD误差$\delta = R +\gamma V(S’) -V(S)$

   e) 使用均方差损失函数$\sum\limits(R +\gamma V(S’) -V(S,w))^2$作Critic网络参数$w$的梯度更新

   f) 更新Actor网络参数$\theta$:
   $$
   \theta = \theta + \alpha \nabla_{\theta}log \pi_{\theta}(S_t,A)\delta
   $$
   对于Actor的分值函数$\nabla_{\theta}log \pi_{\theta}(S_t,A)$，可以选择softmax或者高斯分值函数。

### 总结

基本版的Actor-Critic算法虽然思路很好，但是由于这两个神经网络都需要梯度更新，并且互相依赖，所以比较难以收敛，仍然需要改进。

## 第八节 深度确定性策略梯度(DDPG)

DDPG的思路与DDQN类似，使用了经验回放和双网络的方法来改进Actor-Critic难收敛的问题。

### 从随机策略到确定性策略

DDPG全称是深度确定性策略梯度（Deep Deterministic Policy Gradient），从这个名字来看，它是由D（Deep）+D（Deterministic ）+ PG(Policy Gradient)组成。

确定性策略(DPG)是和随机策略相对而言的，对于某一些动作集合来说，它可能是连续值，或者非常高维的离散值，这样动作的空间维度极大。如果我们使用随机策略，即像DQN一样研究它所有的可能动作的概率，并计算各个可能的动作的价值的话，那需要的样本量是非常大才可行的。于是有人就想出使用确定性策略来简化这个问题。

作为随机策略，在相同的策略，在同一个状态处，采用的动作是基于一个概率分布的，即是不确定的。而确定性策略则决定简单点，虽然在同一个状态处，采用的动作概率不同，但是最大概率只有一个，如果我们只取最大概率的动作，去掉这个概率分布，那么就简单多了。即作为确定性策略，相同的策略，在同一个状态处，动作是唯一确定的，即策略变成
$$
\pi_{\theta}(s) = a
$$

### 从DPG到DDPG

在看确定性策略梯度DPG前，我们看看基于Q值的随机性策略梯度的梯度计算公式：
$$
\nabla_{\theta}J(\pi_{\theta}) = E_{s\sim\rho^{\pi}, a\sim\pi_{\theta}}[\nabla_{\theta}log \pi_{\theta}(s,a)Q_{\pi}(s,a)]
$$
其中状态的采样空间为$\rho^{\pi}$,$\nabla_{\theta}log \pi_{\theta}(s,a)$是分值函数，这代表了随机性策略梯度需要在整个动作的空间$\pi_{\theta}$上进行采样

而DPG基于Q值的确定性策略梯度的梯度计算公式是：
$$
\nabla_{\theta}J(\pi_{\theta}) = E_{s\sim\rho^{\pi}}[\nabla_{\theta} \pi_{\theta}(s)\nabla_{a}Q_{\pi}(s,a)|_{a=\pi_{\theta}(s)}]
$$
跟随机策略梯度的式子相比，少了对动作的积分，多了回报Q函数对动作的导数。

而从DPG到DDPG的过程，完全可以类比DQN到DDQN的过程。除了老生常谈的经验回放以外，我们有了双网络，即当前网络和目标网络的概念。而由于现在我们本来就有Actor网络和Critic两个网络，那么双网络后就变成了4个网络，分别是：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络。2个Actor网络的结构相同，2个Critic网络的结构相同。那么这4个网络的功能各自是什么呢？

### DDPG的原理

DDPG有4个网络，在了解这4个网络的功能之前，我们先复习DDQN的两个网络：当前Q网络和目标Q网络的作用。

DDQN的当前Q网络负责对当前状态$S$使用$\epsilon$贪婪法选择动作$A$，执行动作$A$,获得新状态$S'$和奖励$R$,将样本放入经验回放池，对经验回放池中采样的下一状态$S'$使用贪婪法选择动作$A'$，供目标Q网络计算目标Q值，当目标Q网络计算出目标Q值后，当前Q网络会进行网络参数的更新，并定期把最新网络参数复制到目标Q网络。

DDQN的目标Q网络则负责基于经验回放池计算目标Q值,提供给当前Q网络用，目标Q网络会定期从当前Q网络复制最新网络参数。

现在我们回到DDPG，作为DDPG，Critic当前网络，Critic目标网络和DDQN的当前Q网络，目标Q网络的功能定位基本类似，但是我们有自己的Actor策略网络，因此不需要$\epsilon$-贪婪法这样的选择方式，这部分DDQN的功能到了DDPG可以在Actor当前网络完成。而对经验回放池中采样的下一状态$S'$使用贪婪法选择动作$A'$，这部分工作用来估计目标Q值，因此可以放到Actor网络完成。

基于经验回放池和目标Actor网络提供的$S'$,$A'$计算目标Q值的一部分，这部分由于是评估，因此还是放到Critic目标网络完成。而Critic目标网络计算出目标Q值一部分后，Critic当前网络会计算目标Q值，并进行网络参数的更新，并定期将网络参数复制到Critic目标网络。

此外，Actor当前网络也会基于Critic当前网络计算出的目标Q值，进行网络参数的更新，并定期将网络参数复制到Actor目标网络。

有了上面的思路，我们总结下DDPG 4个网络的功能定位：

1. Actor当前网络：负责策略网络参数$\theta$的迭代更新，负责根据当前状态$S$选择当前动作$A$，用于和环境交互生成$S'$,$R$。
2. Actor目标网络：负责根据经验回放池中采样的下一状态$S'$选择最优下一动作$A'$。网络参数$\theta'$定期从$\theta$复制。
3. Critic当前网络：负责价值网络参数$w$的迭代更新，负责计算负责计算当前Q值$Q(S,A,w)$。目标Q值$y_i = R+\gamma Q'(S',A',w')$
4. Critic目标网络：负责计算目标Q值中的$Q'(S',A',w')$部分。网络参数$w'$定期从$w$复制。

此外，DDPG从当前网络到目标网络的复制和我们之前讲到了DQN不一样。回想DQN，我们是直接把将当前Q网络的参数复制到目标Q网络，即$w'=w$, DDPG这里没有使用这种硬更新，而是使用了软更新，即每次参数只更新一点点，即：
$$
w' \gets \tau w+ (1-\tau)w'
\\
\theta' \gets \tau \theta+ (1-\tau)\theta'
$$
其中$\tau$是更新系数，一般取的比较小，比如0.1或者0.01这样的值。

同时，为了学习过程可以增加一些随机性，增加学习的覆盖，DDPG对选择出来的动作$A$会增加一定的噪声$\mathcal{N}$,即最终和环境交互的动作$A$的表达式是：
$$
A = \pi_{\theta}(S) + \mathcal{N}
$$
最后，我们来看看DDPG的损失函数。对于Critic当前网络，其损失函数和DQN是类似的，都是均方误差，即：
$$
J(w) =\frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2
$$
而对于 Actor当前网络，其损失函数就和PG，A3C不同了，这里由于是确定性策略，原论文定义的损失梯度是：
$$
\nabla_J(\theta) = \frac{1}{m}\sum\limits_{j=1}^m[\nabla_{a}Q_(s_i,a_i,w)|_{s=s_i,a=\pi_{\theta}(s)}\nabla_{\theta} \pi_{\theta(s)}|_{s=s_i}]
$$
这个可以对应上我们第二节的确定性策略梯度，看起来比较麻烦，但是其实理解起来很简单。假如对同一个状态，我们输出了两个不同的动作$a_{1}$和$a_{2}$，从Critic当前网络得到了两个反馈的Q值，分别是$Q_{1}$和$Q_{2}$，假设$Q_{1}$>$Q_{2}$,即采取动作1可以得到更多的奖励，那么策略梯度的思想是什么呢，就是增加$a_{1}$的概率，降低$a_{2}$的概率，也就是说，Actor想要尽可能的得到更大的$Q$值。所以我们的Actor的损失可以简单的理解为得到的反馈$Q$值越大损失越小，得到的反馈$Q$值越小损失越大，因此只要对状态估计网络返回的$Q$值取个负号即可，即：
$$
J(\theta) =  -\frac{1}{m}\sum\limits_{j=1}^m Q_(s_i,a_i,w)
$$

### DDPG算法流程

输入：Actor当前网络，Actor目标网络，Critic当前网络，Critic目标网络,参数分别为$\theta$,$\theta'$,$w$,$w'$衰减因子$\gamma$, 软更新系数$\tau$,批量梯度下降的样本数$m$,目标Q网络参数更新频率$C$。最大迭代次数$T$。随机噪音函数$\mathcal{N}$

输出：最优Actor当前网络参数$\theta$,Critic当前网络参数$w$

1. 随机初始化$\theta$,$w$,$\theta'=theta$,$w'=w$。清空经验回放的集合$D$

2. for i from 1 to T，进行迭代。

   a) 初始化$S$为当前状态序列的第一个状态, 拿到其特征向量$\phi(S)$

   b) 在Actor当前网络基于状态$S$得到动作$A =\pi_{\theta}(\phi(S)) + \mathcal{N}$

   c) 执行动作$A$,得到新状态$S'$,奖励$R$,是否终止状态$is\_end$

   d) 将$\{\phi(S),A,R,\phi(S'),is\_end\}$这个五元组存入经验回放集合$D$

   e) $S=S'$

   f) 从经验回放集合$D$中采样$m$个样本$\{\phi(S_j),A_j,R_j,\phi(S'_j),is\_end_j\}, j=1,2.,,,m$,计算当前目标Q值$y_j$：

   g) 使用均方差损失函数$\frac{1}{m}\sum\limits_{j=1}^m(y_j-Q(\phi(S_j),A_j,w))^2$, 通过神经网络的梯度反向传播来更新Critic当前网络的所有参数$w$

   h) 使用$J(\theta) =  -\frac{1}{m}\sum\limits_{j=1}^m Q_(s_i,a_i,\theta)$,通过神经网络的梯度反向传播来更新Actor当前网络的所有参数$\theta$

   i) 如果i%C=1,则更新Critic目标网络和Actor目标网络参数：
   $$
   w' \gets \tau w+ (1-\tau)w'
   \\
   \theta' \gets \tau \theta+ (1-\tau)\theta'
   $$
   j) 如果$S'$是终止状态，当前轮迭代完毕，否则转到步骤b)

   以上就是DDPG算法的主流程，要注意的是上面2.f中的$\pi_{ \theta'}(\phi(S'_j))$是通过Actor目标网络得到，而$Q'(\phi(S'_j),\pi_{ \theta'}(\phi(S'_j)),w')$则是通过Critic目标网络得到的。

### 总结

DDPG参考了DDQN的算法思想，通过双网络和经验回放，加一些其他的优化，比较好的解决了Actor-Critic难收敛的问题。因此在实际运用中比较多，是一个比较成熟的Actor-Critic算法。

## 第九节 PPO算法

### 信赖域方法TRPO

TRPO中通过严格的数学推导，将目标函数变为：
$$
\begin{array}{l}
\underset{\theta}{\operatorname{maximize}} \quad \hat{\mathbb{E}}_{t}\left[\frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} \mid s_{t}\right)} \hat{A}_{t}\right] \\
\text { s. } t \text {. } \\
\hat{\mathbb{E}}_{t}\left[\mathrm{KL}\left[\pi_{\theta_{\text {old }}}\left(\cdot \mid s_{t}\right), \pi_{\theta}\left(\cdot \mid s_{t}\right)\right]\right] \leq \delta \\
\end{array}
$$
这有两点变化

1、采用重要性采样（importance sample），用旧策略更新新策略（注意引入了重要性采样之后还是on policy的，这一点在代码细节会说）

2、采用信赖域约束，让新旧策略的KL散度小于一个值，即限制了策略的更新幅度，这保证了策略的单调改进性。

在对目标进行线性近似和对约束进行二次近似之后，可以使用共轭梯度算法有效地近似解决该问题。

同时，根据拉格朗日对偶性，它也等价于解决以下无约束惩罚优化问题：
$$
\underset{\theta}{\operatorname{maximize}} \hat{\mathbb{E}}_{t}\left[\frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} \mid s_{t}\right)} \hat{A}_{t}-\beta \operatorname{KL}\left[\pi_{\theta_{\text {old }}}\left(\cdot \mid s_{t}\right), \pi_{\theta}\left(\cdot \mid s_{t}\right)\right]\right]
$$
TRPO 最终使用了硬约束而不是惩罚。这是因为很难选择在不同问题上表现良好的单个 $\beta$ 值，或者甚至在单个问题中，特征在学习过程中发生变化。

而在PPO中，通过自适应 $\beta$，解决了惩罚优化的问题。

### PPO1-近端策略优化惩罚（PPO-penalty）

重要性采样中，我们希望$\theta$和$\theta '$不要相差太大。这个不要相差太大不是说参数取值不能相差太大，而是输入同样的state,网络得到的动作的概率分布不能相差太大。

将两个网络输出动作的概率分布的KL散度作为惩罚加入到目标函数里，得到新的目标函数$\large J^{\theta '}_{PPO}(\theta)$,就是PPO的思想
$$
\begin{aligned}
\LARGE
J^{\theta '}=\mathrm{E}_{(s_t,a_t) \sim \pi_{\theta'}}
[\mathrm{A}^{\theta'}(s_t,a_t) * \frac{p_\theta(a_t|s_t)}{p_\theta'(a_t|s_t)}] \\

\LARGE
J^{\theta '}_{PPO}(\theta)=J^{\theta '}-\beta KL(\theta , \theta ')
\end{aligned}
$$
$\beta$被称为自适应系数，与KL散度共同构成了自适应KL散度。

$\beta$的大小设置是一个重要的问题，PPO使用了一种动态调节的思想。假设优化完成后，如果KL散度的值太大，说明惩罚项没有发挥作用；如果KL散度的值太小，说明惩罚项的影响太大了。为此需要设置可以接受的KL散度的最大值和最小值，把KL散度控制在一个合理的范围内。遵从以下算法：

- if $\large KL(\theta, \theta ' )> KL_{max}$ , increase $\large \beta$
- if $\large KL(\theta, \theta ' )< KL_{min}$ , decrease $\large \beta$

### PPO2-近端策略优化裁剪（PPO-clip）

近端策略优化裁剪希望$\large p_{\theta}(a_{t}|s_{t})$和$\large p_{\theta^k}(a_{t}|s_{t})$,也就是实际学习的模型跟做示范的模型，在优化以后概率分布不要差太多。
$$
\begin{aligned}
\large
J_{P P O 2}^{\theta^{k}}(\theta) \approx \sum_{\left(s_{t}, a_{t}\right)} \min \left(\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right)\right. , \\
&
\large\left.\operatorname{clip}\left(\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)}, 1-\varepsilon, 1+\varepsilon\right) A^{\theta^{k}}\left(s_{t}, a_{t}\right)\right)
\end{aligned}
$$
![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202303252108351.webp)

绿色的线代表min中的第一项，即不做任何处理；蓝色的线代表min中的第二项，代表设置上下界并剪切之后的，如果两个分布相差很大，就会进行剪切。

这其实就是控制$\large p_{\theta}(a_{t}|s_{t})$跟$\large p_{\theta^k}(a_{t}|s_{t})$在优化以后不要差距太大。具体来说：

如果 A > 0，也就是某一个状态-动作的对是好的，我们希望增加这个状态-动作对的概率。也就是想要让$\large p_{\theta}(a_{t}|s_{t})$变大，但它跟$\large p_{\theta^k}(a_{t}|s_{t})$的比值不可以超过$1+\epsilon$。如果超过 $1+\epsilon$ 的话，就没有好处了。红色的线就是目标函数，我们希望目标越大越好，也就是希望$\large p_{\theta}(a_{t}|s_{t})$越大越好。但是$\LARGE \frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)}$只要大过 $1+\epsilon$，就没有好处了。所以在训练的时候，当 $\large p_{\theta}(a_{t}|s_{t})$被 训练到$\LARGE \frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)}>1+\epsilon $时，它就会停止。

如果 A < 0，也就是某一个状态-动作对是不好的，我们希望把$\large p_{\theta}(a_{t}|s_{t})$变小。但它跟$\large p_{\theta^k}(a_{t}|s_{t})$的比值不可以低于$1-\epsilon$，减小到$\LARGE \frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)}$是$1-\epsilon$ 的时候就停了，这样的好处就是不会让$\large p_{\theta}(a_{t}|s_{t})$跟$\large p_{\theta^k}(a_{t}|s_{t})$差距太大。

## TD3算法

### 算法简介

之前在Double DQN算法时就曾分析过Deep Q-Learning (DQN)算法存在高估问题，而DDPG算法是从DQN算法进化得到，因此它也存在一样的问题。为此，TD3算法就很自然地被提出，主要解决DDPG算法的高估问题。

TD3算法也是Actor-Critic (AC)框架下的一种确定性深度强化学习算法，它结合了深度确定性策略梯度算法和双重Q学习，在许多连续控制任务上都取得了不错的表现。

### TD3算法做出的改进

TD3算法在DDPG算法的基础上，提出了三个关键技术：

1. 双重网络 (Double network)：采用两套Critic网络，计算目标值时取二者中的较小值，从而抑制网络过估计问题。
2. 目标策略平滑正则化(Target policy smoothing regularization)：计算目标值时，在下一个状态的动作上加入扰动，从而使得价值评估更准确。
3. 延迟更新 (Delayed update)：Critic网络更新多次后，再更新Actor网络，从而保证Actor网络的训练更加稳定。

### TD3网络结构

TD3算法中包含六个网络，分别是Actor网络，Critic1网络，Critci2网络，Target Actor网络，Target Critic1网络，Target Critic2网络。相较于DDPG算法，TD3算法又多了一套Critic网络，这就是双重网络的由来。

### DQN算法过估计的原因

初始版本的DQN中导致过估计的两个原因

1. 计算TD target时，使用到了最大化，导致计算的结果比真实值要大。
2. 自举(Bootstrapping)，DQN估计TD target和更新网络时使用的是同一个网络，导致第一个原因导致的高估问题被进一步放大。

### 双重网络对于过估计的改进

DDPG采用目标网络解决了自举的问题。但是，除了自举以外，最大化也是造成过估计的重要原因，因此要想彻底解决网络过估计，还需要解决最大化问题。

双重网络是解决最大化问题的有效方法。在TD3算法中，作者引入了两套相同网络架构的Critic网络。计算目标值时，会利用二者间的较小值来估计下一个状态动作对$(s',a')$的状态动作价值，即
$$
y=r+\gamma \min _{i=1,2} Q_{i}^{\prime}\left(s^{\prime}, a^{\prime} \mid \theta_{i}^{Q^{\prime}}\right)
$$

### 目标策略平滑正则化

确定性策略的一个问题是它们可能会过拟合价值估计中的狭窄峰值，导致了相似动作可能会取得完全不同的价值。当更新Critic网络时，使用确定性策略的学习目标极易受到函数逼近误差的影响，从而导致目标估计的方差大，估计值不准确。

而目标策略优化这种方法主要强调：类似的行动应该具有类似的价值。虽然函数近似隐式地实现了这一点，但可以通过修改训练过程显示地强调类似动作之间的关系。具体的实现是利用目标动作周围的区域来计算目标值，从而有利于平滑估计值，用式子表达就是：
$$
y=r+E\left[Q^{\prime}\left(s^{\prime}, \mu^{\prime}\left(s^{\prime} \mid \theta^{\prime}\right)+\epsilon \mid \theta^{Q^{\prime}}\right)\right]
$$


在实际操作时，我们可以通过向目标动作中添加少量随机噪声，并在小批量中求平均值，来近似动作的期望。因此，上式可以修改为:
$$
y=r+\gamma Q^{'}\left(s^{'},\mu^{'}\left(s^{'}\mid\theta^{'}\right)+\epsilon\mid\theta^{Q^{'}}\right) 
\\
\epsilon \sim c l i p\left(N\left(0,\sigma\right),-c,c\right)
$$
其中，我们添加的噪声是服从正态分布的，并且对采样的噪声做了裁剪，以保持目标接近原始动作。直观的说，采用这种方法得出的策略往往更加安全，因为它们为抵抗干扰的动作提供了更高的价值。说了这么多可能不是特别容易理解，不妨来看两张图。

<img src="https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202303211129192.webp" style="zoom:50%;" />

假设上图为Critic网络估计的Q值曲面。这里我们直接采用$Q(s',a')$来估计$Q(s,a)$,因此方差会很大，不利于网络训练。

<img src="https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202303211130794.webp" style="zoom:40%;" />

这次我们采用状态动作对$Q(s',a')$的邻域来估计$Q(s,a)$，从而可以极大地降低方差，提高目标值估计的准确性，保证网络训练过程的稳定。

### 延迟更新

这里的延迟更新指的是Actor网络的延迟更新，即Critic网络更新多次之后再对Actor网络进行更新。这个想法其实是非常直观的，因为Actor网络是通过最大化累积期望回报来更新的，它需要利用Critic网络来进行评估。如果Critic网络非常不稳定，那么Actor网络自然也会出现震荡。

因此，我们可以让Critic网络的更新频率高于Actor网络，即等待Critic网络更加稳定之后再来帮助Actor网络更新。

### TD3算法更新过程

TD3算法的更新过程与DDPG算法的更新过程差别不大，主要区别在于目标值的计算方式。

其中Actor网络通过最大化累积期望回报来更新（确定性策略梯度），Critic1和Critic2网络都是通过最小化评估值与目标值之间的误差来更新（MSE），所有的目标网络都采用软更新的方式来更新（Exponential Moving Average, EMA）。在训练阶段，我们从Replay Buffer中采样一个批次(Batch size) 的数据，假设采样到的一条数据为$(s,a,r,s',done)$,所有网络的更新过程如下。

Critic1和Critic2网络更新过程：利用Target Actor网络计算出状态$s'$下的动作：
$$
a'=\mu'(s'|\theta^{\mu'})
$$
然后基于目标策略平滑正则化，再目标动作$a'$上加入噪声：
$$
a'=a'+\epsilon
\\
\epsilon \sim c l i p\left(N\left(0,\sigma\right),-c,c\right)
$$
然后基于双重网络的思想，计算目标值：
$$
y = r + \gamma min_{i=1,2}Q_{i}'(s',a'|\theta_{i}^{Q'})
$$
最后利用梯度下降算法最小化评估值和目标值之间的误差$L_{c_{i}}$，从而对Critic1和Critic2网络中的参数进行更新。
$$
L_{c_{i}}=(Q_{i}(s,a|\theta^{Q_{i}})-y)^{2}(i=1,2)
$$
Actor网络更新过程：(在Ctitic1和Critic2网络更新$d$步之后，启动Actor网络更新) 利用Actor网络计算出状态$s$下的动作
$$
a_{new}=\mu(s|\theta^{\mu})
$$
**这里需要注意：计算出动作后不需要加入噪声，因为这里是希望Actor网络能够朝着最大值方向更新，加入噪声没有任何意义。**然后利用Critic1或者Critic2网络来计算状态动作对$(s,a_{new})$的评估值，这里我们假定使用Critic1网络：
$$
q_{new}=Q_{1}(s,a_{new}|\theta^{Q_{1}})
$$
最后采用梯度上升算法最大化$q_{new}$,从而完成对Actor网络的更新。

**注：这里之所以可以使用Critic1和Critic2两者中的任何一个来计算Q值，主要是因为Actor网络的目的就在于最大化累积期望回报，没有必要使用最小值。**

目标网络的更新过程：采用软更新方式对目标网络进行更新。引入一个学习率$\tau$，将旧的目标网络参数和新的对应网络参数做加权平均，然后赋值给目标网络。
$$
\theta^{Q_{i}'}=\tau \theta^{Q_{i}}+(1-\tau)\theta^{Q_{i}'}(i=1,2)
\\
\theta^{\mu'} = \tau \theta^{\mu}+(1-\tau)\theta^{\mu'}
$$
学习率（动量）$\tau \in (0,1) $，通常取值为0.005

## SAC算法

### 算法简介

PPO算法是目前最主流的DRL算法，但是PPO是一种on-policy算法，存在sample inefficiency的缺点，需要巨量的采样才能学习。DDPG及其拓展是面向连续控制的off-policy的算法，相对于PPO来说更sample efficient，但是它存在对其超参数敏感，收敛效果差的问题。SAC算法是面向最大熵强化学习开发的一种off-policy算法。与DDPG相比，SAC使用的是随机策略，相比确定性策略具有一定的优势。与 on-policy的PPO算法相比，off-policy的SAC算法采样效率更高。

### 随机策略相比于确定性策略具有什么优势

1. 探索能力更强：在学习或者探索未知环境时，随机性策略可以更好地探索可能性更高的行动，避免陷入局部最优解中。
2. 鲁棒性更好：在面对不确定性和噪声时，随机性策略可以更好地应对，因为它可以通过在不同的行动中进行选择来减少不确定性和噪声的影响。
3. 支持多样性：随机性策略可以促进策略多样性，使得智能体可以适应不同的环境和任务。

### 基于最大熵的RL算法具有什么优势

$$
\pi^{*}=\arg \max _{\pi} \mathbb{E}_{\left(s_{t}, a_{t}\right) \sim \rho_{\pi}}[\sum_{t} \underbrace{R\left(s_{t}, a_{t}\right)}_{\text {reward }}+\alpha \underbrace{H\left(\pi\left(\cdot \mid s_{t}\right)\right)}_{\text {entropy }}]
$$

上式为DRL的学习目标，与传统的DRL学习目标不同，不仅想要长期的回报最大，还想要policy的每一次输出的action的熵最大。这样做其实就是为了让策略随机化，也是在鼓励探索，为具有相似的Q值的动作分配近乎均等的概率，不会给动作范围内任何一个动作分配非常高的概率，避免了反复选择同一个动作而陷入次优。同时通过最大化奖赏，放弃明显没有前途的策略（放弃低奖赏策略）。

其具有以下几个优势：

1）学到policy可以作为更复杂具体任务的初始化。因为通过最大熵，policy不仅仅学到一种解决任务的方法，而是所有all。因此这样的policy就更有利于去学习新的任务。比如我们一开始是学走，然后之后要学朝某一个特定方向走。

2）更强的exploration能力，这是显而易见的，能够更容易的在多模态reward （multimodal reward）下找到更好的模式。比如既要求机器人走的好，又要求机器人节约能源

3）更robust鲁棒，更强的generalization。因为要从不同的方式来探索各种最优的可能性，也因此面对干扰的时候能够更容易做出调整。（干扰会是神经网络学习过程中看到的一种state，既然已经探索到了，学到了就可以更好的做出反应，继续获取高reward）

### 最大熵强化学习

通常强化学习的目标是最大化累计reward:
$$
\sum_{t} \mathbb{E}_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \rho_{\pi}}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
$$
最大熵强化学习中考虑的是最大化带熵的累计奖励：

$$
J(\pi)=\sum_{t=0}^{T} \mathbb{E}_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \rho_{\pi}}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\alpha \mathcal{H}\left(\pi\left(\cdot \mid \mathbf{s}_{t}\right)\right)\right]
$$
上式中的$\alpha$是名为温度系数的超参数，用于调整对熵值的重视程度

这样做的基本目的是什么呢？让策略随机化，即输出的每一个action的概率尽可能分散，而不是集中在一个action上。

## 离线强化学习

## 多智能体强化学习

## 分层强化学习

解决稀疏奖励问题

## 逆强化学习

## 模仿学习

# 第四部分 强化学习技巧

## 优势函数

在强化学习中，优势函数（advantage function）是指一个与状态-动作对相关的函数，用于表示在给定状态$s$下采取某个动作$a$相对于采取平均动作的优势程度。它可以被用来评估某个动作是否比平均水平更好或更差，从而指导智能体的决策。

为什么要引入优势函数，因为当采样很少时，有的动作可能不会被取到。这时那些被采样到的动作的Q都是正的，在之后的迭代过程中被选择的概率会上升，其它行为的概率会下降，这不利于探索，因为未被采样到的动作表现未必不好。

所以，我们需要 "负" 的奖励，即 "相对好"，直接的思路就是引入baseline作为 "好" 与 "不好" 的基准。

优势函数的其中一种定义可以表示为行为价值函数减去状态值函数的差值。
$$
A^{\pi}(s, a)=Q^{\pi}(s, a)-V^{\pi}(s)
$$
注意有$\mathbb{E}_{a \sim \pi(s)}\left[A^{\pi}(s, a)\right]=0$

那么如果Q>V，就是说当前动作能使当前状态更好，即相对是好的。如果Q<V，就是说当前动作使得当前状态更差，即相对是差的。

值函数$V$评估状态$s$的好坏，动作值函数$Q$评估在状态$s$下采取动作$a$的好坏，而优势函数$A$评估的则是在状态$s$采取动作$a$相对于平均回报的好坏，也就是采取这个动作的优势。

优势函数的使用可以使得智能体更加准确地评估行动的质量，从而使得其能够更好地探索和利用环境，提高学习效率和性能。

## 广义优势估计

## 重要性采样

### 重要性采样要解决的问题

​        重要性采样(Importance Sampling)是统计学中的一种采样方法，主要用在一些难以直接采样的数据分布上。思考一个问题：假设随机变量$X$有一个很复杂的概率密度函数$P(x)$，需要求$X$关于某个函数$f(x)$的期望
$$
\LARGE
\mathrm{E} = \mathrm{E}_{\tau \sim p(x)} [f(x)]
$$
​        求解$\mathrm{E}$最理想的办法是将上面的期望展开变成积分的形式——$\int_{x}p(x)f(x)dx$，然后计算积分。如果随机变量$X$的概率分布$P(x)$和函数$f(x)$已知且比较简单，那么这样是可行的，直接通过计算即可。但是如果$P(x)$或$f(x)$比较复杂，无法通过数学计算直接求解积分，就需要其他办法了。比如$P$或$f$是通过神经网络拟合出来的，我们只能把样本$x$输入到神经网络后得到$P(x)$和$f(x)$，而不知道这两个函数的具体表达式，那么就无法直接计算积分。

​        此时又分随机变量$X$是否可以采样这两种情况，如果随机变量$X$是可以采样的，就可以利用蒙特卡洛方法，根据大数定律，采样足够多的样本来近似计算
$$
\LARGE
E\approx \frac{1}{N} \sum_{x_i\sim p(x),i=1 }^{N} f(x_i)
$$
​		但是还有另一种常见的情况：随机变量$X$不可采样，无法获得样本$x$来输入到$p$和$f$里得到$P(x)$和$f(x)$。重要性采样的出现就是为了解决这种很难在原始分布$p(x)$下采样的问题。

#### 重要性采样的做法

既然此时随机变量$X$无法直接从原始分布$p(x)$下采样，那么我们可以另辟蹊径，从一个简单、可采样、定义域与$p(x)$相同的概率分布$\tilde{p}(x)$ 中进行采样。由于我们的最终目标是求$\mathrm{E} = \mathrm{E}_{\tau \sim p(x)} [f(x)]$，通过简单的推导：
$$
\LARGE
\begin{aligned}
\mathrm{E} &= \mathrm{E}_{x \sim p(x)} [f(x)]\\
&=\int_{x}p(x)f(x)dx\\
&=\int_{x}\tilde{p}(x)\frac{p(x)}{\tilde{p}(x)} f(x)dx\\
&= \mathrm{E}_{x \sim \tilde{p}(x)} [\frac{p(x)}{\tilde{p}(x)}f(x)]
\end{aligned}
$$


成功将原始问题转化成了求概率分布$\tilde{p}(x) $下$\LARGE { \frac{p(x) }{\tilde{p}(x)} f(x)}$的期望，而此时概率分布$\tilde{p}(x)$是我们寻找的一个简单、可采样的分布，可以获得样本$x$且$p(x)$和$f(x)$都可以将样本$x$输入到$p$和$f$中得到。因此依然可以通过蒙特卡洛法，从$\tilde{p}(x)$中采样大量的样本来近似计算


$$
\LARGE
E\approx \frac{1}{N} \sum_{x_i\sim \tilde{p} (x),i=1 }^{N} \frac{p(x_i)}{\tilde{p}(x_i) } f(x_i)
$$

#### 重要性采样的缺陷

重要性采样可以不从原来的旧分布，而从新的分布去采集样本，然后求出目标期望，上述推导显示两者理论上是等价的。但等价存在前提条件：2个分布不能相差太大。换句话说，如果2个分布相差过大，那么两者就不会相等，这就是IS的缺陷。

为什么两者不相等呢？直接贴图

![](https://good2-1316920822.cos.ap-nanjing.myqcloud.com/202303251653916.webp)

重要性采样确实可以让用两个分布去求得期望，但期望相等不代表方差相等。如上图红框所示，如果两者差距过大，就会导致方差差距过大。此时如果采样数据不足够时，方差相差太大会导致两者的样本均值相差很大！

#### 总结

重要性采样的实质是按照两个分布对函数f进行了加权。
重要性采样的用处在于对于原分布下不太好求解的情况下，把问题转移到在另一个分布下求解期望。
重要性采样要发挥作用，得保证新旧两个分布相差不要太离谱。

Note:统计学中的重要性采样不等同于强化学习中Priortized experience replay中的重要性采样，两个概念不一致。总结部分

### 重要性采样应用于强化学习

上面说到重要性采样是因为原始数据分布$p(x)$难以直接采样，从而借助一个简单、可采样的分布$\tilde{p}(x)$来计算期望$E$，但是强化学习中使用重要性采样不是因为$p(x)$难以采样，而是不想通过$p(x)$去采样。

我们想要训练的策略是目标策略(Target Policy)，但是为了实现数据的重复使用，需要用行为策略(Behavior Policy）去与环境交互收集数据，以此实现采样。这与重要性采样的做法是一致的。

在PG中，我们用策略$π_θ$去与环境交互，采样出轨迹$\tau$,用于计算$ \nabla_{\theta} \log \pi_{\theta}(\tau) * \mathrm{R}(\tau)$。

现在我们不再用策略$π_θ$去交互，而改成是$\large π_{\theta '}$去交互，也就是另一个网络结构相同，但是参数不同的actor,收集到的轨迹$\tau$是基于参数$\theta '$的

在Policy gradient中，梯度表示为：
$$
\LARGE
\begin{aligned}
\nabla_{\theta} \overline{\mathrm{R}_{\theta}} 
=\mathrm{E}_{\tau \sim p_{\theta}(\tau)} [ \nabla_{\theta} \log p_{\theta}(\tau) * \mathrm{R}(\tau) ]
\end{aligned}
$$


引入另一个参数为$\theta '$的actor之后，梯度变为：
$$
\LARGE
\begin{aligned}
\nabla_{\theta} \overline{\mathrm{R}_{\theta}} 
=\mathrm{E}_{\tau \sim p_{\theta'}(\tau)} [\frac{p_\theta(\tau)}{p_{\theta '}(\tau)} * \nabla_{\theta} \log p_{\theta}(\tau) * \mathrm{R}(\tau) ]
\end{aligned}
$$
转变为了从$\large π_{\theta '}$中去采样，用来训练$\theta$,实现了on-policy到off-policy的转换



引入优势函数:$\LARGE A^\theta(s_t,a_t)$,表示在$s_t$状态采取动作$a_t$,相比其它动作有多好。

可以得到：
$$
\begin{aligned}
\LARGE
\nabla_{\theta} \mathrm{J}(\theta)
= \nabla_{\theta} \overline{\mathrm{R}_{\theta}}


&\LARGE=\mathrm{E}_{(s_t,a_t) \sim \pi_{\theta}}{
\nabla_{\theta}\log \left[p_{\theta}\left(\mathrm{a}^n_{\mathrm{t}} \mid \mathrm{s}^n_{\mathrm{t}}\right)\right]}\} *
\mathrm{A}^\theta(s_t,a_t) \\

&\LARGE=\mathrm{E}_{(s_t,a_t) \sim \pi_{\theta'}}{
\nabla_{\theta}\log \left[p_{\theta}\left(\mathrm{a}^n_{\mathrm{t}} \mid \mathrm{s}^n_{\mathrm{t}}\right)\right]}\} *
\mathrm{A}^{\theta'}(s_t,a_t) * \frac{p_\theta(s_t,a_t)}{p_\theta'(s_t,a_t)} \\

&\large=\mathrm{E}_{(s_t,a_t) \sim \pi_{\theta'}}{
\nabla_{\theta}\log \left[p_{\theta}\left(\mathrm{a}^n_{\mathrm{t}} \mid \mathrm{s}^n_{\mathrm{t}}\right)\right]}\} *
\mathrm{A}^{\theta'}(s_t,a_t) * \frac{p_\theta(a_t|s_t)}{p_\theta'(a_t|s_t)} \frac{p_{\theta}(s_t)}{p_{\theta'}(s_t)} \\

\end{aligned}
$$
观察式子：$\LARGE\frac{p_\theta(a_t|s_t)}{p_\theta'(a_t|s_t)}$的值可以用神经网络直接求出，$\LARGE \frac{p_{\theta}(s_t)}{p_{\theta'}(s_t)}$的值没办法求，因为不可能知道一个状态出现的可能。但如果两个策略的分布相差不太大，我们就可以认为上下两项可以约去。
$$
\begin{aligned}
\LARGE
\nabla_{\theta} \mathrm{J}(\theta)
= \nabla_{\theta} \overline{\mathrm{R}_{\theta}}

&\large=\mathrm{E}_{(s_t,a_t) \sim \pi_{\theta'}}{
\nabla_{\theta}\log \left[p_{\theta}\left(\mathrm{a}^n_{\mathrm{t}} \mid \mathrm{s}^n_{\mathrm{t}}\right)\right]}\} *
\mathrm{A}^{\theta'}(s_t,a_t) * \frac{p_\theta(a_t|s_t)}{p_\theta'(a_t|s_t)} \xcancel{\frac{p_{\theta}(s_t)}{p_{\theta'}(s_t)}} \\

&\LARGE=\mathrm{E}_{(s_t,a_t) \sim \pi_{\theta'}}{
\nabla_{\theta}\log \left[p_{\theta}\left(\mathrm{a}^n_{\mathrm{t}} \mid \mathrm{s}^n_{\mathrm{t}}\right)\right]}\} *
\mathrm{A}^{\theta'}(s_t,a_t) * \frac{p_\theta(a_t|s_t)}{p_\theta'(a_t|s_t)} \\

\end{aligned}
$$
结合公式$\LARGE\nabla f(x) = f(x)\nabla log f(x)$，可以用梯度反推目标函数

$$
\begin{aligned}
\LARGE
J^{\theta '}=\mathrm{E}_{(s_t,a_t) \sim \pi_{\theta'}}
[\mathrm{A}^{\theta'}(s_t,a_t) * \frac{p_\theta(a_t|s_t)}{p_\theta'(a_t|s_t)}] \\
\end{aligned}
$$
要去优化的目标函数就变成了上面这个式子。

## 蒙特卡洛树搜索



## value-based 和 policy-based 差异、优缺点

**Value-based方法**是通过学习状态值函数（State Value Function）或动作值函数（Action Value Function）来实现决策。其中，状态值函数（V）给定一个状态，预测在该状态下获得奖励的长期累计价值。动作值函数（Q）给定一个状态和一个动作，预测在采取该动作之后，在该状态下获得奖励的长期累计价值。

它的缺点有

1. 是不擅长处理连续动作空间
2. 使用$\epsilon$-greedy探索方法，探索能力与Policy-based相比较弱
3. 无法解决随机性策略问题



**Policy-based方法**则是直接学习一个策略函数，将状态映射到动作。

它的优点有

1. 探索能力更强，可以处理高维或者连续动作
2. 可以学习随机策略

它的缺点有

1. 

## 疑问

1. 强化学习中策略的含义
2. DQN 过估计的具体解释

## 需要补充的知识点

MC 为什么无偏但是高方差

TD 为什么有偏但是低方差

补充on-policy与off-policy中 行为策略与目标策略



# 补充

## RLHF人类反馈强化学习



# 每日问题

1. 梯度的概念是什么，如何用数学式子表达？
2. 链式法则是什么，以二元函数为例子表示一下
3. 马尔科夫决策过程中，状态转移概率是怎么表示的？
4. 状态价值函数和动作价值函数如何用折扣回报的式子表示出来？
5. 状态价值函数和动作价值函数的贝尔曼递推式是怎么表达的？
6. 在TD误差算法中，TD目标值和TD误差值分别表示的是什么？
7. TD目标值具体含义是什么？
8. TD误差方法和MC方法各自的优缺点是什么？并解释导致这些优缺点出现的原因？
9. n步时序差分是什么意思
10. 写出SARSA算法的价值函数迭代公式和学习过程
11. 写出Q-Learning算法的价值函数迭代公式和学习过程
12. Double DQN 为什么有效？
13. 什么是过拟合？什么情况下容易出现过拟合的情况
14. 什么是过估计？为什么DQN存在过估计的问题？
15. TD3相对于DDPG做出了哪三项改进？
16. 强化学习如何输出连续动作
17. actor-critic框架有什么好处
18. 为什么DDPG没有直接使用double dqn的做法
